{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaryBall/CSML_notes/blob/master/heterograph_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0YeI-TqptCv",
        "outputId": "6c652149-3f74-453f-9dc5-bcc10f7bb47e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "# mount the google drive to colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIugSBZApzSk",
        "outputId": "b3ebbe03-8470-415b-de4a-280b7063d9d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu118\n",
            "11.8\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lIUV8oebq9kh",
        "outputId": "a5857ea8-1641-44fc-c7d1-5138d56f0ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910459 sha256=1c5e0ca237576b643fd69dbd380595f52675344625b9de0c615700e7a2de71f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/pyg_lib-0.2.0%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.17%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_cluster-1.6.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (884 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m884.9/884.9 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.22.4)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
            "Successfully installed pyg_lib-0.2.0+pt20cu118 torch_cluster-1.6.1+pt20cu118 torch_scatter-2.1.1+pt20cu118 torch_sparse-0.6.17+pt20cu118 torch_spline_conv-1.2.2+pt20cu118\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2023.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.22.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (8.4.0)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2023.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hydra-core\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ray\n",
            "  Downloading ray-2.4.0-cp310-cp310-manylinux2014_x86_64.whl (58.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ray-lightning\n",
            "  Downloading ray_lightning-0.3.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting overrides\n",
            "  Downloading overrides-7.3.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.25.1)\n",
            "Collecting omegaconf<2.4,>=2.2 (from hydra-core)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from hydra-core)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core) (23.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.23.1-py2.py3-none-any.whl (205 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from ray) (23.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray) (3.12.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray) (4.3.3)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray) (1.0.5)\n",
            "Collecting aiosignal (from ray)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist (from ray)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting virtualenv<20.21.1,>=20.0.24 (from ray)\n",
            "  Downloading virtualenv-20.21.0-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio<=1.51.3,>=1.42.0 (from ray)\n",
            "  Downloading grpcio-1.51.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from ray) (1.22.4)\n",
            "Collecting pytorch-lightning==1.6.* (from ray-lightning)\n",
            "  Downloading pytorch_lightning-1.6.5-py3-none-any.whl (585 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.*->ray-lightning) (2.0.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.*->ray-lightning) (4.65.0)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.*->ray-lightning) (2023.4.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.*->ray-lightning) (2.12.2)\n",
            "Collecting pyDeprecate>=0.3.1 (from pytorch-lightning==1.6.*->ray-lightning)\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.*->ray-lightning) (4.5.0)\n",
            "Collecting protobuf!=4.21.0,<5,>=3.19.0 (from wandb)\n",
            "  Downloading protobuf-3.20.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (8.4.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.*->ray-lightning) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.*->ray-lightning) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.*->ray-lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.*->ray-lightning) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.*->pytorch-lightning==1.6.*->ray-lightning) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.*->pytorch-lightning==1.6.*->ray-lightning) (16.0.3)\n",
            "Collecting distlib<1,>=0.3.6 (from virtualenv<20.21.1,>=20.0.24->ray)\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.10/dist-packages (from virtualenv<20.21.1,>=20.0.24->ray) (3.3.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.19.3)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.*->ray-lightning)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.*->ray-lightning) (1.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.*->ray-lightning) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.*->ray-lightning) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.*->ray-lightning) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.*->ray-lightning) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.*->ray-lightning) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.*->ray-lightning) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.*->ray-lightning) (0.40.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.*->pytorch-lightning==1.6.*->ray-lightning) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.*->pytorch-lightning==1.6.*->ray-lightning) (1.3.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.*->ray-lightning)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.*->ray-lightning)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.*->ray-lightning)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.*->ray-lightning) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.*->ray-lightning) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.*->ray-lightning) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.6.*->ray-lightning) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.*->ray-lightning) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.6.*->ray-lightning) (3.2.2)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, pathtools\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=acc1789b9bf25f3634cf50c49b09547c07ef3cd231d08206fd78b6d180b1c0b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=46a65cc7cd4b67233481ff342ff4f12dd0ea5557dbef418d19665dc81420935b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built antlr4-python3-runtime pathtools\n",
            "Installing collected packages: pathtools, distlib, antlr4-python3-runtime, virtualenv, smmap, setproctitle, sentry-sdk, pyDeprecate, protobuf, overrides, omegaconf, multidict, grpcio, frozenlist, docker-pycreds, async-timeout, yarl, hydra-core, gitdb, aiosignal, ray, GitPython, aiohttp, wandb, torchmetrics, pytorch-lightning, ray-lightning\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.54.0\n",
            "    Uninstalling grpcio-1.54.0:\n",
            "      Successfully uninstalled grpcio-1.54.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery 3.9.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.19.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-datastore 2.15.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-firestore 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-translate 3.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.31 aiohttp-3.8.4 aiosignal-1.3.1 antlr4-python3-runtime-4.9.3 async-timeout-4.0.2 distlib-0.3.6 docker-pycreds-0.4.0 frozenlist-1.3.3 gitdb-4.0.10 grpcio-1.51.3 hydra-core-1.3.2 multidict-6.0.4 omegaconf-2.3.0 overrides-7.3.1 pathtools-0.1.2 protobuf-3.20.1 pyDeprecate-0.3.2 pytorch-lightning-1.6.5 ray-2.4.0 ray-lightning-0.3.0 sentry-sdk-1.23.1 setproctitle-1.3.2 smmap-5.0.0 torchmetrics-0.11.4 virtualenv-20.21.0 wandb-0.15.3 yarl-1.9.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install torch_geometric\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "!pip install rdkit\n",
        "!pip install hydra-core wandb hydra-core ray ray-lightning torchmetrics overrides imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RvYCtJy4h5g",
        "outputId": "7772e151-cf66-4a65-d0ec-16faf65c1813"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'content'\n",
            "/content\n",
            "fatal: destination path 'MiDi' already exists and is not an empty directory.\n",
            "/content/MiDi\n"
          ]
        }
      ],
      "source": [
        "%cd content\n",
        "!git clone https://github.com/cvignac/MiDi/\n",
        "%cd MiDi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SaTKOEV5fVf"
      },
      "outputs": [],
      "source": [
        "from src.datasets import qm9_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuqX0T0K-YZg"
      },
      "outputs": [],
      "source": [
        "import hydra\n",
        "import omegaconf\n",
        "\n",
        "@hydra.main(version_base='1.3', config_path='../configs', config_name='config')\n",
        "def get_cfg(cfg: omegaconf.DictConfig):\n",
        "  return cfg\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATs0EKwbBifS"
      },
      "outputs": [],
      "source": [
        "datamodule = qm9_dataset.QM9DataModule(cfg)\n",
        "dataset_infos = qm9_dataset.QM9infos(datamodule=datamodule, cfg=cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p06XLrQgpu9u"
      },
      "source": [
        "# Molecule Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj3aQf7buZTp"
      },
      "source": [
        "## QM9 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vf8KSZgrLzb",
        "outputId": "f1d0308e-9ef0-4ec9-89ea-8f20b0b40933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset: QM9(3000):\n",
            "======================\n",
            "Number of graphs: 3000\n",
            "Number of features: 11\n",
            "Number of classes: 19\n",
            "\n",
            "Data(x=[5, 11], edge_index=[2, 8], edge_attr=[8, 4], y=[1, 19], pos=[5, 3], idx=[1], name='gdb_1', z=[5])\n",
            "===========================================================================================================\n",
            "Number of nodes: 5\n",
            "Number of edges: 8\n",
            "Average node degree: 1.60\n",
            "Has isolated nodes: False\n",
            "Has self-loops: False\n",
            "Is undirected: True\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import QM9\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "\n",
        "dataset = QM9(root='/tmp/QM9')\n",
        "dataset = dataset[0:3000]\n",
        "\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0]  # Get the first graph object.\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('===========================================================================================================')\n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFG2VK5orL5b",
        "outputId": "4fa7f199-aff6-464a-e394-70188b36755a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training graphs: 2000\n",
            "Number of test graphs: 1000\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(12345)\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "train_dataset = dataset[:2000]\n",
        "test_dataset = dataset[2000:]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORTXcSFVrL-A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loNXffqWXpGa"
      },
      "source": [
        "# Synthetic data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoV2Hf-GXoj0",
        "outputId": "984e9183-c9ab-4a20-f9d6-25aa18a6a84b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All binary combinations: [('A', 'A'), ('A', 'B'), ('A', 'C'), ('A', 'D'), ('B', 'B'), ('B', 'C'), ('B', 'D'), ('C', 'C'), ('C', 'D'), ('D', 'D')]\n",
            "Sampled subset (40%): [('C', 'C'), ('B', 'B'), ('C', 'D'), ('A', 'C')]\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import random\n",
        "\n",
        "# Define the nodes\n",
        "nodes = ['A', 'B', 'C', 'D']\n",
        "\n",
        "# Generate all binary combinations\n",
        "combinations = list(itertools.combinations_with_replacement(nodes, 2))\n",
        "\n",
        "# Calculate 40% of the total combinations\n",
        "sample_size = round(len(combinations) * 0.4)\n",
        "\n",
        "# Randomly sample a subset of the combinations\n",
        "subset = random.sample(combinations, sample_size)\n",
        "\n",
        "# Print the results\n",
        "print(\"All binary combinations:\", combinations)\n",
        "print(\"Sampled subset (40%):\", subset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP7EZX9oOO1D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import networkx as n\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH2NVN2unyNv",
        "outputId": "13836d1e-ca87-42f7-ce2d-2fa8023b025e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampled X: [ 0.66462135  0.68144307 -0.30645359]\n",
            "Sampled Y: [ 0.66046493  0.21117113 -0.72055036]\n",
            "Sampled Z: [ 0.79094642  0.34495514 -0.78071473]\n",
            "X^T * Y = 0.803675449902284\n",
            "Y^T * Z = 1.1577812211266454\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "def random_unit_vector(K):\n",
        "    \"\"\"Generate a random K-dimensional unit vector.\"\"\"\n",
        "    v = np.random.randn(K)\n",
        "    return v / np.linalg.norm(v)\n",
        "\n",
        "def sample_vectors(K):\n",
        "    # Generate random K-dimensional unit vectors for X and Y\n",
        "    X = random_unit_vector(K)\n",
        "    Y = random_unit_vector(K)\n",
        "\n",
        "    # Compute the projection of Y onto X\n",
        "    Y_proj_X = np.dot(X, Y) * X\n",
        "\n",
        "    # Compute the orthogonal component of Y with respect to X\n",
        "    Y_orth = Y - Y_proj_X\n",
        "\n",
        "    # Compute Z by adding a scaled orthogonal component of Y to X\n",
        "    Z = X + np.sqrt(1 - np.dot(X, Y)**2) * Y_orth / np.linalg.norm(Y_orth)\n",
        "\n",
        "    return X, Y, Z\n",
        "\n",
        "# Example usage:\n",
        "K = 3\n",
        "\n",
        "X, Y, Z = sample_vectors(K)\n",
        "\n",
        "print(\"Sampled X:\", X)\n",
        "print(\"Sampled Y:\", Y)\n",
        "print(\"Sampled Z:\", Z)\n",
        "print(\"X^T * Y =\", np.dot(X, Y))\n",
        "print(\"Y^T * Z =\", np.dot(Y, Z))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zN617cw9Hl-"
      },
      "source": [
        "# Function for generating Heterogeneous Graphs \n",
        "\n",
        "- with Small world assumption - Watts Strogatz Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE4mRj1vguCE"
      },
      "outputs": [],
      "source": [
        "from IPython.terminal.embed import ultratb\n",
        "import networkx as nx\n",
        "import random\n",
        "from collections import deque\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "\n",
        "def ws_graph (n,k,p):\n",
        "  return nx.watts_strogatz_graph(n, k, p)\n",
        "\n",
        "def get_available_node_types(current_node, graph, meta_graph):\n",
        "    if current_node is not None:\n",
        "      current_node_type = graph.nodes[current_node].get('type')\n",
        "    else:\n",
        "      current_node_type = None\n",
        "    \n",
        "    # process the case when we randomly select a node to start\n",
        "    if not current_node_type:\n",
        "        return list(meta_graph.keys())\n",
        "    else:\n",
        "        return list(meta_graph[current_node_type].keys())\n",
        "\n",
        "\n",
        "def normalize_node_type_prob(available_types, node_type_prob):\n",
        "    total_prob = sum(node_type_prob[available_types].values())\n",
        "    node_type_prob_normalized = {}\n",
        "    for node_type, prob in node_type_prob.items():\n",
        "      normalized_prob = prob / total_prob\n",
        "      node_type_prob_normalized[node_type] = normalized_prob\n",
        "\n",
        "    return node_type_prob_normalized\n",
        "\n",
        "\n",
        "\n",
        "def generate_heterogeneous_small_world(graph, meta_graph, node_type_prob):\n",
        "    # Step 1: Generate a Watts-Strogatz graph\n",
        "    \n",
        "\n",
        "    # Step 2: Normalize node type probabilities\n",
        "    node_type_prob_normalized = {node_type: prob / sum(node_type_prob.values()) for node_type, prob in node_type_prob.items()}\n",
        "    print(node_type_prob_normalized)\n",
        "\n",
        "    # Step 3: Assign node and edge types using BFS\n",
        "    visited = set()\n",
        "    print(graph.nodes())\n",
        "    current_node = None\n",
        "    last_node = None\n",
        "\n",
        "    possible_rel = [(i,j) for i in meta_graph.keys() for j in meta_graph[i].keys()]\n",
        "\n",
        "    for node in graph.nodes():\n",
        "        if node not in visited:\n",
        "            # BFS traversal\n",
        "            queue = deque([node])\n",
        "            \n",
        "            visited.add(node)\n",
        "\n",
        "            while queue:\n",
        "                print(queue)\n",
        "            \n",
        "                current_node = queue.popleft()\n",
        "                \n",
        "                if not graph.nodes[current_node].get('type'):\n",
        "                  possible_start_type = get_available_node_types(current_node, graph, meta_graph)\n",
        "                  sample_prob = [node_type_prob_normalized[node_type] for node_type in possible_start_type]\n",
        "                  node_type = random.choices(possible_start_type, sample_prob, k=1)[0]\n",
        "                  graph.nodes[current_node]['type'] = node_type\n",
        "\n",
        "                available_node_types = get_available_node_types(current_node, graph, meta_graph)\n",
        "\n",
        "                # Assign edge types and add unvisited neighbors to queue\n",
        "                for neighbor in graph.neighbors(current_node):\n",
        "                    if neighbor not in visited:\n",
        "                        # Assign node type\n",
        "                        if not graph.nodes[neighbor].get('type'):\n",
        "                            sample_prob = [node_type_prob_normalized[node_type] for node_type in available_node_types]\n",
        "                            node_type = random.choices(available_node_types, sample_prob, k=1)[0]\n",
        "\n",
        "                            print(graph.nodes[current_node].get('type'), available_node_types)\n",
        "\n",
        "                            print(\"sampled node type for node {vnumber}: {vtype}\".format(vnumber = neighbor, vtype = node_type))\n",
        "                            graph.nodes[neighbor]['type'] = node_type\n",
        "\n",
        "                        queue.append(neighbor)\n",
        "                        visited.add(neighbor)\n",
        "\n",
        "                last_node = current_node\n",
        "            \n",
        "    for u, v  in graph.edges():\n",
        "      if (graph.nodes[u]['type'],graph.nodes[v]['type']) in possible_rel:\n",
        "        print(u,v, graph.nodes[u]['type'], graph.nodes[v]['type'], meta_graph[graph.nodes[u]['type']][graph.nodes[v]['type']])\n",
        "        graph.edges[u, v]['type'] = meta_graph[graph.nodes[u]['type']][graph.nodes[v]['type']]\n",
        "      else:\n",
        "        print(u,v, graph.nodes[u]['type'], graph.nodes[v]['type'], 'NA')\n",
        "        graph.edges[u, v]['type'] = 'NA'\n",
        "    \n",
        "    return graph\n",
        "\n",
        "meta_graph = {\n",
        "    'A': {'C': 'rel_AC'},\n",
        "    'B': {'C': 'rel_BC'},\n",
        "    'C': {'A':'rel_AC','B':'rel_BC', 'D':'rel_CD'},\n",
        "    'D': {'C':'rel_CD','D':'rel_DD'}\n",
        "}\n",
        "node_type_prob = {'A': 1, 'B': 1, 'C': 1, 'D': 1}\n",
        "num_nodes = 20\n",
        "graph = ws_graph(num_nodes, 3, 0.5)\n",
        "hetero_graph = generate_heterogeneous_small_world(graph, meta_graph, node_type_prob)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0sOim0bXdZ5"
      },
      "source": [
        "## Graph_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfJl6L56XEaN",
        "outputId": "399b625d-ec1f-48a7-9ffa-5f4b6a7b99c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['rel_AC', 'rel_BC', 'rel_CD', 'rel_DD']\n"
          ]
        }
      ],
      "source": [
        "# find unique relation types\n",
        "def unique_rel(meta_graph):\n",
        "  unique_elements = set()\n",
        "  for inner_dict in meta_graph.values():\n",
        "    for element in inner_dict.values():\n",
        "      unique_elements.add(element)\n",
        "\n",
        "  return unique_elements\n",
        "\n",
        "print(sorted(unique_rel(meta_graph)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XMQuON-zpUc",
        "outputId": "da9a95f2-073c-44cb-c783-f4122610d665"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EdgeView([(0, 1), (1, 2), (1, 8), (2, 16), (3, 4), (4, 5), (4, 12), (5, 6), (6, 16), (7, 8), (8, 13), (8, 16), (9, 10), (10, 19), (11, 12), (13, 15), (13, 19), (14, 15), (17, 18), (18, 19)])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph.edges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW_oeYMakLA2"
      },
      "source": [
        "$\\nabla(f)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Cgh2-WFEfbR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pos = nx.spring_layout(hetero_graph, seed=0)  # positions for all nodes\n",
        "\n",
        "# nodes\n",
        "options = {\"edgecolors\": \"tab:gray\", \"node_size\": 200, \"alpha\": 0.9}\n",
        "nx.draw_networkx_nodes(hetero_graph, pos, **options)\n",
        "\n",
        "# edges\n",
        "nx.draw_networkx_edges(hetero_graph, pos, width=1.0, alpha=0.5, edge_color=\"tab:red\",)\n",
        "\n",
        "labels = {i: graph.nodes[i]['type']+str(i) for i in range(len(graph.nodes))}\n",
        "nx.draw_networkx_labels(hetero_graph, pos, labels, font_size=5, font_color=\"whitesmoke\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLmE_0IIT1PS",
        "outputId": "7ff9100f-b187-44b9-80d7-e391ef3564e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20, 20, 10)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import itertools\n",
        "\n",
        "def adjacency_tensor(graph, meta_graph):\n",
        "    edge_types = ['rel_'+i+j for i,j in itertools.combinations_with_replacement(meta_graph.keys(), 2)]\n",
        "    n = len(graph.nodes())\n",
        "    r = len(edge_types)\n",
        "\n",
        "    adj_matrices = []\n",
        "\n",
        "    for edge_type in edge_types:\n",
        "        adj_matrix = np.zeros((n, n), dtype=np.int32)\n",
        "        \n",
        "        for u, v, data in graph.edges(data=True):\n",
        "            if data['type'] == edge_type:\n",
        "                adj_matrix[u, v] = 1\n",
        "                adj_matrix[v, u] = 1\n",
        "        \n",
        "        adj_matrices.append(adj_matrix)\n",
        "\n",
        "    edge_dict = {i: edge_types[i] for i in range(len(edge_types))}\n",
        "    return np.stack(adj_matrices, axis=-1), edge_dict\n",
        "\n",
        "adj_tensor, edge_dict = adjacency_tensor(hetero_graph, meta_graph)\n",
        "adj_tensor.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1gvg86mwrY-",
        "outputId": "64ac66a9-38e1-432f-e45f-463977359cf5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({0: 'rel_AA',\n",
              "  1: 'rel_AB',\n",
              "  2: 'rel_AC',\n",
              "  3: 'rel_AD',\n",
              "  4: 'rel_BB',\n",
              "  5: 'rel_BC',\n",
              "  6: 'rel_BD',\n",
              "  7: 'rel_CC',\n",
              "  8: 'rel_CD',\n",
              "  9: 'rel_DD'},\n",
              " {'A': {'C': 'rel_AC'},\n",
              "  'B': {'C': 'rel_BC'},\n",
              "  'C': {'A': 'rel_AC', 'B': 'rel_BC', 'D': 'rel_CD'},\n",
              "  'D': {'C': 'rel_CD', 'D': 'rel_DD'}})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "edge_dict, meta_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQHoxjkgPiwL",
        "outputId": "c3dd51c4-214f-45ec-99f8-896ae29ae932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "1000 100\n"
          ]
        }
      ],
      "source": [
        "def adj_to_cov(W_GT, num_nodes):\n",
        "  # W_GT = nx.adjacency_matrix(hetero_graph).todense()\n",
        "  weights = np.random.lognormal(0, 0.2, (num_nodes, num_nodes))\n",
        "  weights = (weights + weights.T) / 2\n",
        "  \n",
        "  W_GT = W_GT * weights\n",
        "  W_GT = W_GT * num_nodes / (np.sum(W_GT)+1e-10)\n",
        "  L_GT = np.diag(W_GT @ np.ones(num_nodes)) - W_GT\n",
        "  \n",
        "  cov_GT = np.linalg.inv(L_GT + (1e-1) * np.eye(num_nodes))\n",
        "  return cov_GT\n",
        "\n",
        "\n",
        "\n",
        "def generate_rel_signals(num_nodes, adj_tensor, sgl_dim):\n",
        "  noise_sigma = 1e-02\n",
        "  _ ,_, rel_num = adj_tensor.shape\n",
        "  print(rel_num)\n",
        "  emb_dim = sgl_dim * rel_num\n",
        "  print(emb_dim, sgl_dim)\n",
        "  signals_nodes = np.random.multivariate_normal(np.zeros(num_nodes), noise_sigma* np.eye(num_nodes), emb_dim)\n",
        "  signals_edges = np.zeros((rel_num, emb_dim))\n",
        "  \n",
        "  for rel in range(rel_num):\n",
        "    # the dimensions that are specific to relation type\n",
        "    cov = adj_to_cov(adj_tensor[:,:,rel], num_nodes)\n",
        "    # signals_rel = np.random.multivariate_normal(np.zeros(num_nodes), cov, sgl_dim)\n",
        "    # the dimension that is not relevant to relation type\n",
        "    signals_nodes[sgl_dim*rel:sgl_dim*(rel+1)] += np.random.multivariate_normal(np.zeros(num_nodes), cov, sgl_dim)\n",
        "    signals_edges[rel, sgl_dim*rel:sgl_dim*(rel+1)] = 1/sgl_dim\n",
        "  \n",
        "  return signals_nodes.T, signals_edges\n",
        "\n",
        "sgl_dim = 100\n",
        "signal_vtx, signals_edge = generate_rel_signals(num_nodes, adj_tensor, sgl_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LPjIuRU3-Ba"
      },
      "outputs": [],
      "source": [
        "adj_tensor[:,:,2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWUlAkPFvTLe"
      },
      "outputs": [],
      "source": [
        "np.linalg.inv(adj_to_cov(adj_tensor[:,:,2], num_nodes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hqoe5vKr2he"
      },
      "source": [
        "## From signal estimate Laplacian Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFELdOO26bcs"
      },
      "outputs": [],
      "source": [
        "def estimate_L(sgl_vtx, sgl_edge, dim):\n",
        "  out_product = sgl_vtx[:, np.newaxis, np.newaxis, :] * sgl_vtx[np.newaxis, : ,np.newaxis, :] * sgl_edge[ np.newaxis, np.newaxis, :,  :]\n",
        "  _,_,_,K = out_product.shape\n",
        "  L_hat = np.linalg.inv((np.sum(out_product, axis = 3)[:,:,dim]/K))/K\n",
        "  L_hat_undiag = L_hat *(1 - np.diag(L_hat))\n",
        "  threshold = 0.7*(np.max(L_hat_undiag) - np.min(L_hat_undiag))+ np.min(L_hat_undiag)\n",
        "  L_hat_undiag = (L_hat_undiag > threshold) * L_hat_undiag\n",
        "  L_recover = - np.diag(L_hat_undiag @ np.ones(num_nodes)) + L_hat_undiag\n",
        "  return L_recover\n",
        "\n",
        "estimate_L(signal_vtx, signals_edge, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yK-kRMvw3Q3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcO5vCZzw4BA"
      },
      "source": [
        "# Graph Sampling from huge heterogeneous graph\n",
        "\n",
        "Loading graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JAVnfyAiPXVJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24478dd6-568f-42e9-e7c1-fc149d5363f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.dropbox.com/s/yh4grpeks87ugr2/DBLP_processed.zip?dl=1\n",
            "Extracting data/dblp/raw/DBLP_processed.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import DBLP\n",
        "\n",
        "dataset_dblp = DBLP(root='./data/dblp')\n",
        "data = dataset_dblp[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random.mtrand import noncentral_chisquare\n",
        "def data_preprocessing(data):\n",
        "  # node type processing\n",
        "  # for node_type in data.node_types:\n",
        "  #   if data[node_type].x is None:\n",
        "  data['conference'].x = torch.eye(data['conference'].num_nodes)\n",
        "\n",
        "  # edge type processing\n",
        "  edge_onehot = torch.eye(len(data.edge_types))\n",
        "  i=0\n",
        "  for edge_type in data.edge_types:\n",
        "    data[edge_type].x = edge_onehot[i]\n",
        "    i+=1\n",
        "  return data\n",
        "data = data_preprocessing(data)"
      ],
      "metadata": {
        "id": "qAks4dMhQUIo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "rQhD76MVISr9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb285d37-c7bf-4dd3-f093-d29638c08163"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HeteroData(\n",
              "  \u001b[1mauthor\u001b[0m={\n",
              "    x=[4057, 334],\n",
              "    y=[4057],\n",
              "    train_mask=[4057],\n",
              "    val_mask=[4057],\n",
              "    test_mask=[4057]\n",
              "  },\n",
              "  \u001b[1mpaper\u001b[0m={ x=[14328, 4231] },\n",
              "  \u001b[1mterm\u001b[0m={ x=[7723, 50] },\n",
              "  \u001b[1mconference\u001b[0m={\n",
              "    num_nodes=20,\n",
              "    x=[20, 20]\n",
              "  },\n",
              "  \u001b[1m(author, to, paper)\u001b[0m={\n",
              "    edge_index=[2, 19645],\n",
              "    x=[6]\n",
              "  },\n",
              "  \u001b[1m(paper, to, author)\u001b[0m={\n",
              "    edge_index=[2, 19645],\n",
              "    x=[6]\n",
              "  },\n",
              "  \u001b[1m(paper, to, term)\u001b[0m={\n",
              "    edge_index=[2, 85810],\n",
              "    x=[6]\n",
              "  },\n",
              "  \u001b[1m(paper, to, conference)\u001b[0m={\n",
              "    edge_index=[2, 14328],\n",
              "    x=[6]\n",
              "  },\n",
              "  \u001b[1m(term, to, paper)\u001b[0m={\n",
              "    edge_index=[2, 85810],\n",
              "    x=[6]\n",
              "  },\n",
              "  \u001b[1m(conference, to, paper)\u001b[0m={\n",
              "    edge_index=[2, 14328],\n",
              "    x=[6]\n",
              "  }\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PnjCbMepfG3I"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import HGTLoader\n",
        "\n",
        "from torch_geometric.utils import to_scipy_sparse_matrix\n",
        "import torch_geometric.transforms as T\n",
        "import numpy as np\n",
        "\n",
        "loader = HGTLoader(\n",
        "  data,\n",
        "  # Sample 512 nodes per type and per iteration for 4 iterations\n",
        "  num_samples={key: [16] * 4 for key in data.node_types},\n",
        "  # Use a batch size of 128 for sampling training nodes of type paper\n",
        "  batch_size=8,\n",
        "  input_nodes=('paper'),\n",
        "  )\n",
        "\n",
        "sampled_hetero_data = next(iter(loader))\n",
        "def connected_graph(sampled_graph):\n",
        "  subsampled_G = sampled_graph.to_homogeneous()\n",
        "  largest_component = T.LargestConnectedComponents(num_components=1, connection = 'strong')(subsampled_G)\n",
        "  adj = to_scipy_sparse_matrix(subsampled_G.edge_index, num_nodes=subsampled_G.num_nodes)\n",
        "  return largest_component, adj\n",
        "\n",
        "connected_subsample, adj = connected_graph(sampled_hetero_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TWHBVXiljqdb"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "# from torch_geometric.data import dataset\n",
        "def sampling_hetero_graphs(hetero_graph, nnodes_per_type,graph_depth):\n",
        "  input_node = 'paper'\n",
        "  num_samples = 1000\n",
        "  rd_idx = np.random.choice(len(data[input_node].x),num_samples,replace=False)\n",
        "  \n",
        "  loader = HGTLoader(\n",
        "  hetero_graph,\n",
        "  # Sample 512 nodes per type and per iteration for 4 iterations\n",
        "  num_samples={key: [nnodes_per_type] * graph_depth for key in hetero_graph.node_types},\n",
        "  # Use a batch size of 128 for sampling training nodes of type paper\n",
        "  batch_size=1,\n",
        "  input_nodes=(input_node),\n",
        "  )\n",
        "  sub_graphs = []\n",
        "  for idx in rd_idx:\n",
        "    sampled_graph = loader.collate_fn(index=[idx])\n",
        "    connected_subsample, adj = connected_graph(loader.filter_fn(sampled_graph))\n",
        "    sub_graphs.append(connected_subsample)\n",
        "    # sub_graphs.append(sampled_graph)\n",
        "  graph_loader = DataLoader(sub_graphs, batch_size=4, shuffle=False)\n",
        "  return graph_loader\n",
        "\n",
        "data_loader = sampling_hetero_graphs(data, 16, 4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import HeteroConv, Linear, SAGEConv, HeteroDictLinear\n",
        "\n",
        "class LinearProj(torch.nn.Module):\n",
        "    def __init__(self, node_shape, edge_shape, out_channels):\n",
        "        super().__init__()\n",
        "        self.NodeLinear = HeteroDictLinear(in_channels=node_shape,out_channels=out_channels)\n",
        "        self.EdgeLinear = HeteroDictLinear(in_channels=edge_shape,out_channels=out_channels)\n",
        "\n",
        "    def forward(self, batch):\n",
        "      node_attrs = {node_type: batch[node_type].x for node_type in batch.node_types}\n",
        "      num_edges = {edge_type: len(batch[edge_type].e_id) for edge_type in batch.edge_types}\n",
        "      edge_attrs = {'_'.join(edge_type): batch[edge_type].x.repeat(num_edges[edge_type], 1) for edge_type in batch.edge_types}\n",
        "      node_out = self.NodeLinear(node_attrs)\n",
        "      edge_out = self.EdgeLinear(edge_attrs)\n",
        "      for node_type in batch.node_types:\n",
        "        node_out[node_type] = node_out[node_type]/(node_out[node_type].norm(dim=1)[:, None])\n",
        "      for edge_type in edge_attrs:\n",
        "        edge_out[edge_type] = edge_out[edge_type]/(edge_out[edge_type].norm(dim=1)[:, None])\n",
        "\n",
        "      return (node_out, edge_out)\n"
      ],
      "metadata": {
        "id": "DdkRrtaVA48P"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRtsLA8HhBLm",
        "outputId": "dc3a2973-a131-4ff6-e4ea-cf37a9f42bcc"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HeteroData(\n",
              "  \u001b[1mauthor\u001b[0m={\n",
              "    x=[31, 334],\n",
              "    y=[31],\n",
              "    train_mask=[31],\n",
              "    val_mask=[31],\n",
              "    test_mask=[31],\n",
              "    n_id=[31]\n",
              "  },\n",
              "  \u001b[1mpaper\u001b[0m={\n",
              "    x=[56, 4231],\n",
              "    n_id=[56],\n",
              "    input_id=[8],\n",
              "    batch_size=8\n",
              "  },\n",
              "  \u001b[1mterm\u001b[0m={\n",
              "    x=[64, 50],\n",
              "    n_id=[64]\n",
              "  },\n",
              "  \u001b[1mconference\u001b[0m={\n",
              "    num_nodes=9,\n",
              "    x=[9, 20],\n",
              "    n_id=[9]\n",
              "  },\n",
              "  \u001b[1m(author, to, paper)\u001b[0m={\n",
              "    edge_index=[2, 63],\n",
              "    x=[6],\n",
              "    e_id=[63]\n",
              "  },\n",
              "  \u001b[1m(paper, to, author)\u001b[0m={\n",
              "    edge_index=[2, 61],\n",
              "    x=[6],\n",
              "    e_id=[61]\n",
              "  },\n",
              "  \u001b[1m(paper, to, term)\u001b[0m={\n",
              "    edge_index=[2, 90],\n",
              "    x=[6],\n",
              "    e_id=[90]\n",
              "  },\n",
              "  \u001b[1m(paper, to, conference)\u001b[0m={\n",
              "    edge_index=[2, 4],\n",
              "    x=[6],\n",
              "    e_id=[4]\n",
              "  },\n",
              "  \u001b[1m(term, to, paper)\u001b[0m={\n",
              "    edge_index=[2, 135],\n",
              "    x=[6],\n",
              "    e_id=[135]\n",
              "  },\n",
              "  \u001b[1m(conference, to, paper)\u001b[0m={\n",
              "    edge_index=[2, 55],\n",
              "    x=[6],\n",
              "    e_id=[55]\n",
              "  }\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from torch_geometric.utils import negative_sampling\n",
        "\n",
        "def neg_sample(batch, s_ntype, s_idx, num_samples):\n",
        "  possible_edge = [(s_type, t_type) for s_type, _ , t_type in batch.edge_types if s_ntype in [s_type, t_type]]\n",
        "  # print(possible_edge)\n",
        "  # possible_edge += [(s_ntype, s_ntype)]\n",
        "  samples = []\n",
        "  for t_type in batch.node_types:\n",
        "    # print(t_type)\n",
        "    if s_ntype == t_type: continue;\n",
        "    if (s_ntype, t_type) not in possible_edge and (t_type, s_ntype) not in possible_edge:\n",
        "      sample = random.sample(range(batch[t_type].x.shape[0]), num_samples)\n",
        "      samples += [{t_type: val} for val in sample]\n",
        "    # print(samples)\n",
        "  \n",
        "  while len(samples) <= 2*num_samples:\n",
        "    # within type sampling\n",
        "    for t_type in batch.node_types:\n",
        "      if (s_ntype, t_type) in possible_edge:\n",
        "        # print((s_ntype, 'to', t_type))\n",
        "        edge_idx = batch[(s_ntype, 'to', t_type)].edge_index\n",
        "        # print(edge_idx)\n",
        "        # within_samples = negative_sampling(edge_idx, num_neg_samples=num_samples)\n",
        "        # print(edge_idx, s_idx)\n",
        "        indices = torch.where(edge_idx == s_idx)[0]\n",
        "        # Create a list of integers from 0 to 33, excluding 2, 5, and 10\n",
        "        numbers = [i for i in range(batch[t_type].x.shape[0]) if i not in indices.tolist()]\n",
        "        sampled_node = np.random.choice(numbers,num_samples,replace=False)\n",
        "        samples += [{t_type: idx} for idx in sampled_node]\n",
        "      elif (t_type, s_ntype) in possible_edge:\n",
        "        edge_idx = batch[(t_type, 'to', s_ntype)].edge_index\n",
        "        # within_samples = negative_sampling(edge_idx, num_neg_samples=num_samples)\n",
        "        indices = torch.where(edge_idx == s_idx)[0]\n",
        "        # Create a list of integers from 0 to 33, excluding 2, 5, and 10\n",
        "        numbers = [i for i in range(batch[t_type].x.shape[0]) if i not in indices.tolist()]\n",
        "        sampled_node = np.random.choice(numbers, num_samples, replace=False)\n",
        "        samples += [{t_type: idx} for idx in sampled_node]\n",
        "  random.shuffle(samples)\n",
        "  samples = samples[0:2*num_samples]\n",
        "  sampels_dict = {}\n",
        "  for t_type in batch.node_types:\n",
        "    element = []\n",
        "    for item in samples:\n",
        "      key = next(iter(item))\n",
        "      if key == t_type:\n",
        "        element += [item[key]]\n",
        "    sampels_dict[t_type] =  torch.LongTensor(element)\n",
        "\n",
        "  return sampels_dict\n"
      ],
      "metadata": {
        "id": "X86GvFMfdBZf"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in loader:\n",
        "  print(batch)\n",
        "  break"
      ],
      "metadata": {
        "id": "YvHKpXBDN5D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to('cpu')\n",
        "model(batch)[1]['paper_to_term'][:,None, :].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiLhvHoV6zxV",
        "outputId": "0f0cbb5f-6e60-4215-f368-77a31a44f1c2"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([90, 1, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_ns(embs, neg_embs, sigma = 0.01):\n",
        "  # loss with negative sampling\n",
        "  s_emb, r_emb, t_emb = embs\n",
        "  diff_vector = r_emb * (s_emb - t_emb)\n",
        "  smooth = torch.trace(diff_vector.t() @ diff_vector)\n",
        "  diff_neg = r_emb[None,:,:] * (s_emb[None,:,:] - neg_embs)\n",
        "  b, N, K = diff_neg.shape\n",
        "  # N \\times neg_sample \\times K -> N * neg_sample \\times K\n",
        "  inner_product = torch.bmm(diff_neg.permute(0,2,1), diff_neg)\n",
        "\n",
        "  smooth_neg = torch.trace(torch.exp( - 1/sigma * inner_product))\n",
        "  loss = 1/sigma * smooth + torch.log(smooth_neg)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "iGIS3gw65ky1"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, loader, device):\n",
        "    total_examples = total_loss = 0\n",
        "    \"\"\"\n",
        "    for batch in loader:\n",
        "      for edge_type in batch.edge_types:\n",
        "          if batch[edge_type].edge_index.shape[-1] ==0:\n",
        "            continue\n",
        "          s_ntype, _ , t_ntype = edge_type\n",
        "          # source node index and target node idx\n",
        "          s_idx, t_idx  = batch[edge_type].edge_index\n",
        "          for s in s_idx:\n",
        "            samples = neg_sample(batch, s_ntype, s, 5)\n",
        "    \"\"\"\n",
        "    step=0\n",
        "    for batch in loader:\n",
        "        step += 1\n",
        "        if step == 100:break;\n",
        "        optimizer.zero_grad()\n",
        "        batch_size = batch['paper'].batch_size\n",
        "        batch = batch.to(device)\n",
        "        node_out, edge_out = model(batch)\n",
        "        loss = 0\n",
        "        num_edges = 0\n",
        "        \n",
        "        for edge_type in batch.edge_types:\n",
        "          if batch[edge_type].edge_index.shape[-1] ==0:\n",
        "            continue\n",
        "          num_edges += batch[edge_type].edge_index.shape[1]\n",
        "          # source node type and target node type\n",
        "          s_ntype, _ , t_ntype = edge_type\n",
        "          # source node index and target node idx\n",
        "          s_idx, t_idx  = batch[edge_type].edge_index\n",
        "          # source node embedding and target node embedding\n",
        "          s_nemb, t_nemb = node_out[s_ntype][s_idx], node_out[t_ntype][t_idx]\n",
        "          # relation embedding\n",
        "          r_emb = edge_out['_'.join(edge_type)]\n",
        "          tensor_list = []\n",
        "          for s in s_idx:\n",
        "            samples = neg_sample(batch, s_ntype, s, 5)\n",
        "\n",
        "            neg_emb_list = []\n",
        "            for node_type, idx_tensor in samples.items():\n",
        "              if idx_tensor is not None:\n",
        "                neg_emb_list += [node_out[node_type][idx_tensor]]\n",
        "            tensor_list += [torch.cat(neg_emb_list, dim=0)]\n",
        "          \n",
        "          neg_emb = torch.stack(tensor_list, dim=0).permute(1,0,2)\n",
        "          \n",
        "          print(neg_emb.shape)\n",
        "          # print(s_nemb.shape, t_nemb.shape, r_emb.shape)\n",
        "          loss += loss_ns((s_nemb, r_emb, t_nemb), neg_emb)\n",
        "          \n",
        "          # diff_vector = r_emb * (s_nemb - t_nemb)\n",
        "          # loss += torch.trace(diff_vector.t() @ diff_vector)\n",
        "        # loss = loss / num_edges\n",
        "        print('Your bullshit training process: step {}, loss {}'.format(step, loss))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_examples += batch_size\n",
        "        total_loss += float(loss) * batch_size\n",
        "\n",
        "    return total_loss / total_examples"
      ],
      "metadata": {
        "id": "cynixjstWlZR"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = HGTLoader(\n",
        "  data,\n",
        "  # Sample 512 nodes per type and per iteration for 4 iterations\n",
        "  num_samples={key: [16] * 4 for key in data.node_types},\n",
        "  # Use a batch size of 128 for sampling training nodes of type paper\n",
        "  batch_size=8,\n",
        "  input_nodes=('paper'),\n",
        "  )\n",
        "device = 'cuda:0'\n",
        "node_shape = {node_type: data[node_type].x.shape[-1] for node_type in data.node_types}\n",
        "edge_shape = {'_'.join(edge_type): data[edge_type].x.shape[-1] for edge_type in data.edge_types}\n",
        "model = LinearProj(node_shape, edge_shape, 100)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
        "\n",
        "for epoch in range(1, 21):\n",
        "  model.train()\n",
        "  loss = train(model, optimizer, loader, device)\n",
        "  print('loss: {}, epoch: {}'.format(loss, epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "ikzVN0S0extJ",
        "outputId": "dfca292c-878d-4995-eed7-096098edabf1"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 61, 100])\n",
            "torch.Size([10, 61, 100])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-207-b348ce28f156>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss: {}, epoch: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-206-dbd51ee47f64>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loader, device)\u001b[0m\n\u001b[1;32m     49\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m           \u001b[0;31m# print(s_nemb.shape, t_nemb.shape, r_emb.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m           \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_nemb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_nemb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m           \u001b[0;31m# diff_vector = r_emb * (s_nemb - t_nemb)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-205-2a2b4ce548d1>\u001b[0m in \u001b[0;36mloss_ns\u001b[0;34m(embs, neg_embs, sigma)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff_neg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# N \\times neg_sample \\times K -> N * neg_sample \\times K\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mdiff_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiff_neg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0msmooth_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msigma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdiff_neg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mdiff_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msigma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msmooth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmooth_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "22yPa42lNwFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def estimate_L(batch):\n",
        "  out_product = sgl_vtx[:, np.newaxis, np.newaxis, :] * sgl_vtx[np.newaxis, : ,np.newaxis, :] * sgl_edge[ np.newaxis, np.newaxis, :,  :]\n",
        "  _,_,_,K = out_product.shape\n",
        "  L_hat = np.linalg.inv((np.sum(out_product, axis = 3)[:,:,dim]/K))/K\n",
        "  L_hat_undiag = L_hat *(1 - np.diag(L_hat))\n",
        "  threshold = 0.7*(np.max(L_hat_undiag) - np.min(L_hat_undiag))+ np.min(L_hat_undiag)\n",
        "  L_hat_undiag = (L_hat_undiag > threshold) * L_hat_undiag\n",
        "  L_recover = - np.diag(L_hat_undiag @ np.ones(num_nodes)) + L_hat_undiag\n",
        "  return L_recover\n",
        "\n",
        "estimate_L(signal_vtx, signals_edge, 2)"
      ],
      "metadata": {
        "id": "mtGMZeVVfNeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xQnqGi7doWRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U7ZEHHgXfN5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_type = batch.edge_types[0]\n",
        "_idx = batch[edge_type].edge_index\n",
        "node_out[edge_type[0]][edge_idx[0]].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SSdxgODUShW",
        "outputId": "a7fc83d6-e5de-4ecd-8a03-b6b9a45533ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([72, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "from torch_geometric.utils.convert import to_networkx\n",
        "\n",
        "sampled_graph = to_networkx(graph, to_undirected=True).copy()\n",
        "nx.is_connected(sampled_graph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4P_wgOmCF59",
        "outputId": "0c00329b-d8cb-42e3-cff9-4204cce02d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RP4WXq626tDf"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.sampler import (\n",
        "    BaseSampler,\n",
        "    HeteroSamplerOutput,\n",
        "    NodeSamplerInput,\n",
        "    SamplerOutput,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "input_data = NodeSamplerInput(\n",
        "            input_id=input_id,\n",
        "            node=input_nodes,\n",
        "            time=input_time,\n",
        "            input_type=input_type,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEw02Pydg1xi",
        "outputId": "c0645ac5-f42e-440e-ae6f-f9818bdb62e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Data(edge_index=[2, 819], y=[291], train_mask=[291], val_mask=[291], test_mask=[291], n_id=[291], input_id=[260], e_id=[819], node_type=[291], edge_type=[819])"
            ]
          },
          "execution_count": 188,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "connected_subsample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C8htE37KICW",
        "outputId": "23777891-8a76-4cf6-eef9-90191a365d49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 199,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsARKBE7gsql",
        "outputId": "0d42900f-a36f-41ba-db4e-82f2bae37e9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2., 3., 3., 2., 3., 2., 3., 2., 3., 2., 1., 3., 1., 3., 3., 2., 4.,\n",
              "       2., 4., 3., 1., 3., 1., 2., 2., 1., 3., 3., 2., 2., 3., 4., 3., 1.,\n",
              "       2., 1., 1., 2., 1., 1., 1., 1., 3., 2., 2., 3., 2., 2., 1., 1., 1.,\n",
              "       1., 3., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.,\n",
              "       1., 1., 2., 4., 7., 3., 4., 5., 3., 4., 6., 2., 5., 4., 4., 4., 6.,\n",
              "       3., 2., 3., 1., 4., 4., 2., 5., 3., 5., 3., 4., 2., 3., 4., 5., 3.,\n",
              "       2., 4., 2., 2., 4., 3., 3., 3., 2., 2., 3., 1., 3., 2., 1., 3., 2.,\n",
              "       2., 5., 4., 3., 2., 2., 3., 2., 3., 2., 2., 1., 3., 3., 3., 4., 4.,\n",
              "       6., 4., 3., 3., 2., 1., 5., 3., 4., 4., 4., 3., 3., 5., 2., 3., 2.,\n",
              "       3., 2., 2., 2., 4., 2., 3., 1., 2., 1., 1., 3., 1., 2., 2., 2., 4.,\n",
              "       1., 1., 3., 1., 3., 1., 1., 2., 2., 3., 3., 1., 2., 4., 2., 2., 3.,\n",
              "       2., 1., 2., 3., 1., 2., 1., 1., 2., 1., 1., 4., 1., 3., 2., 2., 4.,\n",
              "       1., 3., 1., 1., 4., 2., 3., 2., 2., 2., 3., 2., 1., 1., 1., 2., 1.,\n",
              "       2., 2., 1., 1., 1., 2., 2., 3., 3., 1., 1., 1., 2., 4., 1., 1., 2.,\n",
              "       2., 1., 2., 1., 1., 2., 2., 1., 4., 3., 2., 2., 2., 4., 1., 1., 2.,\n",
              "       2., 2., 1., 2., 1., 3., 2., 1., 2., 1., 2., 1., 2., 2., 2., 1., 1.,\n",
              "       5., 2., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 2., 1., 1., 3.,\n",
              "       2., 1.])"
            ]
          },
          "execution_count": 191,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "adj = nx.adjacency_matrix(sampled_graph)\n",
        "adj @ np.ones(adj.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5MAQRpxCuex",
        "outputId": "0728d042-d6f9-48b8-9c17-c3d31f943c91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://docs.google.com/uc?export=download&id=1Juwx8HtDwSzmVIJ31ooVa1WljI4U5JnA&confirm=t\n",
            "Downloading https://docs.google.com/uc?export=download&id=1Zy6BZH_zLEjKlEFSduKE5tV9qqA_8VtM&confirm=t\n",
            "Downloading https://docs.google.com/uc?export=download&id=1VUcBGr0T0-klqerjAjxRmAqFuld_SMWU&confirm=t\n",
            "Downloading https://docs.google.com/uc?export=download&id=1NI5pa5Chpd-52eSmLW60OnB3WS5ikxq_&confirm=t\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import Yelp\n",
        "\n",
        "dataset_yelp = Yelp(root='./data/yelp')\n",
        "data = dataset_yelp[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZMmysc-P7GZ",
        "outputId": "296da5e1-5c56-4335-8d69-cf5251a968f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "HeteroData(\n",
              "  \u001b[1mauthor\u001b[0m={\n",
              "    x=[4057, 334],\n",
              "    y=[4057],\n",
              "    train_mask=[4057],\n",
              "    val_mask=[4057],\n",
              "    test_mask=[4057]\n",
              "  },\n",
              "  \u001b[1mpaper\u001b[0m={ x=[14328, 4231] },\n",
              "  \u001b[1mterm\u001b[0m={ x=[7723, 50] },\n",
              "  \u001b[1mconference\u001b[0m={ num_nodes=20 },\n",
              "  \u001b[1m(author, to, paper)\u001b[0m={ edge_index=[2, 19645] },\n",
              "  \u001b[1m(paper, to, author)\u001b[0m={ edge_index=[2, 19645] },\n",
              "  \u001b[1m(paper, to, term)\u001b[0m={ edge_index=[2, 85810] },\n",
              "  \u001b[1m(paper, to, conference)\u001b[0m={ edge_index=[2, 14328] },\n",
              "  \u001b[1m(term, to, paper)\u001b[0m={ edge_index=[2, 85810] },\n",
              "  \u001b[1m(conference, to, paper)\u001b[0m={ edge_index=[2, 14328] }\n",
              ")"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pXEs1ZDPl7C",
        "outputId": "106df31a-49c7-452d-80b2-4d4ddefb6518"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['author', 'paper', 'term', 'conference']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.node_types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYQWLUBuE4hA",
        "outputId": "e51f361a-ba46-4097-d152-5d8a7d6c7957"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Data(x=[716847, 300], edge_index=[2, 13954819], y=[716847, 100], train_mask=[716847], val_mask=[716847], test_mask=[716847])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "hztECn9nSRjS",
        "outputId": "14f721e4-b57a-449b-fa0e-c25f13224fca"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-68c32fbf02bf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/data/hetero_data.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_dict$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         raise AttributeError(f\"'{self.__class__.__name__}' has no \"\n\u001b[0m\u001b[1;32m    138\u001b[0m                              f\"attribute '{key}'\")\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'HeteroData' has no attribute 'edge_weights'"
          ]
        }
      ],
      "source": [
        "data.edge_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5weNBoVJGBP"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import Flickr\n",
        "\n",
        "dataset_flickr = Flickr(root='./data/Flickr')\n",
        "data = dataset_flickr[0]\n",
        "row, col = data.edge_index\n",
        "data.edge_weight = 1. / degree(col, data.num_nodes)[col]  # Norm by in-degree.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4Z-X1b8J1TD",
        "outputId": "a0ff544e-e38d-4ef3-da26-4b26068690f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Data(x=[89250, 500], edge_index=[2, 899756], y=[89250], train_mask=[89250], val_mask=[89250], test_mask=[89250], edge_weight=[899756])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "JnjI6nUOQVvL",
        "outputId": "1aa92070-1d15-4c2c-face-03b23c05a526"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-a3f9872efa3a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m loader = GraphSAINTRandomWalkSampler(data, batch_size=60, walk_length=2,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                      \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_coverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                      \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                      num_workers=4)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/loader/graph_saint.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, batch_size, walk_length, num_steps, sample_coverage, save_dir, log, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m                  save_dir: Optional[str] = None, log: bool = True, **kwargs):\n\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwalk_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         super().__init__(data, batch_size, num_steps, sample_coverage,\n\u001b[0m\u001b[1;32m    210\u001b[0m                          save_dir, log, **kwargs)\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/loader/graph_saint.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, batch_size, num_steps, sample_coverage, save_dir, log, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'collate_fn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;34m'node_norm'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;34m'edge_norm'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/data/hetero_data.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_dict$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         raise AttributeError(f\"'{self.__class__.__name__}' has no \"\n\u001b[0m\u001b[1;32m    138\u001b[0m                              f\"attribute '{key}'\")\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'HeteroData' has no attribute 'edge_index'"
          ]
        }
      ],
      "source": [
        "loader = GraphSAINTRandomWalkSampler(data, batch_size=60, walk_length=2,\n",
        "                                     num_steps=5, sample_coverage=100,\n",
        "                                     save_dir=dataset.processed_dir,\n",
        "                                     num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NvsJkE4QV0U",
        "outputId": "3497ac1c-dd5e-4948-bb16-ab0adb51f4f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([176])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loader[2][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "KDa-VVF6Hhl6",
        "outputId": "314620e1-e100-448f-eb18-45ab564e7e31"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-34e6c4d6b3d6>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Norm by in-degree.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/data/hetero_data.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_dict$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         raise AttributeError(f\"'{self.__class__.__name__}' has no \"\n\u001b[0m\u001b[1;32m    138\u001b[0m                              f\"attribute '{key}'\")\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'HeteroData' has no attribute 'edge_index'"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.loader import GraphSAINTRandomWalkSampler\n",
        "from torch_geometric.nn import GraphConv\n",
        "from torch_geometric.typing import WITH_TORCH_SPARSE\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "row, col = data.edge_index\n",
        "data.edge_weight = 1. / degree(col, data.num_nodes)[col]  # Norm by in-degree.\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--use_normalization', action='store_true')\n",
        "args = parser.parse_args()\n",
        "\n",
        "loader = GraphSAINTRandomWalkSampler(data, batch_size=6000, walk_length=2,\n",
        "                                     num_steps=5, sample_coverage=100,\n",
        "                                     save_dir=dataset.processed_dir,\n",
        "                                     num_workers=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOXjEDiKEm6E"
      },
      "outputs": [],
      "source": [
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import IMDB\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "transform = T.ToUndirected()  # Add reverse edge types.\n",
        "data = IMDB(root='./data', transform=transform)[0]\n",
        "\n",
        "train_loader = NeighborLoader(\n",
        "    data,\n",
        "    # Sample 15 neighbors for each node and each edge type for 2 iterations:\n",
        "    num_neighbors=[3] * 2,\n",
        "    # Use a batch size of 128 for sampling training nodes of type \"paper\":\n",
        "    batch_size=128,\n",
        "    input_nodes=('director'),\n",
        ")\n",
        "\n",
        "batch = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "c_6Q_5A0Gf_t",
        "outputId": "e4405c51-13ee-42c0-af23-22f71596ebb7"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-1edc02ae0fde>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mto_networkx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/convert.py\u001b[0m in \u001b[0;36mto_networkx\u001b[0;34m(data, node_attrs, edge_attrs, graph_attrs, to_undirected, remove_self_loops)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_attrs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0medge_attrs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgraph_attrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'HeteroData' object is not callable"
          ]
        }
      ],
      "source": [
        "to_networkx(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-wrRlckF375",
        "outputId": "80810fe1-bfc3-458e-bea4-80633984d088"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "HeteroData(\n",
              "  \u001b[1mmovie\u001b[0m={\n",
              "    x=[225, 3066],\n",
              "    y=[225],\n",
              "    train_mask=[225],\n",
              "    val_mask=[225],\n",
              "    test_mask=[225],\n",
              "    n_id=[225],\n",
              "    num_sampled_nodes=[3]\n",
              "  },\n",
              "  \u001b[1mdirector\u001b[0m={\n",
              "    x=[128, 3066],\n",
              "    n_id=[128],\n",
              "    num_sampled_nodes=[3],\n",
              "    input_id=[128],\n",
              "    batch_size=128\n",
              "  },\n",
              "  \u001b[1mactor\u001b[0m={\n",
              "    x=[584, 3066],\n",
              "    n_id=[584],\n",
              "    num_sampled_nodes=[3]\n",
              "  },\n",
              "  \u001b[1m(movie, to, director)\u001b[0m={\n",
              "    edge_index=[2, 199],\n",
              "    e_id=[199],\n",
              "    num_sampled_edges=[2]\n",
              "  },\n",
              "  \u001b[1m(movie, to, actor)\u001b[0m={\n",
              "    edge_index=[2, 0],\n",
              "    e_id=[0],\n",
              "    num_sampled_edges=[2]\n",
              "  },\n",
              "  \u001b[1m(director, to, movie)\u001b[0m={\n",
              "    edge_index=[2, 225],\n",
              "    e_id=[225],\n",
              "    num_sampled_edges=[2]\n",
              "  },\n",
              "  \u001b[1m(actor, to, movie)\u001b[0m={\n",
              "    edge_index=[2, 675],\n",
              "    e_id=[675],\n",
              "    num_sampled_edges=[2]\n",
              "  },\n",
              "  \u001b[1m(director, rev_to, movie)\u001b[0m={\n",
              "    edge_index=[2, 225],\n",
              "    e_id=[225],\n",
              "    num_sampled_edges=[2]\n",
              "  },\n",
              "  \u001b[1m(actor, rev_to, movie)\u001b[0m={\n",
              "    edge_index=[2, 675],\n",
              "    e_id=[675],\n",
              "    num_sampled_edges=[2]\n",
              "  },\n",
              "  \u001b[1m(movie, rev_to, director)\u001b[0m={\n",
              "    edge_index=[2, 199],\n",
              "    e_id=[199],\n",
              "    num_sampled_edges=[2]\n",
              "  },\n",
              "  \u001b[1m(movie, rev_to, actor)\u001b[0m={\n",
              "    edge_index=[2, 0],\n",
              "    e_id=[0],\n",
              "    num_sampled_edges=[2]\n",
              "  }\n",
              ")"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF5f4amYDyUT",
        "outputId": "028ecce5-ba8c-4413-e075-4efdd80430f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "HeteroData(\n",
              "  \u001b[1mmovie\u001b[0m={\n",
              "    x=[4278, 3066],\n",
              "    y=[4278],\n",
              "    train_mask=[4278],\n",
              "    val_mask=[4278],\n",
              "    test_mask=[4278]\n",
              "  },\n",
              "  \u001b[1mdirector\u001b[0m={ x=[2081, 3066] },\n",
              "  \u001b[1mactor\u001b[0m={ x=[5257, 3066] },\n",
              "  \u001b[1m(movie, to, director)\u001b[0m={ edge_index=[2, 4278] },\n",
              "  \u001b[1m(movie, to, actor)\u001b[0m={ edge_index=[2, 12828] },\n",
              "  \u001b[1m(director, to, movie)\u001b[0m={ edge_index=[2, 4278] },\n",
              "  \u001b[1m(actor, to, movie)\u001b[0m={ edge_index=[2, 12828] }\n",
              ")"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4NoxTu7DNdl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBFxxB0l8ort"
      },
      "outputs": [],
      "source": [
        "from scipy import io\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "mat_file = io.loadmat('gdrive/MyDrive/heterograph_learning/ACM.mat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0opO0WU9H1Z"
      },
      "outputs": [],
      "source": [
        "paper_conf = mat_file['PvsC'].nonzero()[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVfwNdhx9H8H"
      },
      "outputs": [],
      "source": [
        "# DataBase\n",
        "paper_db = np.isin(paper_conf,[1,13])\n",
        "paper_db_idx = np.where(paper_db == True)[0]\n",
        "paper_db_idx = np.sort(np.random.choice(paper_db_idx,994,replace=False))\n",
        "# Data Mining\n",
        "paper_dm = np.isin(paper_conf,[0])\n",
        "paper_dm_idx = np.where(paper_dm == True)[0]\n",
        "# Wireless Communication\n",
        "paper_wc = np.isin(paper_conf,[9,10])\n",
        "paper_wc_idx = np.where(paper_wc == True)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be-MkWeE9IBb"
      },
      "outputs": [],
      "source": [
        "paper_idx = np.sort(list(paper_db_idx)+list(paper_dm_idx)+list(paper_wc_idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbpRAh819IDu"
      },
      "outputs": [],
      "source": [
        "# 0 : database, 1: wireless communication, 2: data mining\n",
        "paper_target = []\n",
        "for idx in paper_idx:\n",
        "    if idx in paper_db_idx:\n",
        "        paper_target.append(0)\n",
        "    elif idx in paper_wc_idx:\n",
        "        paper_target.append(1)\n",
        "    else:\n",
        "        paper_target.append(2)\n",
        "paper_target = np.array(paper_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Nzo7LkW9eLY"
      },
      "outputs": [],
      "source": [
        "authors = mat_file['PvsA'][paper_idx].nonzero()[1]\n",
        "author_dic = {}\n",
        "re_authors = []\n",
        "for author in authors:\n",
        "    if author not in author_dic:\n",
        "        author_dic[author] = len(author_dic) + len(paper_idx)\n",
        "    re_authors.append(author_dic[author])\n",
        "re_authors = np.array(re_authors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK2qH6k09mNr"
      },
      "outputs": [],
      "source": [
        "subjects = mat_file['PvsL'][paper_idx].nonzero()[1]\n",
        "subject_dic = {}\n",
        "re_subjects = []\n",
        "for subject in subjects:\n",
        "    if subject not in subject_dic:\n",
        "        subject_dic[subject] = len(subject_dic) + len(paper_idx) + len(author_dic)\n",
        "    re_subjects.append(subject_dic[subject])\n",
        "re_subjects = np.array(re_subjects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePExgFTH9mR1"
      },
      "outputs": [],
      "source": [
        "node_num = len(paper_idx) + len(author_dic) + len(subject_dic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHRMe5Ec9mUU"
      },
      "outputs": [],
      "source": [
        "papers = mat_file['PvsA'][paper_idx].nonzero()[0]\n",
        "data = np.ones_like(papers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqP41cMG9mWl"
      },
      "outputs": [],
      "source": [
        "A_pa = csr_matrix((data, (papers, re_authors)), shape=(node_num,node_num))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZpMeXWP9mYq"
      },
      "outputs": [],
      "source": [
        "papers = mat_file['PvsL'][paper_idx].nonzero()[0]\n",
        "data = np.ones_like(papers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akLKIybG9ma2"
      },
      "outputs": [],
      "source": [
        "A_ps = csr_matrix((data, (papers, re_subjects)), shape=(node_num,node_num))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLDUp1Lu9z17"
      },
      "outputs": [],
      "source": [
        "A_ap = A_pa.transpose()\n",
        "A_sp = A_ps.transpose()\n",
        "edges = [A_pa,A_ap,A_ps,A_sp]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-5IzEOR9z38",
        "outputId": "f8f708ad-fbff-40d8-f522-c922cb4ea68a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<12499x1903 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 972973 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "terms = mat_file['TvsP'].transpose()[paper_idx].nonzero()[1]\n",
        "term_dic = {}\n",
        "re_terms = []\n",
        "for term in terms:\n",
        "    if term not in term_dic:\n",
        "        term_dic[term] = len(term_dic) + len(paper_idx) + len(author_dic) + len(subject_dic)\n",
        "    re_terms.append(term_dic[term])\n",
        "re_terms = np.array(re_terms)\n",
        "mat_file['TvsP'].transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXG5NKsk9z58",
        "outputId": "44ad666e-3299-4878-c648-f1ed466652d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-18-829e80a76f57>:12: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  paper_feat = np.array(A_pt_tmp[:len(paper_idx),-len(term_dic):].toarray()>0, dtype=np.int)\n",
            "<ipython-input-18-829e80a76f57>:13: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  author_feat = np.array(A_pa_tmp.transpose().dot(A_pt_tmp)[len(paper_idx):len(paper_idx)+len(author_dic),-len(term_dic):].toarray()>0, dtype=np.int)\n",
            "<ipython-input-18-829e80a76f57>:14: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  subject_feat = np.array(A_ps_tmp.transpose().dot(A_pt_tmp)[len(paper_idx)+len(author_dic):len(paper_idx)+len(author_dic)+len(subject_dic),-len(term_dic):].toarray()>0, dtype=np.int)\n"
          ]
        }
      ],
      "source": [
        "# tmp\n",
        "tmp_num_node = node_num + len(term_dic)\n",
        "papers = mat_file['PvsA'][paper_idx].nonzero()[0]\n",
        "data = np.ones_like(papers)\n",
        "A_pa_tmp = csr_matrix((data, (papers, re_authors)), shape=(tmp_num_node,tmp_num_node))\n",
        "papers = mat_file['PvsL'][paper_idx].nonzero()[0]\n",
        "data = np.ones_like(papers)\n",
        "A_ps_tmp = csr_matrix((data, (papers, re_subjects)), shape=(tmp_num_node,tmp_num_node))\n",
        "papers = mat_file['PvsT'][paper_idx].nonzero()[0]\n",
        "data = np.ones_like(papers)\n",
        "A_pt_tmp = csr_matrix((data, (papers, re_terms)), shape=(tmp_num_node,tmp_num_node))\n",
        "paper_feat = np.array(A_pt_tmp[:len(paper_idx),-len(term_dic):].toarray()>0, dtype=np.int)\n",
        "author_feat = np.array(A_pa_tmp.transpose().dot(A_pt_tmp)[len(paper_idx):len(paper_idx)+len(author_dic),-len(term_dic):].toarray()>0, dtype=np.int)\n",
        "subject_feat = np.array(A_ps_tmp.transpose().dot(A_pt_tmp)[len(paper_idx)+len(author_dic):len(paper_idx)+len(author_dic)+len(subject_dic),-len(term_dic):].toarray()>0, dtype=np.int)\n",
        "node_faeture = np.concatenate((paper_feat,author_feat,subject_feat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yFgpZZN9z8P"
      },
      "outputs": [],
      "source": [
        "# Train, Valid\n",
        "train_valid_DB = list(np.random.choice(np.where(paper_target==0)[0],300, replace=False))\n",
        "train_valid_WC = list(np.random.choice(np.where(paper_target==1)[0],300, replace=False))\n",
        "train_valid_DM = list(np.random.choice(np.where(paper_target==2)[0],300, replace=False))\n",
        "\n",
        "train_idx = np.array(train_valid_DB[:200] + train_valid_WC[:200] + train_valid_DM[:200])\n",
        "train_target = paper_target[train_idx]\n",
        "train_label = np.vstack((train_idx,train_target)).transpose()\n",
        "valid_idx = np.array(train_valid_DB[200:] + train_valid_WC[200:] + train_valid_DM[200:])\n",
        "valid_target = paper_target[valid_idx]\n",
        "valid_label = np.vstack((valid_idx,valid_target)).transpose()\n",
        "test_idx = np.array(list((set(np.arange(paper_target.shape[0])) - set(train_idx)) - set(valid_idx)))\n",
        "test_target = paper_target[test_idx]\n",
        "test_label = np.vstack((test_idx,test_target)).transpose()\n",
        "labels = [train_label,valid_label,test_label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Knf0iScP9z-i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zxuvE3H90A2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "likW46aPw80S"
      },
      "source": [
        "## Random Walk with restart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lN_kWvoBw_xB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orGjrqHlVnri"
      },
      "source": [
        "## Generate Adjacancy Tensor from Hetero_graph object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv9oyv18B8ez"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s20RzTd6WX8Y",
        "outputId": "5c876a93-49d2-4279-ec70-efead8840ede"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['rel_AA',\n",
              " 'rel_AB',\n",
              " 'rel_AC',\n",
              " 'rel_AD',\n",
              " 'rel_BB',\n",
              " 'rel_BC',\n",
              " 'rel_BD',\n",
              " 'rel_CC',\n",
              " 'rel_CD',\n",
              " 'rel_DD']"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "edge_types = ['rel_'+i+j for i,j in itertools.combinations_with_replacement(meta_graph.keys(), 2)]\n",
        "edge_types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuWdV1mHI5jv",
        "outputId": "4b6d4117-8997-4e07-c1ef-8208a842f96c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50, 50, 10)"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def adjacency_tensor(graph, meta_graph):\n",
        "    edge_types = ['rel_'+i+j for i,j in itertools.combinations_with_replacement(meta_graph.keys(), 2)]\n",
        "    n = len(graph.nodes())\n",
        "    r = len(edge_types)\n",
        "\n",
        "    adj_matrices = []\n",
        "\n",
        "    for edge_type in edge_types:\n",
        "        adj_matrix = np.empty((n, n), dtype=object)\n",
        "        \n",
        "        for u, v, data in graph.edges(data=True):\n",
        "            if data['type'] == edge_type:\n",
        "                element = graph.nodes[u]['type'] + '_'+ graph.edges[u,v]['type'] +'_'+ graph.nodes[v]['type']\n",
        "                adj_matrix[u, v] = element\n",
        "                adj_matrix[v, u] = adj_matrix[u, v] \n",
        "        \n",
        "        adj_matrices.append(adj_matrix)\n",
        "\n",
        "    return np.stack(adj_matrices, axis=-1)\n",
        "\n",
        "adjacency_tensor_result_cat = adjacency_tensor(hetero_graph, meta_graph)\n",
        "adjacency_tensor_result_cat.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRU15cv5V2q2",
        "outputId": "9da432ac-1392-4181-b62c-e106c716e75a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50, 50, 10)"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def adjacency_tensor(graph, meta_graph):\n",
        "    edge_types = ['rel_'+i+j for i,j in itertools.combinations_with_replacement(meta_graph.keys(), 2)]\n",
        "    n = len(graph.nodes())\n",
        "    r = len(edge_types)\n",
        "\n",
        "    adj_matrices = []\n",
        "\n",
        "    for edge_type in edge_types:\n",
        "        adj_matrix = np.zeros((n, n), dtype=np.int32)\n",
        "        \n",
        "        for u, v, data in graph.edges(data=True):\n",
        "            if data['type'] == edge_type:\n",
        "                adj_matrix[u, v] = 1\n",
        "                adj_matrix[v, u] = 1\n",
        "        \n",
        "        adj_matrices.append(adj_matrix)\n",
        "\n",
        "    return np.stack(adj_matrices, axis=-1)\n",
        "\n",
        "adjacency_tensor_result = adjacency_tensor(hetero_graph, meta_graph)\n",
        "adjacency_tensor_result.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mBWDljuWaZD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from scipy.spatial.distance import squareform\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "import scipy.sparse as sparse\n",
        "from sklearn import metrics\n",
        "import scipy.stats\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "def halfvec_to_topo(w, threshold, device):\n",
        "    \"\"\"\n",
        "    from half vectorisation to matrix in batch way.\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size, l = w.size()\n",
        "    m = int((1 / 2) * (1 + math.sqrt(1 + 8 * l)))\n",
        "\n",
        "    # extract binary edge {0, 1}:\n",
        "    bw = (w.clone().detach() >= threshold).float().to(device)\n",
        "    E = torch.zeros((batch_size, m, m), dtype = w.dtype).to(device)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        E[i, :, :][np.triu_indices(m, 1)] = bw[i].clone().detach()\n",
        "        E[i, :, :] = E[i, :, :].T + E[i, :, :]\n",
        "\n",
        "    return E\n",
        "\n",
        "#%%\n",
        "\n",
        "def torch_sqaureform_to_matrix(w, device):\n",
        "    \"\"\"\n",
        "    from half vectorisation to matrix in batch way.\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size, l = w.size()\n",
        "    m = int((1 / 2) * (1 + math.sqrt(1 + 8 * l)))\n",
        "\n",
        "    E = torch.zeros((batch_size, m, m), dtype = w.dtype).to(device)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        E[i, :, :][np.triu_indices(m, 1)] = w[i].clone().detach()\n",
        "        E[i, :, :] = E[i, :, :].T + E[i, :, :]\n",
        "\n",
        "    return E\n",
        "\n",
        "#%%\n",
        "\n",
        "def torch_squareform_to_vector(A, device):\n",
        "    batch_size, m, _ = A.size()\n",
        "    l = int(m * (m - 1) / 2)\n",
        "\n",
        "    w = torch.zeros((batch_size, l), dtype = A.dtype).to(device)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        w[i, :] = A[i,:,:][np.triu_indices(m, 1)].clone().detach()\n",
        "\n",
        "    return w\n",
        "\n",
        "#%%\n",
        "\n",
        "def soft_threshold(w, eta):\n",
        "    '''\n",
        "    softthreshold function in a batch way.\n",
        "    '''\n",
        "    return (torch.abs(w) >= eta) * torch.sign(w) * (torch.abs(w) - eta)\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "def check_tensor(x, device):\n",
        "    if isinstance(x, np.ndarray) or type(x) in [int, float]:\n",
        "        x = torch.Tensor(x)\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x.to(device=device)\n",
        "    return x\n",
        "\n",
        "#%%\n",
        "\n",
        "def coo_to_sparseTensor(coo):\n",
        "    values = coo.data\n",
        "    indices = np.vstack((coo.row, coo.col))\n",
        "\n",
        "    i = torch.LongTensor(indices)\n",
        "    v = torch.FloatTensor(values)\n",
        "    shape = coo.shape\n",
        "\n",
        "    return torch.sparse.FloatTensor(i, v, torch.Size(shape)).to_dense()\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "def get_degree_operator(m):\n",
        "    ncols =int(m*(m - 1)/2)\n",
        "\n",
        "    I = np.zeros(ncols)\n",
        "    J = np.zeros(ncols)\n",
        "\n",
        "    k = 0\n",
        "    for i in np.arange(1, m):\n",
        "        I[k:(k + m - i)] = np.arange(i, m)\n",
        "        k = k + (m - i)\n",
        "\n",
        "    k = 0\n",
        "    for i in np.arange(1, m):\n",
        "        J[k: (k + m - i)] = i - 1\n",
        "        k = k + m - i\n",
        "\n",
        "    Row = np.tile(np.arange(0, ncols), 2)\n",
        "    Col = np.append(I, J)\n",
        "    Data = np.ones(Col.size)\n",
        "    St = sparse.coo_matrix((Data, (Row, Col)), shape=(ncols, m))\n",
        "    return St.T\n",
        "\n",
        "#%%\n",
        "\n",
        "def get_distance_halfvector(y):\n",
        "    n, _ = y.shape # m nodes, n observations\n",
        "    z = (1 / n) * euclidean_distances(y.T, squared=True)\n",
        "    # z.shape = m, m\n",
        "    return squareform(z, checks=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJt-JM-wWAVX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def _generate_SBM100noise_to_parallel(i, num_nodes, num_signals, graph_hyper, weighted, weight_scale = False):\n",
        "\n",
        "    size = [4, 2, 2, 13, 13, 15, 17, 3, 12, 10, 9]\n",
        "\n",
        "    p = graph_hyper\n",
        "    probs = [[0.95, p, p, p, p, p, p, p, p, p, p],\n",
        "             [p, 1, p, p, p, p, p, p, p, p, p],\n",
        "             [p, p, 1, p, p, p, p, p, p, p, p],\n",
        "             [p, p, p, 0.6, p, p, p, p, p, p, p],\n",
        "             [p, p, p, p, 0.6, p, p, p, p, p, p],\n",
        "             [p, p, p, p, p, 0.5, p, p, p, p, p],\n",
        "             [p, p, p, p, p, p, 0.5, p, p, p, p],\n",
        "             [p, p, p, p, p, p, p, 0.95, p, p, p],\n",
        "             [p, p, p, p, p, p, p, p, 0.65, p, p],\n",
        "             [p, p, p, p, p, p, p, p, p, 0.65, p],\n",
        "             [p, p, p, p, p, p, p, p, p, p, 0.65]]\n",
        "\n",
        "    G = nx.stochastic_block_model(size, probs)\n",
        "\n",
        "    W_GT = nx.adjacency_matrix(G).A\n",
        "\n",
        "    if weighted == 'uniform':\n",
        "        weights = np.random.uniform(0, 2, (num_nodes, num_nodes))\n",
        "        weights = (weights + weights.T) / 2\n",
        "        W_GT = W_GT * weights\n",
        "\n",
        "    if weighted == 'gaussian':\n",
        "        weights = np.random.normal(1, 0.05, (num_nodes, num_nodes))\n",
        "        weights = np.abs(weights)\n",
        "        weights = (weights + weights.T) / 2\n",
        "        W_GT = W_GT * weights\n",
        "\n",
        "    if weighted == 'lognormal':\n",
        "        weights = np.random.lognormal(0, 0.1, (num_nodes, num_nodes))\n",
        "        weights = (weights + weights.T) / 2\n",
        "        W_GT = W_GT * weights\n",
        "\n",
        "\n",
        "    if weight_scale:\n",
        "        W_GT = W_GT * num_nodes / np.sum(W_GT)\n",
        "\n",
        "    L_GT = np.diag(W_GT @ np.ones(num_nodes)) - W_GT\n",
        "\n",
        "    W_GT = scipy.sparse.csr_matrix(W_GT)\n",
        "\n",
        "    cov = np.linalg.inv(L_GT + (1e-06) * np.eye(num_nodes))\n",
        "    z = get_distance_halfvector(np.random.multivariate_normal(np.zeros(num_nodes), cov, num_signals))\n",
        "\n",
        "    return z, W_GT\n",
        "\n",
        "def generate_SBM100noise_parallel(num_samples, num_nodes, num_signals, graph_hyper, weighted, weight_scale):\n",
        "    n_cpu = multiprocess.cpu_count() - 2\n",
        "    pool = multiprocess.Pool(n_cpu)\n",
        "\n",
        "    z_multi, W_multi = zip(*pool.map(partial(_generate_SBM100noise_to_parallel,\n",
        "                                             num_nodes = num_nodes,\n",
        "                                             num_signals = num_signals,\n",
        "                                             graph_hyper = graph_hyper,\n",
        "                                             weighted = weighted,\n",
        "                                             weight_scale = weight_scale),\n",
        "                                     range(num_samples)))\n",
        "\n",
        "    result = {\n",
        "        'z': z_multi,\n",
        "        'W': W_multi\n",
        "    }\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXacU1GJWrr7"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from src.utils_data import *\n",
        "\n",
        "#%%\n",
        "\n",
        "graph_type = 'BA'\n",
        "edge_type = 'lognormal'\n",
        "graph_size = 500\n",
        "\n",
        "graph_hyper = 3\n",
        "\n",
        "data = generate_BA_parallel(num_samples=8064,\n",
        "                            num_signals=3000,\n",
        "                            num_nodes=graph_size,\n",
        "                            graph_hyper=graph_hyper,\n",
        "                            weighted=edge_type,\n",
        "                            weight_scale=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGZleE89pzrF"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3apSmsYKYyTV"
      },
      "source": [
        "## Blessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFxAgVNc0zOO"
      },
      "outputs": [],
      "source": [
        "# Divine beast bless no bug here! \n",
        "#         ┌─┐    ┌─┐\n",
        "#      ┌─┘ ┴───┘ ┴──┐\n",
        "#      │                   │\n",
        "#      │       ───       │\n",
        "#      │  ─┬┘     └┬─  │\n",
        "#      │                   │\n",
        "#      │       ─┴─       │\n",
        "#      │                   │\n",
        "#      └─┐         ┌───┘\n",
        "#          │         │\n",
        "#          │         │\n",
        "#          │         │\n",
        "#          │         └──────────────┐\n",
        "#          │                                  │\n",
        "#          │                                  ├─┐\n",
        "#          │                                  ┌─┘\n",
        "#          │                                  │\n",
        "#          └─┐  ┐  ┌──────┬──┐  ┌──┘\n",
        "#            │  ─┤ ─┤         │  ─┤ ─┤\n",
        "#            └──┴──┘         └──┴──┘"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDym5o3f-luP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QHwjrjzDgpq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bblmMuCaDhvA",
        "outputId": "754c5471-7c3c-4359-ac06-87372921d397"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[5, 9, 2],\n",
              "         [5, 0, 0],\n",
              "         [5, 6, 7],\n",
              "         [0, 6, 8]],\n",
              "\n",
              "        [[1, 6, 6],\n",
              "         [6, 1, 8],\n",
              "         [0, 8, 4],\n",
              "         [7, 6, 4]],\n",
              "\n",
              "        [[0, 2, 8],\n",
              "         [3, 8, 3],\n",
              "         [5, 4, 8],\n",
              "         [5, 8, 2]]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "W = torch.randint(10, (3, 4, 3))\n",
        "W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcuPnkjcDoGM",
        "outputId": "26b77c93-1633-4b28-9eb5-fc6214a4cf80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[4, 4, 0, 2, 3, 0, 4, 2, 2, 1],\n",
              "        [4, 0, 3, 1, 2, 2, 0, 0, 2, 4],\n",
              "        [0, 3, 4, 3, 2, 0, 4, 0, 3, 3],\n",
              "        [4, 2, 1, 2, 4, 3, 4, 0, 2, 3]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "E = torch.randint(5, (4, 10))\n",
        "E"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zET6oFIdEDbE",
        "outputId": "3ae57f8d-fdc3-41c5-973e-25c0789df845"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[2, 0, 3, 3, 3, 1, 1, 0, 1, 4],\n",
              "        [2, 2, 3, 3, 0, 4, 2, 0, 0, 3],\n",
              "        [1, 2, 0, 1, 0, 4, 0, 4, 0, 2]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = torch.randint(5, (3, 10))\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3I2Ra8eJYBx"
      },
      "outputs": [],
      "source": [
        "d_e = 10\n",
        "d_r = 20\n",
        "N = 6\n",
        "W = torch.randint(10, (d_e, d_r, d_e))\n",
        "X = torch.randint(5, (N, d_e))\n",
        "E = torch.randint(3, (N, d_r))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_hbOSu4GHAb"
      },
      "outputs": [],
      "source": [
        "def tucker_decomp(w, h, r, t, ent_num ,rel_mum ):\n",
        "\n",
        "\n",
        "  w = w.view(1, ent_num, rel_mum, ent_num)\n",
        "  r = r.view(-1, 1, 1, rel_mum)\n",
        "  print(w.shape, r.shape)\n",
        "  wr = r @ w\n",
        "  print(wr.shape)\n",
        "\n",
        "  # compute whr = DO(BN(h_n x_1 wr))\n",
        "  wr = wr.view(-1, ent_num, ent_num)\n",
        "  print(wr.shape)\n",
        "  print(h.shape)\n",
        "  whr = (h @ wr)\n",
        "\n",
        "  # Compute whr x_3 t\n",
        "  tensor = whr * t\n",
        "  scores = torch.sum(whr * t, dim=-1)\n",
        "  return tensor, scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NJQ9Gt-FCYq",
        "outputId": "eb1f1cd5-756f-4c1a-f491-6283d00f9e46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 10, 20, 10]) torch.Size([6, 1, 1, 20])\n",
            "torch.Size([6, 10, 1, 10])\n",
            "torch.Size([6, 10, 10])\n",
            "torch.Size([6, 10])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([6, 6, 10])"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tensor, scores = tucker_decomp(W, X, E, X, d_e, d_r)\n",
        "tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL86IH28Gule",
        "outputId": "cfb886ce-e248-4239-d1c1-c96e4b62f0b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 3])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKnRdxKsKHTQ",
        "outputId": "31032475-d9e3-4e9d-cab2-dafc214bb0be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 20, 10]) torch.Size([10, 6])\n",
            "torch.Size([6, 20, 10])\n",
            "torch.Size([6, 1, 20])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([6, 1])"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def score(W, X, E, d_e, d_r):\n",
        "  lhs = X\n",
        "  rel = E\n",
        "  rhs = X\n",
        "  print(W.transpose(0, 2).shape, lhs.transpose(0, 1).shape)\n",
        "  lhs_proj = torch.matmul(W.transpose(0, 2), lhs.transpose(0, 1)).transpose(0, 2) # b, rank_r, rank_e\n",
        "  print(lhs_proj.shape)\n",
        "  rel_proj = rel.view(-1, 1, d_r)\n",
        "  print(rel_proj.shape)\n",
        "  lhs_proj = torch.bmm(rel_proj, lhs_proj).view(-1, d_e)\n",
        "  return torch.sum(lhs_proj * rhs, 1, keepdim=True)\n",
        "\n",
        "score(W, X, E, d_e, d_r).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taTgqgGXOCQl",
        "outputId": "24a1be7e-b817-4273-a96b-39f4b014a528"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "W.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRVG_lNaK4DY"
      },
      "outputs": [],
      "source": [
        "def p_mode_tensor_mat(Tsr, Mtx, p):\n",
        "  Kp = Tsr.shape[p-1]\n",
        "  K = Mtx.shape[0]\n",
        "  # Tsr K1 x K2 x Kp x ... x Kn; Mtx K x Kp\n",
        "\n",
        "  for i in range(K):\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CSOCnNHOTvP"
      },
      "outputs": [],
      "source": [
        "def mode_n_product(x, m, mode):\n",
        "  x = np.asarray(x)\n",
        "  m = np.asarray(m)\n",
        "  if mode <= 0 or mode % 1 != 0:\n",
        "    raise ValueError('`mode` must be a positive interger')\n",
        "  if x.ndim < mode:\n",
        "    raise ValueError('Invalid shape of X for mode = {}: {}'.format(mode, x.shape))\n",
        "  if m.ndim != 2:\n",
        "    raise ValueError('Invalid shape of M: {}'.format(m.shape))\n",
        "  return np.swapaxes(np.swapaxes(x, mode - 1, -1).dot(m.T), mode - 1, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcwcQMxEuF_5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def mode_n_product(x, m, mode):\n",
        "    if mode <= 0 or mode % 1 != 0:\n",
        "        raise ValueError('`mode` must be a positive integer')\n",
        "    if x.ndim < mode:\n",
        "        raise ValueError('Invalid shape of X for mode = {}: {}'.format(mode, x.shape))\n",
        "    if m.ndim != 2:\n",
        "        raise ValueError('Invalid shape of M: {}'.format(m.shape))\n",
        "    return torch.transpose(torch.transpose(x, mode - 1, -1).matmul(m.T), mode - 1, -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcqlC7vFuKXv"
      },
      "outputs": [],
      "source": [
        "N_e = 6\n",
        "N_r = 10\n",
        "emb_size = 100\n",
        "W = torch.randint(10, (N_e, N_r, N_e))\n",
        "X = torch.randint(5, (emb_size, N_e))\n",
        "E = torch.randint(3, (emb_size, N_r))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0iKtdMtuLLi",
        "outputId": "76178a0c-1a2f-4d09-807c-c5fde1611d34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([100, 100, 100])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X1 = mode_n_product(W, X, 1)\n",
        "X2 = mode_n_product(X1, E, 2)\n",
        "X3 = mode_n_product(X2, X, 3)\n",
        "\n",
        "X3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFUmCPEvumSX",
        "outputId": "c0459ad3-4cde-475b-dab8-1fb07a53198a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(707375)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "smooth = 0\n",
        "for i in range(N_e):\n",
        "  for j in range(N_r):\n",
        "    for k in range(N_e):\n",
        "      smooth += torch.sum(W[i,j,k] * X[:,i]* E[:,j] * X[:,k])\n",
        "\n",
        "smooth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG5v5BVZusYJ",
        "outputId": "f65f9c2a-93d1-4168-ac72-d63f2c62f2c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(707375)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dig = 0\n",
        "for i in range(emb_size):\n",
        "  dig += X3[i,i,i]\n",
        "\n",
        "dig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUqWokncujRo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhtQ9a0hPlDp"
      },
      "outputs": [],
      "source": [
        "X1 = mode_n_product(W, X, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dow8FjyzPoAT"
      },
      "outputs": [],
      "source": [
        "X2 = mode_n_product(X1, E, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4QW2gmSP5uL"
      },
      "outputs": [],
      "source": [
        "X3 = mode_n_product(X2, X, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27NlSdrlP9FG",
        "outputId": "59ff10d4-56e0-478d-b1a9-e026bf2b43e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6, 6, 6)"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGadqoZpP_Xc"
      },
      "outputs": [],
      "source": [
        "N_e = 6\n",
        "N_r = 10\n",
        "emb_size = 100\n",
        "W = torch.randint(10, (N_e, N_r, N_e))\n",
        "X = torch.randint(5, (emb_size, N_e))\n",
        "E = torch.randint(3, (emb_size, N_r))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLWBtIf8QSA4",
        "outputId": "ff29995f-9280-4372-e4f1-6afae94c857b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100, 100, 100)"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X1 = mode_n_product(W, X, 1)\n",
        "X2 = mode_n_product(X1, E, 2)\n",
        "X3 = mode_n_product(X2, X, 3)\n",
        "\n",
        "X3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiVwmR0dQVkR",
        "outputId": "738b53fd-ad4c-47d4-d18e-f7805f10ba4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "752439"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dig = 0\n",
        "for i in range(emb_size):\n",
        "  dig += X3[i,i,i]\n",
        "\n",
        "dig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYJOZdQoRbA0",
        "outputId": "3d8d7047-cd08-43d2-aecc-1c16d1b86db2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10])"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[1].shape\n",
        "E[3].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flvm_R_ERY6_",
        "outputId": "ffe3775e-10db-4f21-9e93-5d7c5bbf1645"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0)"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "W[1,3,2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkU5F6WyRTQ1",
        "outputId": "94a1b504-1fb2-40c2-d826-0988a5bafb69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 9,  0,  6,  0,  0,  0,  0, 16,  0,  4,  0,  0,  0,  0, 32,  0,  0, 18,\n",
              "         4,  6,  2, 16,  0, 18, 16,  0,  0,  0,  8,  3,  0,  0,  0,  0,  9,  0,\n",
              "         0,  4,  0,  0,  8,  4,  2,  0, 16,  3,  4,  8,  8, 16,  0,  0,  0,  0,\n",
              "         0,  0,  9,  3,  0,  8,  0, 16,  2,  6,  0,  0,  0,  4,  0,  0,  0,  0,\n",
              "         6, 12, 32, 18,  0, 24, 16,  0,  0,  0, 12,  0,  0, 24,  0,  8,  0,  0,\n",
              "         0,  0, 16,  0,  0, 12,  0,  0,  0,  0])"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "W[1,3,2] * X[:,1]* E[:,3] * X[:,4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0l724d9Qrvj",
        "outputId": "13d02008-508a-4f9e-b38b-872fd8daf262"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(752439)"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "smooth = 0\n",
        "for i in range(N_e):\n",
        "  for j in range(N_r):\n",
        "    for k in range(N_e):\n",
        "      smooth += torch.sum(W[i,j,k] * X[:,i]* E[:,j] * X[:,k])\n",
        "\n",
        "smooth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGbFej-eRsOk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaryBall/CSML_notes/blob/master/hgsl_e2e.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0YeI-TqptCv",
        "outputId": "0af4ad91-1d94-4717-cd31-7930f54b12c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "# mount the google drive to colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIugSBZApzSk",
        "outputId": "9f784e56-8db8-4b60-dc43-d185251dc0c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n",
            "11.8\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIUV8oebq9kh",
        "outputId": "0668ee3c-0662-4bf3-c9fc-eea4cbd58fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910459 sha256=cfa77101ed506efd49d819b992e5d1254ffcd428b9481424763277dd8a0eb913\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/pyg_lib-0.2.0%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.17%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_cluster-1.6.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (884 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m884.9/884.9 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.22.4)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
            "Successfully installed pyg_lib-0.2.0+pt20cu118 torch_cluster-1.6.1+pt20cu118 torch_scatter-2.1.1+pt20cu118 torch_sparse-0.6.17+pt20cu118 torch_spline_conv-1.2.2+pt20cu118\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "# !pip install rdkit\n",
        "# !pip install hydra-core wandb hydra-core ray ray-lightning torchmetrics overrides imageio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loNXffqWXpGa"
      },
      "source": [
        "# Synthetic data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoV2Hf-GXoj0",
        "outputId": "897d9e9a-de7e-4510-c449-117e7c9dfd39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All binary combinations: [('A', 'A'), ('A', 'B'), ('A', 'C'), ('A', 'D'), ('B', 'B'), ('B', 'C'), ('B', 'D'), ('C', 'C'), ('C', 'D'), ('D', 'D')]\n",
            "Sampled subset (40%): [('A', 'C'), ('A', 'B'), ('B', 'B'), ('B', 'D')]\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import random\n",
        "\n",
        "# Define the nodes\n",
        "nodes = ['A', 'B', 'C', 'D']\n",
        "\n",
        "# Generate all binary combinations\n",
        "combinations = list(itertools.combinations_with_replacement(nodes, 2))\n",
        "\n",
        "# Calculate 40% of the total combinations\n",
        "sample_size = round(len(combinations) * 0.4)\n",
        "\n",
        "# Randomly sample a subset of the combinations\n",
        "subset = random.sample(combinations, sample_size)\n",
        "\n",
        "# Print the results\n",
        "print(\"All binary combinations:\", combinations)\n",
        "print(\"Sampled subset (40%):\", subset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zN617cw9Hl-"
      },
      "source": [
        "# Function for generating Heterogeneous Graphs \n",
        "\n",
        "- with Small world assumption - Watts Strogatz Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "id": "SE4mRj1vguCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5458452-8329-44bc-baf6-da58ab4508c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'A': 0.25, 'B': 0.25, 'C': 0.25, 'D': 0.25}\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
            "deque([0])\n",
            "C ['A', 'B', 'D']\n",
            "sampled node type for node 1: B\n",
            "deque([1])\n",
            "B ['C']\n",
            "sampled node type for node 2: C\n",
            "deque([2])\n",
            "C ['A', 'B', 'D']\n",
            "sampled node type for node 3: A\n",
            "deque([3])\n",
            "A ['C']\n",
            "sampled node type for node 11: C\n",
            "deque([11])\n",
            "C ['A', 'B', 'D']\n",
            "sampled node type for node 10: A\n",
            "C ['A', 'B', 'D']\n",
            "sampled node type for node 12: B\n",
            "deque([10, 12])\n",
            "deque([12])\n",
            "B ['C']\n",
            "sampled node type for node 9: C\n",
            "deque([9])\n",
            "C ['A', 'B', 'D']\n",
            "sampled node type for node 8: A\n",
            "deque([8])\n",
            "A ['C']\n",
            "sampled node type for node 7: C\n",
            "deque([7])\n",
            "C ['A', 'B', 'D']\n",
            "sampled node type for node 6: B\n",
            "deque([6])\n",
            "B ['C']\n",
            "sampled node type for node 5: C\n",
            "deque([5])\n",
            "C ['A', 'B', 'D']\n",
            "sampled node type for node 4: B\n",
            "C ['A', 'B', 'D']\n",
            "sampled node type for node 14: D\n",
            "deque([4, 14])\n",
            "deque([14])\n",
            "D ['C', 'D']\n",
            "sampled node type for node 13: C\n",
            "deque([13])\n",
            "0 1 C B ('B', 'to', 'C')\n",
            "1 2 B C ('B', 'to', 'C')\n",
            "2 3 C A ('A', 'to', 'C')\n",
            "3 11 A C ('A', 'to', 'C')\n",
            "4 5 B C ('B', 'to', 'C')\n",
            "5 6 C B ('B', 'to', 'C')\n",
            "5 14 C D ('C', 'to', 'D')\n",
            "6 7 B C ('B', 'to', 'C')\n",
            "7 8 C A ('A', 'to', 'C')\n",
            "8 9 A C ('A', 'to', 'C')\n",
            "9 12 C B ('B', 'to', 'C')\n",
            "10 11 A C ('A', 'to', 'C')\n",
            "10 12 A B NA\n",
            "11 12 C B ('B', 'to', 'C')\n",
            "13 14 C D ('C', 'to', 'D')\n"
          ]
        }
      ],
      "source": [
        "from IPython.terminal.embed import ultratb\n",
        "import networkx as nx\n",
        "import random\n",
        "from collections import deque\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "\n",
        "def ws_graph (n,k,p):\n",
        "  return nx.watts_strogatz_graph(n, k, p)\n",
        "\n",
        "def get_available_node_types(current_node, graph, meta_graph):\n",
        "    if current_node is not None:\n",
        "      current_node_type = graph.nodes[current_node].get('type')\n",
        "    else:\n",
        "      current_node_type = None\n",
        "    \n",
        "    # process the case when we randomly select a node to start\n",
        "    if not current_node_type:\n",
        "        return list(meta_graph.keys())\n",
        "    else:\n",
        "        return list(meta_graph[current_node_type].keys())\n",
        "\n",
        "\n",
        "def normalize_node_type_prob(available_types, node_type_prob):\n",
        "    total_prob = sum(node_type_prob[available_types].values())\n",
        "    node_type_prob_normalized = {}\n",
        "    for node_type, prob in node_type_prob.items():\n",
        "      normalized_prob = prob / total_prob\n",
        "      node_type_prob_normalized[node_type] = normalized_prob\n",
        "\n",
        "    return node_type_prob_normalized\n",
        "\n",
        "\n",
        "\n",
        "def generate_heterogeneous_small_world(graph, meta_graph, node_type_prob):\n",
        "    # Step 1: Generate a Watts-Strogatz graph\n",
        "    \n",
        "\n",
        "    # Step 2: Normalize node type probabilities\n",
        "    node_type_prob_normalized = {node_type: prob / sum(node_type_prob.values()) for node_type, prob in node_type_prob.items()}\n",
        "    print(node_type_prob_normalized)\n",
        "\n",
        "    # Step 3: Assign node and edge types using BFS\n",
        "    visited = set()\n",
        "    print(graph.nodes())\n",
        "    current_node = None\n",
        "    last_node = None\n",
        "\n",
        "    possible_rel = [(i,j) for i in meta_graph.keys() for j in meta_graph[i].keys()]\n",
        "\n",
        "    for node in graph.nodes():\n",
        "        if node not in visited:\n",
        "            # BFS traversal\n",
        "            queue = deque([node])\n",
        "            \n",
        "            visited.add(node)\n",
        "\n",
        "            while queue:\n",
        "                print(queue)\n",
        "            \n",
        "                current_node = queue.popleft()\n",
        "                \n",
        "                if not graph.nodes[current_node].get('type'):\n",
        "                  possible_start_type = get_available_node_types(current_node, graph, meta_graph)\n",
        "                  sample_prob = [node_type_prob_normalized[node_type] for node_type in possible_start_type]\n",
        "                  node_type = random.choices(possible_start_type, sample_prob, k=1)[0]\n",
        "                  graph.nodes[current_node]['type'] = node_type\n",
        "\n",
        "                available_node_types = get_available_node_types(current_node, graph, meta_graph)\n",
        "\n",
        "                # Assign edge types and add unvisited neighbors to queue\n",
        "                for neighbor in graph.neighbors(current_node):\n",
        "                    if neighbor not in visited:\n",
        "                        # Assign node type\n",
        "                        if not graph.nodes[neighbor].get('type'):\n",
        "                            sample_prob = [node_type_prob_normalized[node_type] for node_type in available_node_types]\n",
        "                            node_type = random.choices(available_node_types, sample_prob, k=1)[0]\n",
        "\n",
        "                            print(graph.nodes[current_node].get('type'), available_node_types)\n",
        "\n",
        "                            print(\"sampled node type for node {vnumber}: {vtype}\".format(vnumber = neighbor, vtype = node_type))\n",
        "                            graph.nodes[neighbor]['type'] = node_type\n",
        "\n",
        "                        queue.append(neighbor)\n",
        "                        visited.add(neighbor)\n",
        "\n",
        "                last_node = current_node\n",
        "            \n",
        "    for u, v  in graph.edges():\n",
        "      if (graph.nodes[u]['type'],graph.nodes[v]['type']) in possible_rel:\n",
        "        print(u,v, graph.nodes[u]['type'], graph.nodes[v]['type'], meta_graph[graph.nodes[u]['type']][graph.nodes[v]['type']])\n",
        "        graph.edges[u, v]['type'] = meta_graph[graph.nodes[u]['type']][graph.nodes[v]['type']]\n",
        "      else:\n",
        "        print(u,v, graph.nodes[u]['type'], graph.nodes[v]['type'], 'NA')\n",
        "        graph.edges[u, v]['type'] = 'NA'\n",
        "    \n",
        "    return graph\n",
        "\n",
        "meta_graph = {\n",
        "    'A': {'C': ('A', 'to', 'C')},\n",
        "    'B': {'C': ('B', 'to', 'C')},\n",
        "    'C': {'A': ('A', 'to', 'C'),'B':('B', 'to', 'C'), 'D':('C', 'to', 'D')},\n",
        "    'D': {'C':('C', 'to', 'D'),'D':('D', 'to', 'D')}\n",
        "}\n",
        "node_type_prob = {'A': 1, 'B': 1, 'C': 1, 'D': 1}\n",
        "num_nodes = 15\n",
        "graph = ws_graph(num_nodes, 3, 0.5)\n",
        "hetero_graph = generate_heterogeneous_small_world(graph, meta_graph, node_type_prob)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0sOim0bXdZ5"
      },
      "source": [
        "## Graph_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfJl6L56XEaN",
        "outputId": "2968237b-46fc-4d79-874e-5ea7f83df8be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 'to', 'C'), ('B', 'to', 'C'), ('C', 'to', 'D'), ('D', 'to', 'D')]\n"
          ]
        }
      ],
      "source": [
        "# find unique relation types\n",
        "def unique_rel(meta_graph):\n",
        "  unique_elements = set()\n",
        "  for inner_dict in meta_graph.values():\n",
        "    for element in inner_dict.values():\n",
        "      unique_elements.add(element)\n",
        "\n",
        "  return unique_elements\n",
        "\n",
        "print(sorted(unique_rel(meta_graph)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW_oeYMakLA2"
      },
      "source": [
        "$\\nabla(f)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "id": "5Cgh2-WFEfbR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "aa19e204-98a5-48f6-89ad-8b7815a8fb9a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pos = nx.spring_layout(hetero_graph, seed=0)  # positions for all nodes\n",
        "\n",
        "# nodes\n",
        "options = {\"edgecolors\": \"tab:gray\", \"node_size\": 200, \"alpha\": 0.9}\n",
        "nx.draw_networkx_nodes(hetero_graph, pos, **options)\n",
        "\n",
        "# edges\n",
        "nx.draw_networkx_edges(hetero_graph, pos, width=1.0, alpha=0.5, edge_color=\"tab:red\",)\n",
        "\n",
        "labels = {i: graph.nodes[i]['type']+str(i) for i in range(len(graph.nodes))}\n",
        "nx.draw_networkx_labels(hetero_graph, pos, labels, font_size=5, font_color=\"whitesmoke\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.axis(\"off\")     \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLmE_0IIT1PS",
        "outputId": "7b8564e2-8824-4081-87cb-5367352ff3ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 'to', 'C'), ('B', 'to', 'C'), ('C', 'to', 'D'), ('D', 'to', 'D')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 15, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ],
      "source": [
        "import itertools\n",
        "\n",
        "def adjacency_tensor(graph, meta_graph):\n",
        "    edge_types = sorted(unique_rel(meta_graph))\n",
        "    # edge_types = ['rel_'+i+j for i,j in itertools.combinations_with_replacement(meta_graph.keys(), 2)]\n",
        "    print(edge_types)\n",
        "    n = len(graph.nodes())\n",
        "    r = len(edge_types)\n",
        "\n",
        "    adj_matrices = []\n",
        "\n",
        "    for edge_type in edge_types:\n",
        "        adj_matrix = np.zeros((n, n), dtype=np.int32)\n",
        "        \n",
        "        for u, v, data in graph.edges(data=True):\n",
        "            if data['type'] == edge_type:\n",
        "                adj_matrix[u, v] = 1\n",
        "                adj_matrix[v, u] = 1\n",
        "        \n",
        "        adj_matrices.append(adj_matrix)\n",
        "\n",
        "    edge_dict = {i: edge_types[i] for i in range(len(edge_types))}\n",
        "    return np.stack(adj_matrices, axis=-1), edge_dict\n",
        "\n",
        "adj_tsr, edge_dict = adjacency_tensor(hetero_graph, meta_graph)\n",
        "adj_tsr.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1gvg86mwrY-",
        "outputId": "874126cc-6d24-4ce9-d78d-7e693485b3c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({0: ('A', 'to', 'C'),\n",
              "  1: ('B', 'to', 'C'),\n",
              "  2: ('C', 'to', 'D'),\n",
              "  3: ('D', 'to', 'D')},\n",
              " {'A': {'C': ('A', 'to', 'C')},\n",
              "  'B': {'C': ('B', 'to', 'C')},\n",
              "  'C': {'A': ('A', 'to', 'C'), 'B': ('B', 'to', 'C'), 'D': ('C', 'to', 'D')},\n",
              "  'D': {'C': ('C', 'to', 'D'), 'D': ('D', 'to', 'D')}})"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ],
      "source": [
        "edge_dict, meta_graph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adj_tsr.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMDF6He8ni1C",
        "outputId": "fa19f2ca-66c4-4da2-d7c1-ee3dda173054"
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 15, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQHoxjkgPiwL",
        "outputId": "ee90c463-6c25-4199-b483-99c0f6901bef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "400 100\n"
          ]
        }
      ],
      "source": [
        "def adj_to_cov(W_GT, num_nodes):\n",
        "  # W_GT = nx.adjacency_matrix(hetero_graph).todense()\n",
        "  weights = np.random.lognormal(0, 0.2, (num_nodes, num_nodes))\n",
        "  weights = (weights + weights.T) / 2\n",
        "  \n",
        "  W_GT = W_GT * weights\n",
        "  W_GT = W_GT * num_nodes / (np.sum(W_GT)+1e-10)\n",
        "  L_GT = np.diag(W_GT @ np.ones(num_nodes)) - W_GT\n",
        "  \n",
        "  cov_GT = np.linalg.inv(L_GT + (1e-1) * np.eye(num_nodes))\n",
        "  return cov_GT\n",
        "\n",
        "\n",
        "\n",
        "def generate_rel_signals(adj_tensor, sgl_dim):\n",
        "  noise_sigma = 1e-02\n",
        "  num_nodes,_, rel_num = adj_tensor.shape\n",
        "  print(rel_num)\n",
        "  emb_dim = sgl_dim * rel_num\n",
        "  print(emb_dim, sgl_dim)\n",
        "  signals_nodes = np.random.multivariate_normal(np.zeros(num_nodes), noise_sigma* np.eye(num_nodes), emb_dim)\n",
        "  signals_edges = np.zeros((rel_num, emb_dim))\n",
        "  \n",
        "  for rel in range(rel_num):\n",
        "    # the dimensions that are specific to relation type\n",
        "    cov = adj_to_cov(adj_tensor[:,:,rel], num_nodes)\n",
        "    # signals_rel = np.random.multivariate_normal(np.zeros(num_nodes), cov, sgl_dim)\n",
        "    # the dimension that is not relevant to relation type\n",
        "    signals_nodes[sgl_dim*rel:sgl_dim*(rel+1)] += np.random.multivariate_normal(np.zeros(num_nodes), cov, sgl_dim)\n",
        "    signals_edges[rel, sgl_dim*rel:sgl_dim*(rel+1)] = 1/sgl_dim\n",
        "  \n",
        "  return signals_nodes.T, signals_edges\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sgl_dim = 100\n",
        "signal_vtx, signals_edge = generate_rel_signals(adj_tsr, sgl_dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "def get_edge_idx(n_id, n_id_ori, edge_idx):\n",
        "  # Expand B to have the same shape as (C.size(0), B.size(0))\n",
        "  expanded_ori = n_id_ori.expand(edge_idx.shape[0], -1)\n",
        "  # Compute a mask where each row corresponds to the comparison of C[i] with B\n",
        "  mask = (expanded_ori == edge_idx.unsqueeze(1))\n",
        "  # Get the index in A by using the computed mask\n",
        "  # The method `max` is used to get the last occurrence, replace it with `min` for the first occurrence\n",
        "  # Use the indices to get the corresponding elements from A\n",
        "  _, indices = mask.max(dim=1)\n",
        "  return n_id[indices]\n",
        "\n",
        "\n",
        "def generate_hetero_sgls(hetero_graph, meta_graph, adj_tensor, sgl_dim, output_dim):\n",
        "  edge_types = sorted(unique_rel(meta_graph))\n",
        "  noise_sigma = 1e-02\n",
        "  num_nodes,_ ,rel_num = adj_tensor.shape\n",
        "  print(rel_num)\n",
        "  emb_dim = sgl_dim * rel_num\n",
        "  print(emb_dim, sgl_dim)\n",
        "  signals_nodes = np.random.multivariate_normal(np.zeros(num_nodes), noise_sigma* np.eye(num_nodes), emb_dim)\n",
        "\n",
        "  \n",
        "  signals_edges = np.zeros((rel_num, emb_dim))\n",
        "  \n",
        "  for i in range(len(edge_types)):\n",
        "    # the dimensions that are specific to relation type\n",
        "    cov = adj_to_cov(adj_tensor[:,:,i], num_nodes)\n",
        "    # signals_rel = np.random.multivariate_normal(np.zeros(num_nodes), cov, sgl_dim)\n",
        "    # the dimension that is not relevant to relation type\n",
        "    signals_nodes[sgl_dim*i:sgl_dim*(i+1)] += np.random.multivariate_normal(np.zeros(num_nodes), cov, sgl_dim)\n",
        "    signals_edges[i, sgl_dim*i:sgl_dim*(i+1)] = 1/sgl_dim\n",
        "  \n",
        "  signals_nodes = torch.Tensor(signals_nodes)\n",
        "  graph_pyg = HeteroData()\n",
        "  trans_matrix = torch.rand(signals_nodes.shape[-1], output_dim)\n",
        "\n",
        "  for node_type in meta_graph:\n",
        "    node_idxs = [node_idx for node_idx in hetero_graph.nodes if node_type == hetero_graph.nodes[node_idx]['type']]\n",
        "    graph_pyg[node_type].x  = signals_nodes[node_idxs] @ trans_matrix\n",
        "    graph_pyg[node_type].n_id_ori = torch.Tensor(node_idxs).int()\n",
        "    graph_pyg[node_type].n_id = torch.Tensor(range(len(graph_pyg[node_type].n_id_ori))).int()\n",
        "  \n",
        "  for i, j in itertools.combinations_with_replacement(meta_graph.keys(), 2):\n",
        "    if (i, 'to', j) in edge_types:\n",
        "      print('rel_'+i+j)\n",
        "      edge_type_idx = edge_types.index((i, 'to', j))\n",
        "      non_zeros = torch.tensor(adj_tensor)[:,:,edge_type_idx].nonzero().T\n",
        "      mask = torch.isin(non_zeros[0, :], graph_pyg[i].n_id_ori)\n",
        "      print(non_zeros[:, mask])\n",
        "\n",
        "      s_idx = get_edge_idx(graph_pyg[i].n_id, graph_pyg[i].n_id_ori, non_zeros[:, mask][0])\n",
        "      t_idx = get_edge_idx(graph_pyg[j].n_id, graph_pyg[j].n_id_ori, non_zeros[:, mask][1])\n",
        "      print(torch.stack((s_idx, t_idx), dim=0))\n",
        "\n",
        "      graph_pyg[(i, 'to', j)].edge_index = torch.stack((s_idx, t_idx), dim=0)\n",
        "  \n",
        "  edge_onehot = torch.eye(len(graph_pyg.edge_types))\n",
        "\n",
        "  i=0\n",
        "  for edge_type in graph_pyg.edge_types:\n",
        "    graph_pyg[edge_type].x = edge_onehot[i].reshape(1,-1)\n",
        "    i+=1\n",
        "\n",
        "\n",
        "  return graph_pyg\n",
        "\n",
        "data = generate_hetero_sgls(hetero_graph, meta_graph, adj_tsr, sgl_dim, output_dim = 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULMclthonR05",
        "outputId": "212bc405-d97f-480f-ce8f-6b8b892adde0"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "400 100\n",
            "rel_AC\n",
            "tensor([[ 3,  3,  8,  8, 10],\n",
            "        [ 2, 11,  7,  9, 11]])\n",
            "tensor([[0, 0, 1, 1, 2],\n",
            "        [1, 5, 3, 4, 5]], dtype=torch.int32)\n",
            "rel_BC\n",
            "tensor([[ 1,  1,  4,  6,  6, 12, 12],\n",
            "        [ 0,  2,  5,  5,  7,  9, 11]])\n",
            "tensor([[0, 0, 1, 2, 2, 3, 3],\n",
            "        [0, 1, 2, 2, 3, 4, 5]], dtype=torch.int32)\n",
            "rel_CD\n",
            "tensor([[ 5, 13],\n",
            "        [14, 14]])\n",
            "tensor([[2, 6],\n",
            "        [0, 0]], dtype=torch.int32)\n",
            "rel_DD\n",
            "tensor([], size=(2, 0), dtype=torch.int64)\n",
            "tensor([], size=(2, 0), dtype=torch.int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['D']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hKKMOj26f5n",
        "outputId": "e73bbe0d-5078-4531-b0fc-641db0d75cb8"
      },
      "execution_count": 399,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': tensor([[ 3.6991,  7.5822,  9.7906,  5.4923,  5.8905,  5.7355,  4.9892,  8.2958,\n",
              "          3.0284,  6.0156,  0.9729,  7.9009,  5.0206,  6.7804,  3.6630, 10.6558,\n",
              "          3.7842, -0.5147,  8.4176, -3.2667,  3.2054, -0.9291,  5.7305,  6.4572,\n",
              "          3.7055,  9.0545,  6.6750,  3.4784,  8.7374,  4.1874,  4.7241,  3.9080,\n",
              "          2.6208,  8.3157,  1.2000,  5.3513,  2.3079,  5.4160,  0.7103,  0.7952,\n",
              "          1.8397,  6.2756,  0.0647, 10.2602,  0.8520,  0.6558,  7.4690,  4.4765,\n",
              "          0.7712,  2.3049, -0.9029,  1.1872,  6.4782,  7.5953,  3.1303,  2.2345,\n",
              "          3.4018,  3.5369,  7.2633,  0.5647,  3.1622,  4.6396,  9.8936,  5.8729,\n",
              "          6.2661,  2.5476,  1.1603, 10.7901,  5.4626, -1.0924,  6.5631,  6.3952,\n",
              "          5.0452,  9.8019,  1.9856,  0.3741,  3.7823,  6.2954,  4.7528,  0.9537,\n",
              "          7.9782,  3.5944,  9.5754,  6.5056,  4.4423,  7.9705,  3.3248,  3.0347,\n",
              "          2.7748,  6.9532,  0.7307,  7.5953,  0.4325,  3.4124,  3.5306,  5.3932,\n",
              "          4.1110,  5.7453, -3.6386,  6.9751]], device='cuda:0'), 'n_id_ori': tensor([14], device='cuda:0', dtype=torch.int32), 'n_id': tensor([0], device='cuda:0', dtype=torch.int32)}"
            ]
          },
          "metadata": {},
          "execution_count": 399
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import scipy.sparse as sparse\n",
        "\n",
        "def coo_to_sparseTensor(coo):\n",
        "    values = coo.data\n",
        "    indices = np.vstack((coo.row, coo.col))\n",
        "\n",
        "    i = torch.LongTensor(indices)\n",
        "    v = torch.FloatTensor(values)\n",
        "    shape = coo.shape\n",
        "\n",
        "    return torch.sparse.FloatTensor(i, v, torch.Size(shape)).to_dense()\n",
        "\n",
        "def get_degree_operator(m):\n",
        "  \n",
        "    ncols =int(m*(m - 1)/2)\n",
        "\n",
        "    I = np.zeros(ncols)\n",
        "    J = np.zeros(ncols)\n",
        "\n",
        "    k = 0\n",
        "    for i in np.arange(1, m):\n",
        "        I[k:(k + m - i)] = np.arange(i, m)\n",
        "        k = k + (m - i)\n",
        "\n",
        "    k = 0\n",
        "    for i in np.arange(1, m):\n",
        "        J[k: (k + m - i)] = i - 1\n",
        "        k = k + m - i\n",
        "\n",
        "    Row = np.tile(np.arange(0, ncols), 2)\n",
        "    Col = np.append(I, J)\n",
        "    Data = np.ones(Col.size)\n",
        "    St = sparse.coo_matrix((Data, (Row, Col)), shape=(ncols, m))\n",
        "    return St.T"
      ],
      "metadata": {
        "id": "eWdRaf1Fl0co"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import HeteroConv, Linear, SAGEConv, HeteroDictLinear\n",
        "\n",
        "class LinearProj(torch.nn.Module):\n",
        "    def __init__(self, node_shape, edge_shape, out_channels):\n",
        "        super().__init__()\n",
        "        self.NodeLinear = HeteroDictLinear(in_channels=node_shape,out_channels=out_channels)\n",
        "        self.EdgeLinear = Linear(edge_shape[0],out_channels)\n",
        "\n",
        "    def forward(self, batch):\n",
        "      node_attrs = {node_type: batch[node_type].x for node_type in batch.node_types}\n",
        "\n",
        "      node_out = self.NodeLinear(node_attrs)\n",
        "      edge_out = {edge_type: self.EdgeLinear(batch[edge_type].x) for edge_type in batch.edge_types}\n",
        "      \n",
        "      for node_type in batch.node_types:\n",
        "        node_out[node_type] = node_out[node_type]/(node_out[node_type].norm(dim=1)[:, None])\n",
        "      for edge_type in batch.edge_types:\n",
        "        edge_out[edge_type] = edge_out[edge_type]/(edge_out[edge_type].norm(dim=1)[:, None])\n",
        "\n",
        "      return (node_out, edge_out)\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, node_shape, edge_shape, w_l, D, out_channels=100):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.out_channels = out_channels\n",
        "        self.w = torch.nn.Parameter(torch.rand(w_l))\n",
        "        self.D = D\n",
        "        self.hetero_linear = LinearProj(node_shape, edge_shape, out_channels=out_channels)\n",
        "        self.classifier = Classifier()\n",
        "        \n",
        "\n",
        "    def forward(self, batch, device):\n",
        "\n",
        "        # D is the degree operator\n",
        "\n",
        "        # `x_dict` holds feature matrices of all node types\n",
        "        # `edge_index_dict` holds all edge indices of all edge types\n",
        "        node_out, edge_out = self.hetero_linear(batch)\n",
        "        pred_list = []\n",
        "        m = batch.num_nodes\n",
        "        r = len(batch.edge_types)\n",
        "        batch_size = 1\n",
        "        n_emb = torch.zeros(m, self.out_channels).to(device)\n",
        "        \n",
        "        for n_type in batch.node_types:\n",
        "          n_emb[batch[n_type].n_id_ori] = node_out[n_type]\n",
        "          # print('node_out shape',node_out[n_type].shape)\n",
        "          # print(batch[n_type].n_id)\n",
        "        r_emb = [edge_out[r_type] for r_type in sorted(batch.edge_types)]\n",
        "        r_emb = torch.cat(r_emb, dim=0)\n",
        "        \n",
        "        diff_tensor = (n_emb[:, None, None, :] - n_emb[None,: , None, :]) * r_emb[ None, None, :, :]\n",
        "        # print(r_emb.shape)\n",
        "        # print(n_emb.shape)\n",
        "        # print('n_emb', torch.max(torch.abs(n_emb)))\n",
        "        # print('r_emb',torch.max(torch.abs(r_emb)))\n",
        "        sigma = 1e-3\n",
        "        smooth_tensor = torch.exp(-1/sigma * torch.norm(diff_tensor, dim=-1)**2)\n",
        "        smooth_tensor = smooth_tensor.unsqueeze(0)\n",
        "        mask = torch.triu(torch.ones(m, m), diagonal=1).bool()\n",
        "        z = smooth_tensor[:, mask].view(batch_size, -1)\n",
        "\n",
        "        return self.w.view(1,-1), z\n",
        "\n"
      ],
      "metadata": {
        "id": "MGQCk6cvn5qW"
      },
      "execution_count": 528,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "def objective(w, D, z, l2_penalty=0.01, log_penalty=0.1):\n",
        "  alpha = log_penalty\n",
        "  beta = l2_penalty \n",
        "\n",
        "  w = F.relu(w)\n",
        "  \n",
        "  f1 = beta * torch.norm(w, 2) ** 2\n",
        "  f2 = 1e4 * w @ z.T\n",
        "  f3 = - alpha * torch.sum(torch.log(w@D.T))\n",
        "  print(f1,f2[0][0],f3)\n",
        "\n",
        "  if torch.all(w.ge(0)) >= 0:\n",
        "    return f1 + f2[0][0] + f3\n",
        "  else:\n",
        "    return 10**3\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "node_shape = {node_type: data[node_type].x.shape[-1] for node_type in data.node_types}\n",
        "edge_shape = [data[edge_type].x.shape[-1] for edge_type in data.edge_types]\n",
        "\n",
        "w_l = int(data.num_nodes*(data.num_nodes-1)*len(data.edge_types)/2)\n",
        "D_ori = coo_to_sparseTensor(get_degree_operator(data.num_nodes)).to(device)\n",
        "eye = torch.eye(int(data.num_nodes*(data.num_nodes-1)/2))\n",
        "shift_sum = eye.repeat_interleave(len(data.edge_types), dim=1)\n",
        "D = D_ori @ shift_sum.to(device)\n",
        "\n",
        "model = Model(node_shape, edge_shape, w_l, D, out_channels=100)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.1)\n",
        "data = data.to(device)\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch in range(1, 501):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  loss = 0\n",
        "  w, z = model(data, device)\n",
        "  loss = objective(w=w.to(device), D=D.to(device), z=z.to(device))\n",
        "  print(loss)\n",
        "  loss.backward()\n",
        "  clip_grad_norm_(model.parameters(), 5)\n",
        "  optimizer.step()\n",
        "\n",
        "  print('loss: {}, epoch: {}'.format(loss, epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifVs1-vdxjoD",
        "outputId": "f52f1347-b50f-4fbe-80c2-8fb868faa78f"
      },
      "execution_count": 533,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.2741, device='cuda:0', grad_fn=<MulBackward0>) tensor(8077.4800, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.9185, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(8073.8354, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: 8073.83544921875, epoch: 1\n",
            "tensor(1.2701, device='cuda:0', grad_fn=<MulBackward0>) tensor(4125.9141, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.9153, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(4122.2690, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: 4122.26904296875, epoch: 2\n",
            "tensor(1.2661, device='cuda:0', grad_fn=<MulBackward0>) tensor(1667.5687, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.9121, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(1663.9227, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: 1663.9227294921875, epoch: 3\n",
            "tensor(1.2621, device='cuda:0', grad_fn=<MulBackward0>) tensor(473.0529, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.9089, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(469.4061, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: 469.40606689453125, epoch: 4\n",
            "tensor(1.2582, device='cuda:0', grad_fn=<MulBackward0>) tensor(85.0396, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.9057, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(81.3920, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: 81.39202880859375, epoch: 5\n",
            "tensor(1.2542, device='cuda:0', grad_fn=<MulBackward0>) tensor(10.4036, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.9026, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(6.7553, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: 6.755317211151123, epoch: 6\n",
            "tensor(1.2503, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.4173, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8994, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-2.2318, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -2.2317652702331543, epoch: 7\n",
            "tensor(1.2464, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.3072, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8962, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.3426, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.3425745964050293, epoch: 8\n",
            "tensor(1.2425, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0963, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8931, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5543, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5543131828308105, epoch: 9\n",
            "tensor(1.2386, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0410, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8900, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6104, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.610440969467163, epoch: 10\n",
            "tensor(1.2347, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0176, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8871, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6348, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6347532272338867, epoch: 11\n",
            "tensor(1.2308, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0095, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8841, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6438, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.643845319747925, epoch: 12\n",
            "tensor(1.2269, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0068, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8812, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6475, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.647510051727295, epoch: 13\n",
            "tensor(1.2231, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0059, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8784, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6495, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6494646072387695, epoch: 14\n",
            "tensor(1.2192, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0054, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8756, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6509, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6508967876434326, epoch: 15\n",
            "tensor(1.2154, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0051, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8728, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6523, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6522769927978516, epoch: 16\n",
            "tensor(1.2116, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0046, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8700, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6538, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6537728309631348, epoch: 17\n",
            "tensor(1.2078, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0041, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8672, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6554, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.655376434326172, epoch: 18\n",
            "tensor(1.2040, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0035, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8644, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6570, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6570076942443848, epoch: 19\n",
            "tensor(1.2002, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0029, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8617, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6586, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.65859317779541, epoch: 20\n",
            "tensor(1.1964, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0025, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8590, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6601, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6600961685180664, epoch: 21\n",
            "tensor(1.1926, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0021, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8563, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6615, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6615102291107178, epoch: 22\n",
            "tensor(1.1889, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0018, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8536, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6628, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6628456115722656, epoch: 23\n",
            "tensor(1.1852, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0016, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8509, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6641, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6641159057617188, epoch: 24\n",
            "tensor(1.1814, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0014, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8482, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6653, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.665336847305298, epoch: 25\n",
            "tensor(1.1777, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0012, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8455, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6665, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6665186882019043, epoch: 26\n",
            "tensor(1.1740, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0011, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8428, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6677, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6676712036132812, epoch: 27\n",
            "tensor(1.1703, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0010, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8401, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6688, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6687979698181152, epoch: 28\n",
            "tensor(1.1667, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0009, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8375, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6699, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.669905424118042, epoch: 29\n",
            "tensor(1.1630, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0008, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8348, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6710, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6709938049316406, epoch: 30\n",
            "tensor(1.1594, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0007, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8321, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6721, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.672064781188965, epoch: 31\n",
            "tensor(1.1557, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0006, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8295, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6731, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6731185913085938, epoch: 32\n",
            "tensor(1.1521, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0006, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8268, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6742, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6741554737091064, epoch: 33\n",
            "tensor(1.1485, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0005, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8242, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6752, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6751744747161865, epoch: 34\n",
            "tensor(1.1449, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0005, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8215, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6762, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.676175355911255, epoch: 35\n",
            "tensor(1.1413, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0004, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8189, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6772, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.677159070968628, epoch: 36\n",
            "tensor(1.1377, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0004, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8162, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6781249046325684, epoch: 37\n",
            "tensor(1.1342, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0004, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8136, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6791, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6790730953216553, epoch: 38\n",
            "tensor(1.1306, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8109, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6800, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.680004835128784, epoch: 39\n",
            "tensor(1.1271, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8083, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6809, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6809206008911133, epoch: 40\n",
            "tensor(1.1235, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8057, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6818, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6818206310272217, epoch: 41\n",
            "tensor(1.1200, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8030, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6827, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.682706356048584, epoch: 42\n",
            "tensor(1.1165, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.8004, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6836, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.683577537536621, epoch: 43\n",
            "tensor(1.1130, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7977, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6844, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6844358444213867, epoch: 44\n",
            "tensor(1.1095, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7951, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6853, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.685281753540039, epoch: 45\n",
            "tensor(1.1061, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7924, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6861, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6861159801483154, epoch: 46\n",
            "tensor(1.1026, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7898, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6869, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.686939239501953, epoch: 47\n",
            "tensor(1.0992, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7872, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6878, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.687750816345215, epoch: 48\n",
            "tensor(1.0957, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7845, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6886, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.688551187515259, epoch: 49\n",
            "tensor(1.0923, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7819, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6893, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6893396377563477, epoch: 50\n",
            "tensor(1.0889, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7792, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6901, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.690114974975586, epoch: 51\n",
            "tensor(1.0855, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7766, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6909, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6908786296844482, epoch: 52\n",
            "tensor(1.0821, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7739, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6916, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6916275024414062, epoch: 53\n",
            "tensor(1.0787, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7713, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6924, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6923630237579346, epoch: 54\n",
            "tensor(1.0753, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7686, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6931, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.693084239959717, epoch: 55\n",
            "tensor(1.0720, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7660, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6938, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.693790912628174, epoch: 56\n",
            "tensor(1.0686, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7633, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6945, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6944832801818848, epoch: 57\n",
            "tensor(1.0653, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7607, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6952, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6951611042022705, epoch: 58\n",
            "tensor(1.0620, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7580, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6958, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6958250999450684, epoch: 59\n",
            "tensor(1.0587, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7554, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6965, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6964752674102783, epoch: 60\n",
            "tensor(1.0554, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7527, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6971, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.697112560272217, epoch: 61\n",
            "tensor(1.0521, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7501, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6977, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.697736978530884, epoch: 62\n",
            "tensor(1.0488, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7474, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6984, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.698350429534912, epoch: 63\n",
            "tensor(1.0455, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7448, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6990, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6989541053771973, epoch: 64\n",
            "tensor(1.0423, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7421, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6995, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6995491981506348, epoch: 65\n",
            "tensor(1.0390, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7395, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7001, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.70013689994812, epoch: 66\n",
            "tensor(1.0358, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7368, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7007, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7007172107696533, epoch: 67\n",
            "tensor(1.0325, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7341, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7013, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.701292037963867, epoch: 68\n",
            "tensor(1.0293, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7315, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7019, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.701859474182129, epoch: 69\n",
            "tensor(1.0261, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7288, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7024, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.702420473098755, epoch: 70\n",
            "tensor(1.0229, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7262, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7030, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7029733657836914, epoch: 71\n",
            "tensor(1.0197, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7235, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7035, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7035164833068848, epoch: 72\n",
            "tensor(1.0166, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7208, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7041, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.704050064086914, epoch: 73\n",
            "tensor(1.0134, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7182, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7046, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.704573154449463, epoch: 74\n",
            "tensor(1.0102, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7155, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7051, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.705085515975952, epoch: 75\n",
            "tensor(1.0071, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7129, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7056, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.705585479736328, epoch: 76\n",
            "tensor(1.0039, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7102, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7061, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7060747146606445, epoch: 77\n",
            "tensor(1.0008, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7075, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7066, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.706552505493164, epoch: 78\n",
            "tensor(0.9977, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7049, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7070, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7070186138153076, epoch: 79\n",
            "tensor(0.9946, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.7022, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7075, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7074739933013916, epoch: 80\n",
            "tensor(0.9915, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6996, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7079, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.707918643951416, epoch: 81\n",
            "tensor(0.9884, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6969, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7084, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7083520889282227, epoch: 82\n",
            "tensor(0.9853, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6943, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7088, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.708775758743286, epoch: 83\n",
            "tensor(0.9823, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6916, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7092, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.709188938140869, epoch: 84\n",
            "tensor(0.9792, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6890, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7096, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.709592342376709, epoch: 85\n",
            "tensor(0.9762, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6863, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7100, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7099862098693848, epoch: 86\n",
            "tensor(0.9732, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6836, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7104, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7103703022003174, epoch: 87\n",
            "tensor(0.9701, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6810, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7107, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7107458114624023, epoch: 88\n",
            "tensor(0.9671, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6783, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7111, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.711111068725586, epoch: 89\n",
            "tensor(0.9641, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6757, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7115, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.71146821975708, epoch: 90\n",
            "tensor(0.9611, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6730, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7118, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7118163108825684, epoch: 91\n",
            "tensor(0.9581, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6704, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7122, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.712155818939209, epoch: 92\n",
            "tensor(0.9552, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6678, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7125, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.712486505508423, epoch: 93\n",
            "tensor(0.9522, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6651, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7128, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7128095626831055, epoch: 94\n",
            "tensor(0.9492, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6625, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7131, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7131242752075195, epoch: 95\n",
            "tensor(0.9463, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6598, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7134, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7134311199188232, epoch: 96\n",
            "tensor(0.9434, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6572, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7137, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7137298583984375, epoch: 97\n",
            "tensor(0.9404, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6546, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7140, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7140212059020996, epoch: 98\n",
            "tensor(0.9375, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6519, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7143, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7143049240112305, epoch: 99\n",
            "tensor(0.9346, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6493, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7146, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7145817279815674, epoch: 100\n",
            "tensor(0.9317, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6467, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7149, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.714850664138794, epoch: 101\n",
            "tensor(0.9288, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6440, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7151, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7151126861572266, epoch: 102\n",
            "tensor(0.9259, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6414, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7154, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7153682708740234, epoch: 103\n",
            "tensor(0.9231, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6388, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7156, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.715616464614868, epoch: 104\n",
            "tensor(0.9202, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6362, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7159, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.715857744216919, epoch: 105\n",
            "tensor(0.9174, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6336, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7161, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.716092824935913, epoch: 106\n",
            "tensor(0.9145, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6309, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7163, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.716320276260376, epoch: 107\n",
            "tensor(0.9117, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.9379e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6283, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7165, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.716541051864624, epoch: 108\n",
            "tensor(0.9089, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.6526e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6257, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7168, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.716754913330078, epoch: 109\n",
            "tensor(0.9061, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.3642e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6231, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7170, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7169625759124756, epoch: 110\n",
            "tensor(0.9033, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.0770e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6205, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7172, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.717163562774658, epoch: 111\n",
            "tensor(0.9005, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.7947e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6179, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7174, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.717357635498047, epoch: 112\n",
            "tensor(0.8977, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.5202e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6153, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7175, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7175447940826416, epoch: 113\n",
            "tensor(0.8949, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.2559e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6127, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7177, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7177257537841797, epoch: 114\n",
            "tensor(0.8921, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.0034e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6101, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7179, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.717900276184082, epoch: 115\n",
            "tensor(0.8894, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.7638e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6075, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7181, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.718067169189453, epoch: 116\n",
            "tensor(0.8866, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.5377e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6049, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7182, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.718228578567505, epoch: 117\n",
            "tensor(0.8839, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.3251e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.6024, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7184, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.718383312225342, epoch: 118\n",
            "tensor(0.8812, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.1258e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5998, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7185, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.718531608581543, epoch: 119\n",
            "tensor(0.8785, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.9393e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5972, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7187, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7186737060546875, epoch: 120\n",
            "tensor(0.8757, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.7649e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5946, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7188, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.718809127807617, epoch: 121\n",
            "tensor(0.8730, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.6018e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5921, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7189, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7189390659332275, epoch: 122\n",
            "tensor(0.8704, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.4488e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5895, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7191, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.719062328338623, epoch: 123\n",
            "tensor(0.8677, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.3051e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5869, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7192, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.719179153442383, epoch: 124\n",
            "tensor(0.8650, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.1695e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5843, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7193, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7192904949188232, epoch: 125\n",
            "tensor(0.8623, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.0409e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5818, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7194, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.719395637512207, epoch: 126\n",
            "tensor(0.8597, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.9184e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5792, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7195, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7194948196411133, epoch: 127\n",
            "tensor(0.8570, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.8007e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5767, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7196, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.719588279724121, epoch: 128\n",
            "tensor(0.8544, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.6871e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5741, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7197, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7196760177612305, epoch: 129\n",
            "tensor(0.8518, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.5766e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5716, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7198, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7197577953338623, epoch: 130\n",
            "tensor(0.8491, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.4684e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5690, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7198, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7198338508605957, epoch: 131\n",
            "tensor(0.8465, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.3618e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5665, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7199, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7199039459228516, epoch: 132\n",
            "tensor(0.8439, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.2562e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5639, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7200, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.719968318939209, epoch: 133\n",
            "tensor(0.8413, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.1512e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5614, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7200, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7200276851654053, epoch: 134\n",
            "tensor(0.8387, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.0464e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5589, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7201, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7200801372528076, epoch: 135\n",
            "tensor(0.8362, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.9415e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5563, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7201, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7201287746429443, epoch: 136\n",
            "tensor(0.8336, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.8366e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5538, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7202, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7201709747314453, epoch: 137\n",
            "tensor(0.8310, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.7316e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5513, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7202, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7202072143554688, epoch: 138\n",
            "tensor(0.8285, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.6266e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5488, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7202, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.72023868560791, epoch: 139\n",
            "tensor(0.8259, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.5217e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5463, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7203, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7202646732330322, epoch: 140\n",
            "tensor(0.8234, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.4173e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5437, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7203, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7202844619750977, epoch: 141\n",
            "tensor(0.8209, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.3137e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5412, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7203, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.72029972076416, epoch: 142\n",
            "tensor(0.8184, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.2110e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5387, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7203, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.720309257507324, epoch: 143\n",
            "tensor(0.8159, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.1098e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5362, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7203, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.720313549041748, epoch: 144\n",
            "tensor(0.8134, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.0103e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5337, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7203, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7203128337860107, epoch: 145\n",
            "tensor(0.8109, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.9128e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5312, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7203, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7203054428100586, epoch: 146\n",
            "tensor(0.8084, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.8175e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5287, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7203, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.720294237136841, epoch: 147\n",
            "tensor(0.8059, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.7248e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5262, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7203, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7202775478363037, epoch: 148\n",
            "tensor(0.8034, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.6348e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5237, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7203, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7202553749084473, epoch: 149\n",
            "tensor(0.8010, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.5476e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5212, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7202, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7202277183532715, epoch: 150\n",
            "tensor(0.7985, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.4634e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5188, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7202, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7201950550079346, epoch: 151\n",
            "tensor(0.7961, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.3822e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5163, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7202, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7201576232910156, epoch: 152\n",
            "tensor(0.7937, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.3039e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5138, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7201, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7201151847839355, epoch: 153\n",
            "tensor(0.7912, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.2286e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5113, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7201, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.720067024230957, epoch: 154\n",
            "tensor(0.7888, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.1562e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5089, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7200, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7200140953063965, epoch: 155\n",
            "tensor(0.7864, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.0866e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5064, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7200, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7199556827545166, epoch: 156\n",
            "tensor(0.7840, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.0196e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5039, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7199, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.719892740249634, epoch: 157\n",
            "tensor(0.7816, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.9551e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.5015, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7198, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.71982479095459, epoch: 158\n",
            "tensor(0.7792, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.8930e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4990, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7198, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7197515964508057, epoch: 159\n",
            "tensor(0.7768, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.8331e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4966, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7197, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7196741104125977, epoch: 160\n",
            "tensor(0.7745, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.7751e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4941, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7196, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.719590663909912, epoch: 161\n",
            "tensor(0.7721, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.7190e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4916, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7195, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7195029258728027, epoch: 162\n",
            "tensor(0.7698, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.6645e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4892, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7194, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7194104194641113, epoch: 163\n",
            "tensor(0.7674, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.6115e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4868, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7193, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.719312906265259, epoch: 164\n",
            "tensor(0.7651, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.5598e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4843, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7192, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.719210147857666, epoch: 165\n",
            "tensor(0.7627, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.5093e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4819, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7191, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7191038131713867, epoch: 166\n",
            "tensor(0.7604, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.4598e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4794, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7190, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.718991756439209, epoch: 167\n",
            "tensor(0.7581, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.4113e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4770, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7189, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7188754081726074, epoch: 168\n",
            "tensor(0.7558, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.3635e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4746, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7188, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7187538146972656, epoch: 169\n",
            "tensor(0.7535, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.3164e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4722, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7186, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.718628406524658, epoch: 170\n",
            "tensor(0.7512, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.2700e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4697, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7185, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7184982299804688, epoch: 171\n",
            "tensor(0.7489, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.2241e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4673, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7184, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.71836256980896, epoch: 172\n",
            "tensor(0.7467, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.1788e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4649, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7182, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7182228565216064, epoch: 173\n",
            "tensor(0.7444, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.1340e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4625, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7181, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.71807861328125, epoch: 174\n",
            "tensor(0.7421, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.0897e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4601, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7179, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7179298400878906, epoch: 175\n",
            "tensor(0.7399, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.0459e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4577, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7178, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.717776298522949, epoch: 176\n",
            "tensor(0.7376, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.0025e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4553, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7176, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.717618942260742, epoch: 177\n",
            "tensor(0.7354, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.9597e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4529, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7175, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7174558639526367, epoch: 178\n",
            "tensor(0.7332, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.9175e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4505, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7173, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.717289686203003, epoch: 179\n",
            "tensor(0.7309, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.8758e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4481, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7171, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.717118263244629, epoch: 180\n",
            "tensor(0.7287, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.8347e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4457, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7169, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.71694278717041, epoch: 181\n",
            "tensor(0.7265, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.7943e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4433, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7168, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7167627811431885, epoch: 182\n",
            "tensor(0.7243, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.7545e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4409, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7166, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.716578245162964, epoch: 183\n",
            "tensor(0.7221, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.7155e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4385, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7164, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7163898944854736, epoch: 184\n",
            "tensor(0.7199, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.6772e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4361, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7162, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7161970138549805, epoch: 185\n",
            "tensor(0.7178, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.6396e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4338, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7160, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7159996032714844, epoch: 186\n",
            "tensor(0.7156, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.6028e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4314, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7158, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7157983779907227, epoch: 187\n",
            "tensor(0.7134, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.5669e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4290, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7156, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.715592622756958, epoch: 188\n",
            "tensor(0.7113, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.5317e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4267, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7154, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7153830528259277, epoch: 189\n",
            "tensor(0.7091, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.4973e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4243, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7152, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7151684761047363, epoch: 190\n",
            "tensor(0.7070, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.4638e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4219, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7150, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7149505615234375, epoch: 191\n",
            "tensor(0.7048, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.4310e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4196, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7147, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.714728355407715, epoch: 192\n",
            "tensor(0.7027, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.3991e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4172, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7145, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7145016193389893, epoch: 193\n",
            "tensor(0.7006, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.3680e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4149, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7143, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.714271068572998, epoch: 194\n",
            "tensor(0.6985, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.3376e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4125, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7140, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.714036464691162, epoch: 195\n",
            "tensor(0.6964, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.3080e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4102, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7138, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7137985229492188, epoch: 196\n",
            "tensor(0.6943, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.2792e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4078, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7136, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7135558128356934, epoch: 197\n",
            "tensor(0.6922, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.2511e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4055, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7133, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.713308572769165, epoch: 198\n",
            "tensor(0.6901, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.2237e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4032, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7131, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7130582332611084, epoch: 199\n",
            "tensor(0.6880, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.1970e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.4008, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7128, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.712803840637207, epoch: 200\n",
            "tensor(0.6859, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.1709e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3985, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7125, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.712545871734619, epoch: 201\n",
            "tensor(0.6839, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.1455e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3962, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7123, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.712283134460449, epoch: 202\n",
            "tensor(0.6818, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.1207e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3939, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7120, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7120168209075928, epoch: 203\n",
            "tensor(0.6798, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.0966e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3915, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7117, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.711747169494629, epoch: 204\n",
            "tensor(0.6777, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.0729e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3892, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7115, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.711473226547241, epoch: 205\n",
            "tensor(0.6757, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.0499e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3869, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7112, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.711195707321167, epoch: 206\n",
            "tensor(0.6737, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.0274e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3846, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7109, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.710914134979248, epoch: 207\n",
            "tensor(0.6716, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.0053e-05, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3823, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7106, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7106289863586426, epoch: 208\n",
            "tensor(0.6696, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.8384e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3800, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7103, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7103404998779297, epoch: 209\n",
            "tensor(0.6676, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.6281e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3777, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7100, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.710047960281372, epoch: 210\n",
            "tensor(0.6656, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.4225e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3754, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7098, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7097513675689697, epoch: 211\n",
            "tensor(0.6636, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.2215e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3731, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7095, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.709451675415039, epoch: 212\n",
            "tensor(0.6616, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.0249e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3708, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7091, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7091479301452637, epoch: 213\n",
            "tensor(0.6597, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.8326e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3685, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7088, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.708841323852539, epoch: 214\n",
            "tensor(0.6577, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.6445e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3662, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7085, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7085301876068115, epoch: 215\n",
            "tensor(0.6557, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.4604e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3639, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7082, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7082157135009766, epoch: 216\n",
            "tensor(0.6537, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.2804e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3617, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7079, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7078986167907715, epoch: 217\n",
            "tensor(0.6518, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.1043e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3594, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7076, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7075769901275635, epoch: 218\n",
            "tensor(0.6498, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.9320e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3571, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7073, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.707252264022827, epoch: 219\n",
            "tensor(0.6479, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.7635e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3548, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7069, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7069239616394043, epoch: 220\n",
            "tensor(0.6460, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.5986e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3526, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7066, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.706592321395874, epoch: 221\n",
            "tensor(0.6440, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.4374e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3503, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7063, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7062573432922363, epoch: 222\n",
            "tensor(0.6421, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.2797e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3480, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7059, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.705918312072754, epoch: 223\n",
            "tensor(0.6402, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.1255e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3458, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7056, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7055764198303223, epoch: 224\n",
            "tensor(0.6383, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.9748e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3435, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7052, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7052316665649414, epoch: 225\n",
            "tensor(0.6364, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.8274e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3413, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7049, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.704882860183716, epoch: 226\n",
            "tensor(0.6345, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.6833e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3390, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7045, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7045304775238037, epoch: 227\n",
            "tensor(0.6326, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.5424e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3368, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7042, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7041749954223633, epoch: 228\n",
            "tensor(0.6307, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.4046e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3345, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7038, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.703817367553711, epoch: 229\n",
            "tensor(0.6288, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.2700e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3323, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7035, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7034552097320557, epoch: 230\n",
            "tensor(0.6270, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.1384e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3301, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7031, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7030906677246094, epoch: 231\n",
            "tensor(0.6251, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.0097e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3278, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7027, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7027220726013184, epoch: 232\n",
            "tensor(0.6232, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.8840e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3256, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7024, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7023510932922363, epoch: 233\n",
            "tensor(0.6214, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.7610e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3234, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7020, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7019758224487305, epoch: 234\n",
            "tensor(0.6195, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.6409e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3212, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7016, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.701598882675171, epoch: 235\n",
            "tensor(0.6177, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.5235e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3189, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7012, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7012181282043457, epoch: 236\n",
            "tensor(0.6159, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.4087e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3167, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7008, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.700833797454834, epoch: 237\n",
            "tensor(0.6140, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.2965e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3145, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7004, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7004475593566895, epoch: 238\n",
            "tensor(0.6122, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.1869e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3123, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.7001, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.7000572681427, epoch: 239\n",
            "tensor(0.6104, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.0798e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3101, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6997, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6996638774871826, epoch: 240\n",
            "tensor(0.6086, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.9751e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3079, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6993, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.699268102645874, epoch: 241\n",
            "tensor(0.6068, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.8728e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3057, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6989, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.698868751525879, epoch: 242\n",
            "tensor(0.6050, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.7729e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3035, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6985, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.698467493057251, epoch: 243\n",
            "tensor(0.6032, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.6753e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.3013, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6981, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.69806170463562, epoch: 244\n",
            "tensor(0.6014, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.5799e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2991, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6977, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6976542472839355, epoch: 245\n",
            "tensor(0.5997, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.4868e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2969, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6972, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6972436904907227, epoch: 246\n",
            "tensor(0.5979, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.3959e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2947, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6968, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6968300342559814, epoch: 247\n",
            "tensor(0.5961, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.3072e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2925, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6964, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.696413516998291, epoch: 248\n",
            "tensor(0.5944, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.2205e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2904, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6960, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6959941387176514, epoch: 249\n",
            "tensor(0.5926, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.1360e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2882, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6956, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6955718994140625, epoch: 250\n",
            "tensor(0.5909, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.0535e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2860, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6951, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6951472759246826, epoch: 251\n",
            "tensor(0.5891, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.9730e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2838, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6947, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6947193145751953, epoch: 252\n",
            "tensor(0.5874, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.8945e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2817, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6943, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.694288730621338, epoch: 253\n",
            "tensor(0.5857, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.8180e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2795, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6939, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6938555240631104, epoch: 254\n",
            "tensor(0.5839, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.7434e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2774, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6934, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6934194564819336, epoch: 255\n",
            "tensor(0.5822, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.6706e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2752, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6930, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.692981243133545, epoch: 256\n",
            "tensor(0.5805, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.5997e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2730, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6925, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.692540168762207, epoch: 257\n",
            "tensor(0.5788, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.5307e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2709, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6921, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.692095994949341, epoch: 258\n",
            "tensor(0.5771, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.4633e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2688, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6916, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6916491985321045, epoch: 259\n",
            "tensor(0.5754, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.3978e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2666, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6912, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.691200017929077, epoch: 260\n",
            "tensor(0.5737, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.3339e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2645, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6907, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6907482147216797, epoch: 261\n",
            "tensor(0.5720, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.2716e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2623, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6903, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.690293788909912, epoch: 262\n",
            "tensor(0.5704, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.2110e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2602, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6898, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6898372173309326, epoch: 263\n",
            "tensor(0.5687, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.1519e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2581, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6894, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.689377784729004, epoch: 264\n",
            "tensor(0.5670, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.0943e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2559, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6889, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.688915252685547, epoch: 265\n",
            "tensor(0.5654, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.0381e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2538, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6885, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6884515285491943, epoch: 266\n",
            "tensor(0.5637, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.9832e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2517, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6880, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6879844665527344, epoch: 267\n",
            "tensor(0.5621, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.9297e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2496, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6875, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6875152587890625, epoch: 268\n",
            "tensor(0.5604, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.8774e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2475, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6870, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6870431900024414, epoch: 269\n",
            "tensor(0.5588, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.8264e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2454, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6866, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6865692138671875, epoch: 270\n",
            "tensor(0.5571, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.7764e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2432, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6861, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6860926151275635, epoch: 271\n",
            "tensor(0.5555, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.7274e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2411, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6856, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6856133937835693, epoch: 272\n",
            "tensor(0.5539, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.6795e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2390, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6851, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6851320266723633, epoch: 273\n",
            "tensor(0.5523, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.6325e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2369, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6846, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6846485137939453, epoch: 274\n",
            "tensor(0.5507, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.5863e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2348, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6842, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6841626167297363, epoch: 275\n",
            "tensor(0.5491, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.5411e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2328, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6837, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6836743354797363, epoch: 276\n",
            "tensor(0.5475, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.4967e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2307, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6832, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6831841468811035, epoch: 277\n",
            "tensor(0.5459, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.4531e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2286, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6827, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6826910972595215, epoch: 278\n",
            "tensor(0.5443, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.4103e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2265, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6822, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6821961402893066, epoch: 279\n",
            "tensor(0.5427, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.3683e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2244, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6817, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.681698799133301, epoch: 280\n",
            "tensor(0.5411, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.3272e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2223, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6812, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.681199550628662, epoch: 281\n",
            "tensor(0.5396, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.2870e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2203, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6807, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6806979179382324, epoch: 282\n",
            "tensor(0.5380, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.2476e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2182, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6802, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6801939010620117, epoch: 283\n",
            "tensor(0.5364, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.2091e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2161, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6797, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.679687738418579, epoch: 284\n",
            "tensor(0.5349, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.1716e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2141, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6792, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.679180145263672, epoch: 285\n",
            "tensor(0.5333, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.1349e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2120, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6787, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6786704063415527, epoch: 286\n",
            "tensor(0.5318, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.0993e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2100, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6781580448150635, epoch: 287\n",
            "tensor(0.5303, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.0645e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2079, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6776, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6776437759399414, epoch: 288\n",
            "tensor(0.5287, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.0307e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2059, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6771275997161865, epoch: 289\n",
            "tensor(0.5272, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.9978e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2038, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6766088008880615, epoch: 290\n",
            "tensor(0.5257, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.9656e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.2018, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6761, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.676088333129883, epoch: 291\n",
            "tensor(0.5242, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.9343e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1997, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6755659580230713, epoch: 292\n",
            "tensor(0.5227, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.9036e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1977, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.675041913986206, epoch: 293\n",
            "tensor(0.5211, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.8736e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1957, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.674515724182129, epoch: 294\n",
            "tensor(0.5196, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.8442e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1936, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6740, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6739871501922607, epoch: 295\n",
            "tensor(0.5182, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.8152e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1916, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6735, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.673457622528076, epoch: 296\n",
            "tensor(0.5167, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.7867e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1896, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6729, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6729254722595215, epoch: 297\n",
            "tensor(0.5152, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.7586e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1876, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6724, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.672391891479492, epoch: 298\n",
            "tensor(0.5137, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.7307e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1856, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6719, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6718556880950928, epoch: 299\n",
            "tensor(0.5122, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.7032e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1835, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6713, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6713180541992188, epoch: 300\n",
            "tensor(0.5107, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.6760e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1815, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6708, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.670778751373291, epoch: 301\n",
            "tensor(0.5093, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.6489e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1795, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6702, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6702370643615723, epoch: 302\n",
            "tensor(0.5078, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.6221e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1775, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6697, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.669694185256958, epoch: 303\n",
            "tensor(0.5064, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.5955e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1755, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6691, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.66914963722229, epoch: 304\n",
            "tensor(0.5049, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.5691e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1735, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6686, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.668602705001831, epoch: 305\n",
            "tensor(0.5035, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.5429e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1715, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6681, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6680545806884766, epoch: 306\n",
            "tensor(0.5020, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.5169e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1695, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6675, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.66750431060791, epoch: 307\n",
            "tensor(0.5006, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.4911e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1676, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6670, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6669533252716064, epoch: 308\n",
            "tensor(0.4992, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.4654e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1656, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6664, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6663992404937744, epoch: 309\n",
            "tensor(0.4977, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.4399e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1636, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6658, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.665843963623047, epoch: 310\n",
            "tensor(0.4963, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.4146e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1616, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6653, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.665287494659424, epoch: 311\n",
            "tensor(0.4949, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.3894e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1596, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6647, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.664729356765747, epoch: 312\n",
            "tensor(0.4935, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.3644e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1577, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6642, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6641690731048584, epoch: 313\n",
            "tensor(0.4921, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.3397e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1557, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6636, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6636078357696533, epoch: 314\n",
            "tensor(0.4907, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.3151e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1537, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6630, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6630444526672363, epoch: 315\n",
            "tensor(0.4893, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.2908e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1518, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6625, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6624796390533447, epoch: 316\n",
            "tensor(0.4879, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.2668e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1498, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6619, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6619131565093994, epoch: 317\n",
            "tensor(0.4865, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.2430e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1479, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6613, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6613457202911377, epoch: 318\n",
            "tensor(0.4851, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.2196e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1459, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6608, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.660776138305664, epoch: 319\n",
            "tensor(0.4838, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.1965e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1440, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6602, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.660205125808716, epoch: 320\n",
            "tensor(0.4824, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.1739e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1420, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6596, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.659633159637451, epoch: 321\n",
            "tensor(0.4810, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.1515e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1401, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6591, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.659059524536133, epoch: 322\n",
            "tensor(0.4797, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.1296e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1382, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6585, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6584842205047607, epoch: 323\n",
            "tensor(0.4783, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.1082e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1362, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6579, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.657907247543335, epoch: 324\n",
            "tensor(0.4770, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.0871e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1343, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6573, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6573290824890137, epoch: 325\n",
            "tensor(0.4756, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.0665e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1324, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6567, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.656749725341797, epoch: 326\n",
            "tensor(0.4743, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.0463e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1304, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6562, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6561686992645264, epoch: 327\n",
            "tensor(0.4729, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.0265e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1285, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6556, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6555862426757812, epoch: 328\n",
            "tensor(0.4716, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.0071e-06, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1266, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6550, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.655003309249878, epoch: 329\n",
            "tensor(0.4703, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.8820e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1247, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6544, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6544179916381836, epoch: 330\n",
            "tensor(0.4690, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.6968e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1228, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6538, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.653831958770752, epoch: 331\n",
            "tensor(0.4676, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.5156e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1209, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6532, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6532440185546875, epoch: 332\n",
            "tensor(0.4663, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.3384e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1190, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6527, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6526548862457275, epoch: 333\n",
            "tensor(0.4650, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.1650e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1171, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6521, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.652064800262451, epoch: 334\n",
            "tensor(0.4637, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.9952e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1152, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6515, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6514735221862793, epoch: 335\n",
            "tensor(0.4624, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.8289e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1133, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6509, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.650881052017212, epoch: 336\n",
            "tensor(0.4611, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.6660e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1114, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6503, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.65028715133667, epoch: 337\n",
            "tensor(0.4598, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.5063e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1095, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6497, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.649691581726074, epoch: 338\n",
            "tensor(0.4585, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.3497e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1076, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6491, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6490957736968994, epoch: 339\n",
            "tensor(0.4573, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.1961e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1058, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6485, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.648498296737671, epoch: 340\n",
            "tensor(0.4560, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.0454e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1039, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6479, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.647899627685547, epoch: 341\n",
            "tensor(0.4547, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.8975e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1020, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6473, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6473002433776855, epoch: 342\n",
            "tensor(0.4534, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.7522e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.1001, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6467, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6466991901397705, epoch: 343\n",
            "tensor(0.4522, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.6097e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0983, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6461, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.64609694480896, epoch: 344\n",
            "tensor(0.4509, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.4696e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0964, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6455, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.645493745803833, epoch: 345\n",
            "tensor(0.4497, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.3323e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0946, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6449, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6448898315429688, epoch: 346\n",
            "tensor(0.4484, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.1975e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0927, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6443, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.644284725189209, epoch: 347\n",
            "tensor(0.4472, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.0653e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0908, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6437, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6436779499053955, epoch: 348\n",
            "tensor(0.4459, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.9358e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0890, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6431, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6430702209472656, epoch: 349\n",
            "tensor(0.4447, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.8089e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0872, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6425, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6424622535705566, epoch: 350\n",
            "tensor(0.4435, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.6846e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0853, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6419, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.641853094100952, epoch: 351\n",
            "tensor(0.4422, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.5629e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0835, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6412, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.641242504119873, epoch: 352\n",
            "tensor(0.4410, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.4437e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0816, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6406, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6406311988830566, epoch: 353\n",
            "tensor(0.4398, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.3270e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0798, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6400, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.640019178390503, epoch: 354\n",
            "tensor(0.4386, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.2126e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0780, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6394, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6394057273864746, epoch: 355\n",
            "tensor(0.4374, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.1006e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0762, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6388, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.638791084289551, epoch: 356\n",
            "tensor(0.4362, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.9905e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0743, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6382, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.638176202774048, epoch: 357\n",
            "tensor(0.4349, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.8825e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0725, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6376, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6375603675842285, epoch: 358\n",
            "tensor(0.4338, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.7762e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0707, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6369, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6369428634643555, epoch: 359\n",
            "tensor(0.4326, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.6716e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0689, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6363, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.636324644088745, epoch: 360\n",
            "tensor(0.4314, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.5685e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0671, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6357, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6357059478759766, epoch: 361\n",
            "tensor(0.4302, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.4668e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0653, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6351, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.63508677482605, epoch: 362\n",
            "tensor(0.4290, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.3664e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0635, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6345, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6344666481018066, epoch: 363\n",
            "tensor(0.4278, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.2672e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0617, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6338, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.633845329284668, epoch: 364\n",
            "tensor(0.4267, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.1692e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0599, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6332, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.633223056793213, epoch: 365\n",
            "tensor(0.4255, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.0723e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0581, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6326, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6326005458831787, epoch: 366\n",
            "tensor(0.4243, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.9765e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0563, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6320, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.631976366043091, epoch: 367\n",
            "tensor(0.4232, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.8820e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0545, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6314, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6313517093658447, epoch: 368\n",
            "tensor(0.4220, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.7887e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0527, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6307, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6307265758514404, epoch: 369\n",
            "tensor(0.4209, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.6966e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0510, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6301, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.630100727081299, epoch: 370\n",
            "tensor(0.4197, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.6060e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0492, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6295, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.629474401473999, epoch: 371\n",
            "tensor(0.4186, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.5168e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0474, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6288, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6288464069366455, epoch: 372\n",
            "tensor(0.4174, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.4291e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0456, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6282, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.628218650817871, epoch: 373\n",
            "tensor(0.4163, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.3429e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0439, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6276, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.627589702606201, epoch: 374\n",
            "tensor(0.4152, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.2585e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0421, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6270, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.626960515975952, epoch: 375\n",
            "tensor(0.4140, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.1757e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0404, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6263, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6263301372528076, epoch: 376\n",
            "tensor(0.4129, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.0946e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0386, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6257, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.625699281692505, epoch: 377\n",
            "tensor(0.4118, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.0154e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0369, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6251, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.625067949295044, epoch: 378\n",
            "tensor(0.4107, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.9379e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0351, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6244, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6244359016418457, epoch: 379\n",
            "tensor(0.4096, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.8622e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0334, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6238, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6238036155700684, epoch: 380\n",
            "tensor(0.4085, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.7884e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0316, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6232, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6231698989868164, epoch: 381\n",
            "tensor(0.4074, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.7163e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0299, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6225, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6225366592407227, epoch: 382\n",
            "tensor(0.4063, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.6460e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0282, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6219, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6219019889831543, epoch: 383\n",
            "tensor(0.4052, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.5775e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0264, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6213, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.621267318725586, epoch: 384\n",
            "tensor(0.4041, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.5106e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0247, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6206, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6206319332122803, epoch: 385\n",
            "tensor(0.4030, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.4454e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0230, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6200, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.619995355606079, epoch: 386\n",
            "tensor(0.4019, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.3818e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0213, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6194, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.619359254837036, epoch: 387\n",
            "tensor(0.4008, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.3198e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0195, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6187, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6187219619750977, epoch: 388\n",
            "tensor(0.3997, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.2592e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0178, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6181, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.61808443069458, epoch: 389\n",
            "tensor(0.3987, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.2000e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0161, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6174, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6174466609954834, epoch: 390\n",
            "tensor(0.3976, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.1422e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0144, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6168, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.616807222366333, epoch: 391\n",
            "tensor(0.3965, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.0856e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0127, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6162, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.61616849899292, epoch: 392\n",
            "tensor(0.3955, device='cuda:0', grad_fn=<MulBackward0>) tensor(3.0302e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0110, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6155, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6155290603637695, epoch: 393\n",
            "tensor(0.3944, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.9760e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0093, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6149, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.61488938331604, epoch: 394\n",
            "tensor(0.3934, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.9229e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0076, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6142, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6142494678497314, epoch: 395\n",
            "tensor(0.3923, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.8708e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0059, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6136, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6136085987091064, epoch: 396\n",
            "tensor(0.3913, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.8198e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0042, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6130, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6129674911499023, epoch: 397\n",
            "tensor(0.3902, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.7696e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0026, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6123, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.612326145172119, epoch: 398\n",
            "tensor(0.3892, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.7203e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-4.0009, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6117, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6116840839385986, epoch: 399\n",
            "tensor(0.3882, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.6719e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9992, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6110, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.611041784286499, epoch: 400\n",
            "tensor(0.3871, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.6244e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9975, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6104, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6103992462158203, epoch: 401\n",
            "tensor(0.3861, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.5776e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9959, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6098, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6097562313079834, epoch: 402\n",
            "tensor(0.3851, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.5317e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9942, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6091, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6091129779815674, epoch: 403\n",
            "tensor(0.3841, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.4865e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9925, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6085, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.608469009399414, epoch: 404\n",
            "tensor(0.3831, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.4421e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9909, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6078, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.60782527923584, epoch: 405\n",
            "tensor(0.3820, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.3984e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9892, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6072, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6071808338165283, epoch: 406\n",
            "tensor(0.3810, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.3554e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9876, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6065, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.606536865234375, epoch: 407\n",
            "tensor(0.3800, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.3133e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9859, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6059, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6058919429779053, epoch: 408\n",
            "tensor(0.3790, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.2718e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9843, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6052, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6052472591400146, epoch: 409\n",
            "tensor(0.3780, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.2311e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9826, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6046, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6046016216278076, epoch: 410\n",
            "tensor(0.3770, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.1910e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9810, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6040, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6039562225341797, epoch: 411\n",
            "tensor(0.3760, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.1517e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9794, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6033, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6033105850219727, epoch: 412\n",
            "tensor(0.3751, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.1132e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9777, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6027, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6026647090911865, epoch: 413\n",
            "tensor(0.3741, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.0752e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9761, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6020, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6020185947418213, epoch: 414\n",
            "tensor(0.3731, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.0381e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9745, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6014, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.601372003555298, epoch: 415\n",
            "tensor(0.3721, device='cuda:0', grad_fn=<MulBackward0>) tensor(2.0015e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9729, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6007, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.6007256507873535, epoch: 416\n",
            "tensor(0.3712, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.9657e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9712, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.6001, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.600078821182251, epoch: 417\n",
            "tensor(0.3702, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.9305e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9696, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5994, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5994322299957275, epoch: 418\n",
            "tensor(0.3692, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.8960e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9680, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5988, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.598785161972046, epoch: 419\n",
            "tensor(0.3683, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.8622e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9664, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5981, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5981380939483643, epoch: 420\n",
            "tensor(0.3673, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.8290e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9648, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5975, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5974905490875244, epoch: 421\n",
            "tensor(0.3664, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.7964e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9632, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5968, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5968434810638428, epoch: 422\n",
            "tensor(0.3654, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.7645e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9616, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5962, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.596196174621582, epoch: 423\n",
            "tensor(0.3645, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.7332e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9600, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5955, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.595548391342163, epoch: 424\n",
            "tensor(0.3635, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.7025e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9584, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5949, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.594900369644165, epoch: 425\n",
            "tensor(0.3626, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.6724e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9568, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5943, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.594252824783325, epoch: 426\n",
            "tensor(0.3616, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.6430e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9552, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5936, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.593604803085327, epoch: 427\n",
            "tensor(0.3607, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.6140e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9537, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5930, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5929572582244873, epoch: 428\n",
            "tensor(0.3598, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.5857e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9521, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5923, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5923094749450684, epoch: 429\n",
            "tensor(0.3588, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.5579e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9505, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5917, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.591660737991333, epoch: 430\n",
            "tensor(0.3579, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.5307e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9489, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5910, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5910134315490723, epoch: 431\n",
            "tensor(0.3570, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.5040e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9474, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5904, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.590364933013916, epoch: 432\n",
            "tensor(0.3561, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.4779e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9458, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5897, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.589716911315918, epoch: 433\n",
            "tensor(0.3552, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.4522e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9442, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5891, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.58906888961792, epoch: 434\n",
            "tensor(0.3543, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.4270e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9427, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5884, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.588420867919922, epoch: 435\n",
            "tensor(0.3534, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.4024e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9411, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5878, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5877726078033447, epoch: 436\n",
            "tensor(0.3525, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.3781e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9396, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5871, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.587124824523926, epoch: 437\n",
            "tensor(0.3516, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.3544e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9380, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5865, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5864768028259277, epoch: 438\n",
            "tensor(0.3507, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.3310e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9365, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5858, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.585829257965088, epoch: 439\n",
            "tensor(0.3498, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.3081e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9350, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5852, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.58518123626709, epoch: 440\n",
            "tensor(0.3489, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.2856e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9334, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5845, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.58453369140625, epoch: 441\n",
            "tensor(0.3480, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.2635e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9319, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5839, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.583885431289673, epoch: 442\n",
            "tensor(0.3471, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.2418e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9303, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5832, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.583237886428833, epoch: 443\n",
            "tensor(0.3462, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.2205e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9288, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5826, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5825908184051514, epoch: 444\n",
            "tensor(0.3454, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.1995e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9273, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5819, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5819430351257324, epoch: 445\n",
            "tensor(0.3445, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.1789e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9258, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5813, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.581295967102051, epoch: 446\n",
            "tensor(0.3436, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.1586e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9243, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5806, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.580648422241211, epoch: 447\n",
            "tensor(0.3427, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.1387e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9227, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5800, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5800015926361084, epoch: 448\n",
            "tensor(0.3419, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.1192e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9212, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5794, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.579354763031006, epoch: 449\n",
            "tensor(0.3410, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.0999e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9197, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5787, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5787079334259033, epoch: 450\n",
            "tensor(0.3402, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.0810e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9182, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.578061580657959, epoch: 451\n",
            "tensor(0.3393, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.0625e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9167, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5774, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5774147510528564, epoch: 452\n",
            "tensor(0.3385, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.0442e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9152, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5768, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.576768398284912, epoch: 453\n",
            "tensor(0.3376, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.0263e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9137, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5761, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.576122760772705, epoch: 454\n",
            "tensor(0.3368, device='cuda:0', grad_fn=<MulBackward0>) tensor(1.0086e-07, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9122, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5754764080047607, epoch: 455\n",
            "tensor(0.3359, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.9129e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9108, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5748, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5748307704925537, epoch: 456\n",
            "tensor(0.3351, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.7425e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9093, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5742, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5741848945617676, epoch: 457\n",
            "tensor(0.3343, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.5750e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9078, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5735, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5735397338867188, epoch: 458\n",
            "tensor(0.3334, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.4103e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9063, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5729, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5728952884674072, epoch: 459\n",
            "tensor(0.3326, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.2485e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9048, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5723, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5722503662109375, epoch: 460\n",
            "tensor(0.3318, device='cuda:0', grad_fn=<MulBackward0>) tensor(9.0895e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9034, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5716, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.571605682373047, epoch: 461\n",
            "tensor(0.3309, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.9332e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9019, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5710, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5709614753723145, epoch: 462\n",
            "tensor(0.3301, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.7797e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.9004, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5703, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.570317029953003, epoch: 463\n",
            "tensor(0.3293, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.6288e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8990, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5697, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5696732997894287, epoch: 464\n",
            "tensor(0.3285, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.4806e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8975, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5690, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5690300464630127, epoch: 465\n",
            "tensor(0.3277, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.3350e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8961, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5684, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5683863162994385, epoch: 466\n",
            "tensor(0.3269, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.1920e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8946, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5677, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5677437782287598, epoch: 467\n",
            "tensor(0.3261, device='cuda:0', grad_fn=<MulBackward0>) tensor(8.0516e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8932, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5671, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.567101001739502, epoch: 468\n",
            "tensor(0.3253, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.9138e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8917, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5665, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5664584636688232, epoch: 469\n",
            "tensor(0.3245, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.7785e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8903, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5658, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5658161640167236, epoch: 470\n",
            "tensor(0.3237, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.6457e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8889, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5652, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5651750564575195, epoch: 471\n",
            "tensor(0.3229, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.5155e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8874, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5645, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.564533233642578, epoch: 472\n",
            "tensor(0.3221, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.3878e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8860, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5639, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.563892364501953, epoch: 473\n",
            "tensor(0.3213, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.2625e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8846, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5633, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5632517337799072, epoch: 474\n",
            "tensor(0.3205, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.1396e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8831, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5626, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5626113414764404, epoch: 475\n",
            "tensor(0.3197, device='cuda:0', grad_fn=<MulBackward0>) tensor(7.0192e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8817, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5620, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.561971664428711, epoch: 476\n",
            "tensor(0.3190, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.9012e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8803, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5613, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5613317489624023, epoch: 477\n",
            "tensor(0.3182, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.7856e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8789, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5607, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.560692310333252, epoch: 478\n",
            "tensor(0.3174, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.6724e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8775, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5601, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5600531101226807, epoch: 479\n",
            "tensor(0.3167, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.5616e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8761, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5594, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.559415102005005, epoch: 480\n",
            "tensor(0.3159, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.4531e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8747, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5588, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.55877685546875, epoch: 481\n",
            "tensor(0.3151, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.3469e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8733, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5581, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5581390857696533, epoch: 482\n",
            "tensor(0.3144, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.2429e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8719, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5575, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.557502031326294, epoch: 483\n",
            "tensor(0.3136, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.1412e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8705, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5569, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5568649768829346, epoch: 484\n",
            "tensor(0.3128, device='cuda:0', grad_fn=<MulBackward0>) tensor(6.0417e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8691, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5562, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5562281608581543, epoch: 485\n",
            "tensor(0.3121, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.9443e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8677, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5556, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5555925369262695, epoch: 486\n",
            "tensor(0.3113, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.8489e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8663, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5550, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5549569129943848, epoch: 487\n",
            "tensor(0.3106, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.7557e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8649, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5543, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.554321765899658, epoch: 488\n",
            "tensor(0.3099, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.6644e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8635, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5537, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5536868572235107, epoch: 489\n",
            "tensor(0.3091, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.5751e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8622, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5531, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5530529022216797, epoch: 490\n",
            "tensor(0.3084, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.4876e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8608, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5524, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5524187088012695, epoch: 491\n",
            "tensor(0.3076, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.4020e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8594, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5518, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.551785469055176, epoch: 492\n",
            "tensor(0.3069, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.3182e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8581, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5512, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.551152467727661, epoch: 493\n",
            "tensor(0.3062, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.2361e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8567, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5505, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.550520181655884, epoch: 494\n",
            "tensor(0.3055, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.1557e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8553, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5499, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5498883724212646, epoch: 495\n",
            "tensor(0.3047, device='cuda:0', grad_fn=<MulBackward0>) tensor(5.0769e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8540, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5493, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5492570400238037, epoch: 496\n",
            "tensor(0.3040, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.9997e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8526, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5486, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.548625946044922, epoch: 497\n",
            "tensor(0.3033, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.9241e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8513, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5480, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.5479955673217773, epoch: 498\n",
            "tensor(0.3026, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.8501e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8499, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5474, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.547365665435791, epoch: 499\n",
            "tensor(0.3019, device='cuda:0', grad_fn=<MulBackward0>) tensor(4.7777e-08, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.8486, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor(-3.5467, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "loss: -3.546736717224121, epoch: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(torch.ones(data.num_nodes, data.num_nodes), diagonal=1).bool()\n",
        "adj_vec = torch.Tensor(adj_tsr).unsqueeze(0)[:,mask].view(1, -1)"
      ],
      "metadata": {
        "id": "xmV_Sma4_wt4"
      },
      "execution_count": 534,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj_vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVmtbgxdAQFH",
        "outputId": "0375b3ad-d462-4066-b035-e41d61f88c48"
      },
      "execution_count": 535,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 535
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.relu(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVb3mhao_qiS",
        "outputId": "0df99b0a-2c8b-4123-80f3-7805a8058574"
      },
      "execution_count": 536,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3115, 0.1288, 0.5070, 0.5559, 0.1270, 0.1224, 0.1250, 0.1222, 0.3443,\n",
              "         0.1311, 0.1707, 0.1298, 0.1978, 0.1249, 0.1529, 0.1226, 0.2628, 0.2759,\n",
              "         0.1241, 0.2341, 0.1568, 0.1437, 0.3054, 0.1462, 0.3312, 0.1446, 0.1450,\n",
              "         0.1855, 0.1319, 0.5599, 0.1320, 0.3407, 0.1326, 0.1262, 0.2305, 0.2552,\n",
              "         0.1911, 0.1561, 0.1308, 0.1537, 0.2806, 0.5156, 0.1398, 0.2171, 0.2288,\n",
              "         0.1882, 0.1262, 0.1324, 0.2327, 0.1188, 0.4478, 0.1239, 0.2950, 0.1232,\n",
              "         0.2349, 0.4774, 0.2337, 0.2186, 0.3147, 0.1253, 0.1260, 0.1499, 0.1712,\n",
              "         0.3245, 0.1148, 0.1127, 0.1198, 0.1207, 0.5143, 0.3894, 0.1734, 0.2191,\n",
              "         0.4654, 0.1149, 0.1216, 0.1187, 0.1219, 0.1236, 0.4694, 0.1277, 0.1210,\n",
              "         0.5324, 0.1225, 0.1266, 0.3276, 0.5593, 0.4193, 0.1857, 0.1241, 0.1789,\n",
              "         0.1457, 0.1250, 0.1282, 0.1327, 0.3317, 0.1272, 0.1262, 0.2098, 0.1425,\n",
              "         0.2429, 0.2874, 0.1211, 0.3407, 0.4708, 0.1184, 0.4616, 0.4134, 0.1198,\n",
              "         0.1204, 0.2042, 0.1261, 0.1424, 0.3254, 0.1184, 0.5674, 0.1826, 0.2705,\n",
              "         0.1931, 0.1240, 0.5064, 0.1179, 0.3457, 0.1157, 0.1408, 0.2219, 0.1465,\n",
              "         0.3325, 0.2986, 0.1320, 0.1253, 0.1924, 0.1260, 0.4051, 0.5125, 0.2773,\n",
              "         0.1198, 0.1267, 0.1268, 0.1194, 0.4122, 0.4508, 0.3073, 0.4280, 0.2198,\n",
              "         0.1236, 0.3983, 0.4371, 0.1273, 0.4121, 0.2754, 0.4091, 0.1198, 0.1831,\n",
              "         0.2569, 0.1271, 0.1193, 0.3201, 0.5421, 0.2346, 0.2045, 0.1256, 0.3655,\n",
              "         0.1794, 0.3318, 0.2097, 0.3966, 0.4309, 0.2644, 0.1310, 0.1304, 0.3032,\n",
              "         0.1254, 0.1229, 0.1989, 0.1983, 0.3657, 0.1201, 0.4274, 0.1372, 0.3081,\n",
              "         0.1346, 0.1199, 0.5917, 0.4077, 0.1319, 0.1363, 0.1313, 0.2117, 0.1897,\n",
              "         0.2084, 0.1290, 0.1235, 0.2281, 0.1226, 0.1974, 0.3043, 0.5462, 0.1283,\n",
              "         0.1295, 0.1289, 0.3845, 0.1868, 0.1172, 0.3391, 0.1454, 0.2048, 0.3422,\n",
              "         0.2882, 0.4033, 0.5301, 0.3307, 0.2743, 0.5221, 0.1230, 0.4822, 0.4404,\n",
              "         0.1493, 0.1102, 0.1135, 0.1959, 0.1218, 0.1195, 0.5480, 0.2020, 0.1292,\n",
              "         0.1406, 0.1286, 0.4174, 0.5691, 0.5294, 0.2083, 0.3080, 0.2798, 0.1779,\n",
              "         0.1201, 0.3757, 0.1171, 0.1239, 0.1364, 0.1672, 0.1146, 0.3807, 0.1715,\n",
              "         0.1233, 0.2814, 0.1277, 0.1222, 0.1199, 0.1418, 0.2469, 0.4985, 0.1256,\n",
              "         0.1181, 0.4672, 0.4064, 0.1222, 0.4904, 0.2221, 0.1200, 0.3145, 0.1329,\n",
              "         0.1330, 0.1277, 0.1253, 0.2198, 0.1275, 0.1264, 0.2995, 0.2819, 0.1212,\n",
              "         0.3330, 0.1198, 0.1262, 0.4754, 0.3105, 0.1859, 0.1201, 0.1187, 0.1533,\n",
              "         0.1183, 0.1962, 0.5411, 0.3155, 0.1253, 0.3499, 0.1464, 0.2016, 0.2029,\n",
              "         0.3363, 0.5130, 0.1190, 0.1192, 0.1911, 0.1315, 0.1357, 0.4419, 0.5508,\n",
              "         0.2222, 0.1294, 0.3830, 0.1218, 0.3984, 0.2330, 0.1370, 0.3698, 0.4692,\n",
              "         0.1335, 0.5008, 0.1302, 0.1273, 0.1244, 0.1297, 0.1204, 0.4020, 0.4355,\n",
              "         0.3108, 0.1221, 0.5438, 0.2072, 0.3948, 0.1547, 0.1324, 0.1321, 0.1315,\n",
              "         0.5371, 0.1309, 0.4887, 0.3513, 0.1259, 0.1270, 0.3507, 0.3542, 0.1610,\n",
              "         0.1278, 0.1217, 0.1268, 0.1404, 0.1210, 0.1181, 0.2814, 0.1278, 0.1212,\n",
              "         0.1275, 0.1214, 0.2395, 0.1331, 0.5110, 0.4104, 0.2212, 0.1285, 0.1298,\n",
              "         0.2878, 0.1265, 0.1270, 0.5273, 0.1590, 0.1285, 0.1273, 0.4856, 0.1271,\n",
              "         0.2055, 0.5087, 0.3030, 0.2978, 0.1276, 0.5215, 0.1305, 0.1841, 0.1656,\n",
              "         0.1168, 0.2735, 0.1255, 0.1160, 0.1301, 0.4995, 0.1169, 0.4640, 0.1436,\n",
              "         0.1240, 0.1340, 0.1327, 0.1364, 0.1342, 0.3719, 0.1207, 0.1271, 0.2843,\n",
              "         0.1220, 0.1241, 0.2543, 0.3101, 0.1195, 0.3187, 0.3822, 0.1216, 0.4145,\n",
              "         0.3431, 0.1353, 0.1319, 0.1748, 0.1326, 0.3075, 0.1306, 0.1272, 0.1345,\n",
              "         0.1412, 0.1344, 0.1313, 0.1663, 0.1541, 0.2089, 0.2651, 0.1807, 0.1212,\n",
              "         0.1216, 0.2744, 0.1182, 0.5024, 0.4005, 0.2354]], device='cuda:0',\n",
              "       grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 536
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hqoe5vKr2he"
      },
      "source": [
        "## From signal estimate Laplacian Matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ADMM():\n",
        "    def __init__(self, l2_penalty, log_penalty, step_size=1e-02, relaxation_factor = 1.8):\n",
        "        self.alpha = log_penalty  # the penalty before log barrier\n",
        "        self.beta = l2_penalty  # the penalty before l2 term\n",
        "        self.gn = step_size\n",
        "        self.relax = relaxation_factor\n",
        "\n",
        "    def initialisation(self, l, m, batch_size=1):\n",
        "        w = torch.zeros((batch_size, l)).float().to(device)\n",
        "        v = torch.zeros((batch_size, m)).float().to(device)\n",
        "        return w, v\n",
        "\n",
        "    def prox_log_barrier(self, y, gn, alpha):\n",
        "        up = y ** 2 + 4 * gn * alpha\n",
        "        up = torch.clamp(up, 1e-08)\n",
        "        return (y - torch.sqrt(up)) / 2\n",
        "\n",
        "    def objective(self, w, D, z):\n",
        "        f1 = self.beta * torch.norm(w, 2) ** 2\n",
        "        f2 = w.T @ z\n",
        "        f3 = - self.alpha * torch.sum(torch.log(D @ w))\n",
        "\n",
        "        if all(np.round(w, 4) >= 0):\n",
        "            return f1 + f2 + f3\n",
        "        else:\n",
        "            return 10**3\n",
        "\n",
        "    def solve(self, smooth, max_iter=1000, verbose=True):\n",
        "        batch_size, m, _, r = smooth.shape\n",
        "        mask = torch.triu(torch.ones(m, m), diagonal=1).bool()\n",
        "        z = smooth[:, mask].view(batch_size, -1)\n",
        "        l = int(m*(m-1)*r/2)\n",
        "\n",
        "        D_ori = coo_to_sparseTensor(get_degree_operator(m)).to(device)\n",
        "        eye = torch.eye(int(m*(m-1)/2))\n",
        "        shift_sum = eye.repeat_interleave(r, dim=1)\n",
        "        D = D_ori @ shift_sum.to(device)\n",
        "\n",
        "        # D * shift_sum if the new operator\n",
        "\n",
        "        # initialise:\n",
        "        w, v = self.initialisation(l, m, batch_size)\n",
        "        zero_vec = torch.zeros((batch_size, l)).to(device)\n",
        "        # w_list = torch.empty(size=(batch_size, max_ite, l))\n",
        "        # print(w_list.shape)\n",
        "\n",
        "        lambda_ = self.relax\n",
        "\n",
        "        for i in range(max_iter):\n",
        "\n",
        "            y1 = w - self.gn * (2 * self.beta * w + torch.matmul(v, D))\n",
        "            p1 = torch.max(zero_vec, y1 - 2 * self.gn * z)\n",
        "\n",
        "            y2 = v + self.gn * torch.matmul(2 * p1 - w, D.T)\n",
        "            p2 = self.prox_log_barrier(y2, self.gn, self.alpha)\n",
        "\n",
        "            w = w + lambda_ * (p1 - w)\n",
        "            v = v + lambda_ * (p2 - v)\n",
        "\n",
        "            # w_list[:, i, :] = w\n",
        "\n",
        "        return w\n",
        "\n",
        "#%%\n",
        "\n",
        "class PDS():\n",
        "    def __init__(self, l2_psi, log_psi, step_size):\n",
        "        self.alpha = log_psi  # the penalty before log barrier\n",
        "        self.beta = l2_psi  # the penalty before l2 term\n",
        "        self.gn = step_size\n",
        "\n",
        "    def prox_log_barrier(self, y, gn, alpha):\n",
        "        return (y - torch.sqrt(y ** 2 + 4 * gn * alpha)) / 2\n",
        "\n",
        "    def initialisation(self, l, m, batch_size):\n",
        "        w = torch.zeros((batch_size, l)).float().to(device)\n",
        "        v = torch.zeros((batch_size, m)).float().to(device)\n",
        "        return w, v\n",
        "\n",
        "    def solve(self, smooth, max_iter = 500):\n",
        "        # z \\in 1* m* m * r\n",
        "        batch_size, m, _, r = smooth.shape\n",
        "        mask = torch.triu(torch.ones(m, m), diagonal=1).bool()\n",
        "        z = smooth[:, mask].view(batch_size, -1)\n",
        "        l = int(m*(m-1)*r/2)\n",
        "\n",
        "        D_ori = coo_to_sparseTensor(get_degree_operator(m)).to(device)\n",
        "        eye = torch.eye(int(m*(m-1)/2))\n",
        "        shift_sum = eye.repeat_interleave(r, dim=1)\n",
        "        D = D_ori @ shift_sum.to(device)\n",
        "\n",
        "        # initialise:\n",
        "        w, v = self.initialisation(l, m, batch_size)\n",
        "        zero_vec = torch.zeros((batch_size, l)).to(device)\n",
        "        # w_list = torch.empty(size=(batch_size, max_iter, l)).to(device)\n",
        "        for i in range(max_iter):\n",
        "            # print(z.shape)\n",
        "            y1 = w - self.gn * (2 * self.beta * w + 2 * z + torch.matmul(v, D))\n",
        "            y2 = v + self.gn * torch.matmul(w, D.T)\n",
        "\n",
        "            p1 = torch.max(zero_vec, y1)\n",
        "            p2 = self.prox_log_barrier(y2, self.gn, self.alpha)\n",
        "\n",
        "            q1 = p1 - self.gn * (2 * self.beta * p1 + 2 * z + torch.matmul(p2, D))\n",
        "            q2 = p2 + self.gn * torch.matmul(p1, D.T)\n",
        "\n",
        "            w = w - y1 + q1\n",
        "            v = v - y2 + q2\n",
        "\n",
        "            # w_list[:, i, :] = w\n",
        "\n",
        "        return w\n"
      ],
      "metadata": {
        "id": "DiYPmEAFtNai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "import numpy as np\n",
        "def auc_test(model, loader, device):\n",
        "  preds = []\n",
        "  ground_truths = []\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  for test_batch in tqdm.tqdm(loader):\n",
        "      test_batch = test_batch.to(device)\n",
        "      preds.append(model(test_batch))\n",
        "      with torch.no_grad():\n",
        "        for edge_type in test_batch.edge_types:\n",
        "          ground_truths.append(test_batch[edge_type].edge_label)\n",
        "  pred = torch.cat(preds, dim=0).cpu().detach().numpy()\n",
        "  # pred = pred/np.max(pred)\n",
        "  ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
        "  auc = roc_auc_score(ground_truth, pred)\n",
        "  f1 = f1_score(ground_truth, (pred>0.5))\n",
        "  return auc, f1"
      ],
      "metadata": {
        "id": "C927h38nwdmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pds_opt = PDS(1, 1, 1e-2)"
      ],
      "metadata": {
        "id": "VYuxhkoA11ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "def estimate_w(signal_vtx, signals_edge, adj_tensor, optimizer):\n",
        "  diff_tensor = (signal_vtx[:, None, None, :] - signal_vtx[None,: , None, :]) * signals_edge[ None, None, :, :]\n",
        "  diff_tensor = torch.Tensor(diff_tensor)\n",
        "  num_nodes,_,num_relations ,K = diff_tensor.shape\n",
        "  smooth_tensor = torch.norm(diff_tensor, dim=-1)**2\n",
        "  # est_tensor = torch.exp(- 1/sigma * smooth_tensor)\n",
        "  adj_tensor = torch.Tensor(adj_tensor).unsqueeze(0).to(device)\n",
        "  mask = torch.triu(torch.ones(num_nodes, num_nodes), diagonal=1).bool()\n",
        "  adj_vec = adj_tensor[:, mask].view(1, -1)\n",
        "  print(smooth_tensor.shape)\n",
        "  w = optimizer.solve(smooth_tensor.unsqueeze(0).to(device), max_iter = 500)\n",
        "  # Normalization\n",
        "  w = w/torch.max(w)\n",
        "  GMSE_error = torch.sum(torch.square(w-adj_vec)/(w+1e-12))\n",
        "\n",
        "  num_samples = w.shape[0] * w.shape[1]*w.shape[0]\n",
        "  edge_indices = torch.where(adj_vec == 1)\n",
        "  # Use these indices to access the corresponding elements in B\n",
        "  out_edges = w[edge_indices]\n",
        "  print(adj_vec[edge_indices])\n",
        "  print(w)\n",
        "  print(out_edges)\n",
        "  link_error = torch.mean(torch.square(adj_vec[edge_indices] - out_edges))\n",
        "  print(GMSE_error/num_samples)\n",
        "  print(torch.sqrt(link_error))\n",
        "  auc = roc_auc_score(adj_vec.to('cpu')[0], w.to('cpu')[0])\n",
        "  f1 = f1_score(adj_vec.to('cpu')[0], (w>0.8).to('cpu')[0])\n",
        "  print(auc, f1)\n",
        "  # auc = roc_auc_score(adj_vec[edge_indices].to('cpu').detach().numpy(), out_edge\n",
        "\n",
        "\n",
        "estimate_w(signal_vtx, signals_edge, adj_tsr, pds_opt)"
      ],
      "metadata": {
        "id": "6Qm7BNj-oCca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "admm_opt = ADMM(l2_penalty=2, log_penalty=1, step_size=1e-02, relaxation_factor = 1.8)\n",
        "estimate_w(signal_vtx, signals_edge, adj_tsr, admm_opt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k1teprfrc85",
        "outputId": "ea8c3b76-d382-4c2d-8350-8a8a24717f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50, 50, 15])\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[ 9.3078e-02, -2.9427e-44,  5.7762e-02,  ..., -2.9427e-44,\n",
            "          1.6641e-01,  9.5891e-02]], device='cuda:0')\n",
            "tensor([0.9519, 0.8509, 0.8758, 0.9388, 1.0000, 0.8812, 0.9650, 0.9385, 0.9189,\n",
            "        0.8664, 0.8285, 0.8937, 0.8985, 0.9194, 0.9301, 0.9270, 0.9160, 0.9065,\n",
            "        0.8700, 0.9062, 0.9256, 0.9198, 0.8712, 0.9219, 0.9439, 0.8959, 0.9051,\n",
            "        0.8782, 0.8873, 0.9201, 0.9253, 0.9095, 0.8782, 0.8949, 0.9091, 0.9214,\n",
            "        0.8798, 0.8581, 0.9152, 0.9520, 0.9108, 0.8998, 0.9200, 0.9185, 0.8673,\n",
            "        0.8894, 0.9047, 0.9358, 0.9261, 0.9774], device='cuda:0')\n",
            "tensor(0.0736, device='cuda:0')\n",
            "tensor(0.0966, device='cuda:0')\n",
            "0.999587448840382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcO5vCZzw4BA"
      },
      "source": [
        "# Graph Sampling from huge heterogeneous graph\n",
        "\n",
        "Loading graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAVnfyAiPXVJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b147688-1776-456b-e052-37fe6aa03c34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.dropbox.com/s/yh4grpeks87ugr2/DBLP_processed.zip?dl=1\n",
            "Extracting data/dblp/raw/DBLP_processed.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import DBLP\n",
        "\n",
        "dataset_dblp = DBLP(root='./data/dblp')\n",
        "data = dataset_dblp[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Link Prediction"
      ],
      "metadata": {
        "id": "j5A_1jP9rmui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random.mtrand import noncentral_chisquare\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from torch_geometric.loader import HGTLoader\n",
        "\n",
        "def data_preprocessing(data):\n",
        "  # node type processing\n",
        "  # for node_type in data.node_types:\n",
        "  #   if data[node_type].x is None:\n",
        "  data['conference'].x = torch.eye(data['conference'].num_nodes)\n",
        "\n",
        "  # edge type processing\n",
        "  edge_onehot = torch.eye(len(data.edge_types))\n",
        "  i=0\n",
        "  for edge_type in data.edge_types:\n",
        "    data[edge_type].x = edge_onehot[i].reshape(1,-1)\n",
        "    i+=1\n",
        "  return data\n",
        "\n",
        "def data_spliting(data):\n",
        "  transform = RandomLinkSplit(num_val=0.3, num_test=0.2, \n",
        "                            is_undirected=True,\n",
        "                            neg_sampling_ratio=1.0,\n",
        "                            add_negative_train_samples=True,\n",
        "                            edge_types=data.edge_types)\n",
        "  train_test_split = transform(data)\n",
        "  data_list = []\n",
        "  for b_data in train_test_split:\n",
        "    b_data.generate_ids()\n",
        "    \n",
        "    for edge_type in b_data.edge_types:\n",
        "      b_data[edge_type].e_id = torch.arange(len(b_data[edge_type].edge_label))\n",
        "      b_data[edge_type].edge_index = b_data[edge_type].edge_label_index\n",
        "      del b_data[edge_type].edge_label_index\n",
        "    data_list.append(b_data)\n",
        "  \n",
        "  return data_list\n",
        "\n",
        "def data_batching(data, batch_size, input_nodes):\n",
        "    loader = HGTLoader(\n",
        "    data,\n",
        "    # Sample 512 nodes per type and per iteration for 4 iterations\n",
        "    num_samples={key: [8] * 4 for key in data.node_types},\n",
        "    # Use a batch size of 128 for sampling training nodes of type paper\n",
        "    batch_size = batch_size,\n",
        "    input_nodes=input_nodes,\n",
        "    )\n",
        "    return loader\n",
        "\n",
        "\n",
        "data = data_preprocessing(data)\n",
        "train_data, val_data, test_data = data_spliting(data)\n",
        "train_loader = data_batching(train_data, 8, ('paper'))\n",
        "val_loader = data_batching(val_data, 8, ('paper'))\n",
        "test_loader = data_batching(test_data, 4, ('paper'))"
      ],
      "metadata": {
        "id": "2k2yyvqZBGAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))\n",
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7_RdFuTdhtB",
        "outputId": "95585fa6-0ccc-40dd-e3b7-177989cefee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HeteroData(\n",
              "  \u001b[1mauthor\u001b[0m={\n",
              "    x=[29, 334],\n",
              "    y=[29],\n",
              "    train_mask=[29],\n",
              "    val_mask=[29],\n",
              "    test_mask=[29],\n",
              "    n_id=[29]\n",
              "  },\n",
              "  \u001b[1mpaper\u001b[0m={\n",
              "    x=[32, 4231],\n",
              "    n_id=[32],\n",
              "    input_id=[8],\n",
              "    batch_size=8\n",
              "  },\n",
              "  \u001b[1mterm\u001b[0m={\n",
              "    x=[32, 50],\n",
              "    n_id=[32]\n",
              "  },\n",
              "  \u001b[1mconference\u001b[0m={\n",
              "    num_nodes=10,\n",
              "    x=[10, 20],\n",
              "    n_id=[10]\n",
              "  },\n",
              "  \u001b[1m(author, to, paper)\u001b[0m={\n",
              "    edge_index=[2, 31],\n",
              "    x=[1, 6],\n",
              "    edge_label=[31],\n",
              "    e_id=[31]\n",
              "  },\n",
              "  \u001b[1m(paper, to, author)\u001b[0m={\n",
              "    edge_index=[2, 19],\n",
              "    x=[1, 6],\n",
              "    edge_label=[19],\n",
              "    e_id=[19]\n",
              "  },\n",
              "  \u001b[1m(paper, to, term)\u001b[0m={\n",
              "    edge_index=[2, 16],\n",
              "    x=[1, 6],\n",
              "    edge_label=[16],\n",
              "    e_id=[16]\n",
              "  },\n",
              "  \u001b[1m(paper, to, conference)\u001b[0m={\n",
              "    edge_index=[2, 0],\n",
              "    x=[1, 6],\n",
              "    edge_label=[0],\n",
              "    e_id=[0]\n",
              "  },\n",
              "  \u001b[1m(term, to, paper)\u001b[0m={\n",
              "    edge_index=[2, 34],\n",
              "    x=[1, 6],\n",
              "    edge_label=[34],\n",
              "    e_id=[34]\n",
              "  },\n",
              "  \u001b[1m(conference, to, paper)\u001b[0m={\n",
              "    edge_index=[2, 21],\n",
              "    x=[1, 6],\n",
              "    edge_label=[21],\n",
              "    e_id=[21]\n",
              "  }\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for edge_type in train_data.edge_types:\n",
        "  print(train_data[edge_type].edge_label.shape)\n",
        "for edge_type in train_data.edge_types:\n",
        "  print(train_data[edge_type].edge_index.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "530j0V6WrgIu",
        "outputId": "7dc07820-503a-4830-d9a8-e640b2d797ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([19646])\n",
            "torch.Size([19646])\n",
            "torch.Size([85810])\n",
            "torch.Size([14330])\n",
            "torch.Size([85810])\n",
            "torch.Size([14330])\n",
            "torch.Size([2, 19646])\n",
            "torch.Size([2, 19646])\n",
            "torch.Size([2, 85810])\n",
            "torch.Size([2, 14330])\n",
            "torch.Size([2, 85810])\n",
            "torch.Size([2, 14330])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_complete_subgraph(subgraph, graph):\n",
        "  for edge_type in subgraph.edge_types:\n",
        "    # assuming edge_index is your edge index tensor with shape [2, E]\n",
        "    # and sampled_nodes is your tensor of node indices to include in the subgraph\n",
        "\n",
        "    # get the start and end nodes for each edge\n",
        "    s_ntype, _, t_ntype = edge_type\n",
        "    s_mapping = {old_id: new_id for new_id, old_id in enumerate(subgraph[s_ntype].n_id.tolist())}\n",
        "    t_mapping = {old_id: new_id for new_id, old_id in enumerate(subgraph[t_ntype].n_id.tolist())}\n",
        "\n",
        "    s_nodes, t_nodes = graph[edge_type].edge_index\n",
        "\n",
        "    # find the indices of the edges where both the start node and end node are in sampled_nodes\n",
        "    mask = (torch.isin(s_nodes, subgraph[s_ntype].n_id) & torch.isin(t_nodes, subgraph[t_ntype].n_id))\n",
        "    print(mask.shape)\n",
        "\n",
        "    # subset the edge index tensor to include only these edges\n",
        "    subgraph_edge_index = graph[edge_type].edge_index[:, mask]\n",
        "    s_idx, t_idx = subgraph_edge_index\n",
        "    new_s_nodes = torch.tensor([s_mapping[node.item()] for node in s_idx])\n",
        "    new_e_nodes = torch.tensor([t_mapping[node.item()] for node in t_idx])\n",
        "\n",
        "    # create a new edge index tensor with the re-indexed node IDs\n",
        "    subgraph[edge_type].edge_index = torch.stack([new_s_nodes, new_e_nodes])\n",
        "    subgraph[edge_type].edge_label = graph[edge_type].edge_label[mask]\n",
        "    subgraph[edge_type].e_id = graph[edge_type].e_id[mask]\n",
        "  \n",
        "  return subgraph\n",
        "\n",
        "for batch in test_loader:\n",
        "  batch = get_complete_subgraph(batch, train_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "Vv6rioeGWUtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import HeteroConv, Linear, SAGEConv, HeteroDictLinear\n",
        "\n",
        "class NodeLinProj(torch.nn.Module):\n",
        "    def __init__(self, node_shape, out_channels):\n",
        "        super().__init__()\n",
        "        self.NodeLinear = HeteroDictLinear(in_channels=node_shape,out_channels=out_channels)\n",
        "        self.EdgeLinear = Linear(in_features=edge_shape, out_features=out_channels)\n",
        "        \n",
        "\n",
        "    def forward(self, batch):\n",
        "      node_attrs = {node_type: batch[node_type].x for node_type in batch.node_types}\n",
        "      edge_attrs = {'_'.join(edge_type): batch[edge_type].x for edge_type in batch.edge_types}\n",
        "\n",
        "      node_out = self.NodeLinear(node_attrs)\n",
        "      edge_out = {'_'.join(edge_type): self.EdgeLinear(batch[edge_type].x) for edge_type in batch.edge_types}(edge_attrs)\n",
        "      \n",
        "      for node_type in batch.node_types:\n",
        "        node_out[node_type] = node_out[node_type]/(node_out[node_type].norm(dim=1)[:, None])\n",
        "      for edge_type in edge_attrs:\n",
        "        edge_out[edge_type] = edge_out[edge_type]/(edge_out[edge_type].norm(dim=1)[:, None]) \n",
        "\n",
        "      return (node_out, edge_out)\n",
        "\n",
        "class EdgeLinProj(torch.nn.Module):\n",
        "  def __init__(self, edge_shape, out_channels):\n",
        "    self.EdgeLinear = Linear(in_features=edge_shape, out_features=out_channels)\n",
        "  \n",
        "  def forward(self, batch):\n",
        "    edge_attrs = {'_'.join(edge_type): batch[edge_type].x for edge_type in batch.edge_types}\n",
        "\n",
        "\n",
        "class Classifier(torch.nn.Module):\n",
        "    def forward(self, s_node_x, t_node_x ,r_emb, edge_index):\n",
        "        sigma = 1e-3\n",
        "        s_idx, t_idx = edge_index[0], edge_index[1]\n",
        "        s_nemb, t_nemb = s_node_x[s_idx], t_node_x[t_idx]\n",
        "        diff_vector = r_emb * (s_nemb - t_nemb)\n",
        "        smooth = torch.exp(- 1/sigma * torch.norm(diff_vector, dim=-1)**2)\n",
        "        # print(diff_vector.shape, torch.norm(diff_vector, dim=-1).shape)\n",
        "        # print(torch.norm(diff_vector, dim=-1)**2)\n",
        "        # smooth = torch.exp(-1/sigma* (torch.norm(diff_vector, dim=-1)))\n",
        "        \n",
        "        return smooth\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, node_shape, edge_shape, out_channels=100):\n",
        "        super().__init__()\n",
        "        self.hetero_linear = LinearProj(node_shape, edge_shape, out_channels=100)\n",
        "        self.classifier = Classifier()\n",
        "\n",
        "    def forward(self, batch):\n",
        "\n",
        "        # `x_dict` holds feature matrices of all node types\n",
        "        # `edge_index_dict` holds all edge indices of all edge types\n",
        "        node_out, edge_out = self.hetero_linear(batch)\n",
        "        pred_list = []\n",
        "        for edge_type in batch.edge_types:\n",
        "          s_ntype, _, t_ntype = edge_type\n",
        "          s_emb, t_emb = node_out[s_ntype], node_out[t_ntype]\n",
        "          r_emb = edge_out['_'.join(edge_type)]\n",
        "          # print(self.classifier(s_emb, t_emb, r_emb, batch[edge_type].edge_index).shape)\n",
        "          # print('gt: {}'.format(batch[edge_type].edge_label))\n",
        "          # print('pred:{}'.format(self.classifier(s_emb, t_emb, r_emb, batch[edge_type].edge_index)))\n",
        "          pred_list += [self.classifier(s_emb, t_emb, r_emb, batch[edge_type].edge_index)]\n",
        "        pred = torch.cat(pred_list)\n",
        "        return pred"
      ],
      "metadata": {
        "id": "ummtLl6jouRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_shape = {node_type: train_data[node_type].x.shape[-1] for node_type in train_data.node_types}\n",
        "# edge_shape = {'_'.join(edge_type): train_data[edge_type].x.shape[-1] for edge_type in train_data.edge_types}\n",
        "edge_shape = train_data[edge_type].x.shape[-1]\n",
        "\n",
        "model = Model(node_shape, edge_shape, out_channels=500)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']"
      ],
      "metadata": {
        "id": "3qFg0FHhpL8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "import numpy as np\n",
        "def auc_test(mode, loader, device):\n",
        "  preds = []\n",
        "  ground_truths = []\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  for test_batch in tqdm.tqdm(loader):\n",
        "      test_batch = test_batch.to(device)\n",
        "      preds.append(model(test_batch))\n",
        "      with torch.no_grad():\n",
        "        for edge_type in test_batch.edge_types:\n",
        "          ground_truths.append(test_batch[edge_type].edge_label)\n",
        "  pred = torch.cat(preds, dim=0).cpu().detach().numpy()\n",
        "  pred = pred/np.max(pred)\n",
        "  ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
        "  auc = roc_auc_score(ground_truth, pred)\n",
        "  f1 = f1_score(ground_truth, (pred>0.5))\n",
        "  return auc, f1"
      ],
      "metadata": {
        "id": "tZRbI6I48SDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.loader import HGTLoader\n",
        "from torch_geometric.loader import LinkNeighborLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: '{device}'\")\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=0)\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer, \n",
        "                                        T_0 = 8,# Number of iterations for the first restart\n",
        "                                        T_mult = 1, # A factor increases TiTi​ after a restart\n",
        "                                        eta_min = 1e-5) # Minimum learning rate\n",
        "# Adam best learning rate: 1e-2, sigma= 1e-3, weight decay = 0\n",
        "# SGD best learning rate: 1e-3, decay = 0.1, no sigma\n",
        "\n",
        "\n",
        "for epoch in range(1, 21):\n",
        "    total_loss = total_examples = 0\n",
        "    for batch in tqdm.tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        batch = batch.to(device)\n",
        "        pred = model(batch)\n",
        "        \n",
        "        gt_list = []\n",
        "        for edge_type in batch.edge_types:\n",
        "          gt_list.append(batch[edge_type].edge_label)\n",
        "        ground_truth = torch.cat(gt_list)\n",
        "        loss = F.binary_cross_entropy(pred, ground_truth, reduction='mean')\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * pred.numel()\n",
        "        total_examples += pred.numel()\n",
        "    scheduler.step()\n",
        "    \n",
        "    print(f\"Epoch: {epoch:03d}, Loss: {total_loss / total_examples:.4f}, learning_rate: {get_lr(optimizer):.4f}\")\n",
        "    if epoch % 10 == 0:\n",
        "      auc_result, f1_sco = auc_test(model,val_loader, device)\n",
        "      print(f\"AUC: {auc_result:.4f}, f1 score: {f1_sco:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek5vyZo9mqNI",
        "outputId": "705d1f3a-8655-42de-865d-44d2dd90abd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: 'cuda:0'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:32<00:00, 55.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 0.2719, learning_rate: 0.0096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:29<00:00, 60.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 002, Loss: 0.2668, learning_rate: 0.0085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:29<00:00, 59.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 003, Loss: 0.2626, learning_rate: 0.0069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:30<00:00, 58.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 004, Loss: 0.2570, learning_rate: 0.0050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:29<00:00, 61.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 005, Loss: 0.2516, learning_rate: 0.0031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:29<00:00, 61.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 006, Loss: 0.2465, learning_rate: 0.0015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:29<00:00, 61.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 007, Loss: 0.2434, learning_rate: 0.0004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:30<00:00, 59.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 008, Loss: 0.2412, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:29<00:00, 61.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 009, Loss: 0.2563, learning_rate: 0.0096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:29<00:00, 60.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 010, Loss: 0.2556, learning_rate: 0.0085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:15<00:00, 117.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.8055, f1 score: 0.7232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:31<00:00, 56.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 011, Loss: 0.2534, learning_rate: 0.0069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:30<00:00, 59.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 012, Loss: 0.2491, learning_rate: 0.0050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:29<00:00, 60.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 013, Loss: 0.2442, learning_rate: 0.0031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:29<00:00, 60.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 014, Loss: 0.2422, learning_rate: 0.0015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:29<00:00, 60.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 015, Loss: 0.2392, learning_rate: 0.0004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:30<00:00, 57.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 016, Loss: 0.2374, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:31<00:00, 57.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 017, Loss: 0.2501, learning_rate: 0.0096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:30<00:00, 58.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 018, Loss: 0.2491, learning_rate: 0.0085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:31<00:00, 56.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 019, Loss: 0.2480, learning_rate: 0.0069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:30<00:00, 58.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 020, Loss: 0.2445, learning_rate: 0.0050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:15<00:00, 115.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.7990, f1 score: 0.7138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auc_result, f1 = auc_test(model,val_loader, device)\n",
        "print(auc_result, f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsNFzMM4D5PF",
        "outputId": "6e5ce76e-bbb9-47ed-d046-e6f835fbc218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1791/1791 [00:17<00:00, 100.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7989262263970094 0.7137463638051619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'gdrive/MyDrive/heterograph_learning/linear_model.pth'\n",
        "torch.save(model.state_dict(), path)"
      ],
      "metadata": {
        "id": "NpHhum2n7jNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading model\n",
        "path = 'gdrive/MyDrive/heterograph_learning/linear_model.pth'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX-M3NMwYZow",
        "outputId": "c39f6cf0-f062-462b-c88b-1d849974c4b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_to_adjtensor(batch, node_out, edge_out):\n",
        "  # create an empty dictionary to store the indices\n",
        "  indices_dict = {}\n",
        "  # list to hold the tensors\n",
        "  tensor_list = []\n",
        "  start_index = 0\n",
        "  for node_type in batch.node_types:\n",
        "    node_emb = node_out[node_type]\n",
        "    end_index = start_index + node_emb.size(0)\n",
        "    indices_dict[node_type] = (start_index, end_index)  # store the start and end index for this type\n",
        "    tensor_list.append(node_emb)\n",
        "    start_index = end_index  # update the start index for the next type\n",
        "    # concatenate the tensors\n",
        "\n",
        "  node_tensor = torch.cat(tensor_list, dim=0)\n",
        "\n",
        "  adj_matrices = []\n",
        "  # print('start transforming')\n",
        "  for edge_type in batch.edge_types:\n",
        "    # print(edge_type, batch[edge_type].edge_index)\n",
        "    \n",
        "    if batch[edge_type].edge_index.shape[-1] ==0:\n",
        "        continue\n",
        "    # Initialize the adjacency matrix\n",
        "    N = node_tensor.size(0)\n",
        "    adj_matrix = torch.zeros(N, N)\n",
        "    # Populate the adjacency matrix\n",
        "    s_ntype, _, e_ntype = edge_type\n",
        "    start_indices = indices_dict[s_ntype]\n",
        "    end_indices = indices_dict[e_ntype]\n",
        "\n",
        "    # map the indices in edge_index to their corresponding indices in the big tensor\n",
        "    edge_idx = batch[edge_type].edge_index\n",
        "    edge_label = batch[edge_type].edge_label\n",
        "    edge_idx[0] += start_indices[0]  # add the start index of the start node type to the start nodes in edge_index\n",
        "    edge_idx[1] += end_indices[0]  # add the start index of the end node type to the end nodes in edge_index\n",
        "    \n",
        "    for i in range(edge_idx.shape[1]):\n",
        "      if edge_label[i] == 1:\n",
        "        one_edge = edge_idx[:, i]\n",
        "        adj_matrix[one_edge[0], one_edge[1]] = 1\n",
        "    adj_matrices.append(adj_matrix)\n",
        "\n",
        "  adj_matrices = torch.stack(adj_matrices, dim=-1)\n",
        "  return node_tensor, adj_matrices\n",
        "\n"
      ],
      "metadata": {
        "id": "Jl18YKLh7rmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "step = 0\n",
        "GMSE_error = 0\n",
        "total_samples = 0\n",
        "link_errors = []\n",
        "sigma = 1e-3\n",
        "\n",
        "for batch in tqdm.tqdm(test_loader):\n",
        "  batch.to(device)\n",
        "  model.to(device)\n",
        "  step+=1\n",
        "  node_out, edge_out = model.hetero_linear(batch)\n",
        "  n_emb, adj_tensor = batch_to_adjtensor(batch, node_out, edge_out)\n",
        "  # r_emb = [edge_out['_'.join(e_type)] for e_type in batch.edge_types if batch[e_type].edge_index.shape[0] != 0]\n",
        "  r_emb = [edge_out['_'.join(e_type)] for e_type in batch.edge_types if batch[e_type].edge_index.shape[-1] != 0]\n",
        "  \n",
        "  r_emb = torch.cat(r_emb, dim=0)\n",
        "  diff_tensor = (n_emb[:, None, None, :] - n_emb[None,: , None, :]) * r_emb[ None, None, :, :]\n",
        "  num_nodes,_,num_relations ,K = diff_tensor.shape\n",
        "  smooth_tensor = torch.norm(diff_tensor, dim=-1)**2\n",
        "  est_tensor = torch.exp(- 1/sigma * smooth_tensor)\n",
        "  \n",
        "  adj_tsr = adj_tensor.to(device)\n",
        "  \n",
        "  GMSE_error += torch.sum(torch.square(est_tensor-adj_tensor))\n",
        "\n",
        "  num_samples = est_tensor.shape[0] * est_tensor.shape[1]*est_tensor.shape[0]\n",
        "  total_samples += num_samples\n",
        "\n",
        "  edge_indices = torch.where(adj_tensor == 1)\n",
        "  # Use these indices to access the corresponding elements in B\n",
        "  out_edges = est_tensor[edge_indices]\n",
        "  link_error = torch.mean(torch.square(adj_tensor[edge_indices] - out_edges))\n",
        "  link_errors.append(torch.sqrt(link_error))\n",
        "\n",
        "print(GMSE_error/total_samples)\n",
        "print(torch.mean(torch.tensor(link_errors)))\n",
        "\n",
        "# Find the indices of the 1 elements in A\n",
        "\n",
        "'''\n",
        "  # combinations = itertools.combinations_with_replacement(batch.node_types, 2)\n",
        "  # possible_edge =  [(s_type,'to',t_type) for s_type,t_type in combinations]\n",
        "  true_edge_types = batch.edge_types\n",
        "  for edge_type in true_edge_types:\n",
        "    if batch[edge_type].edge_index.shape[-1] ==0:\n",
        "      continue\n",
        "    if edge_type in true_edge_types:\n",
        "      print(batch[edge_type].edge_index)\n",
        "    s_ntype, _ , t_ntype = edge_type\n",
        "    # source node embedding and target node embedding\n",
        "    s_nemb, t_nemb = node_out[s_ntype], node_out[t_ntype]\n",
        "    \n",
        "    n_emb = torch.cat([s_nemb, t_nemb], dim=0)\n",
        "    r_emb = edge_out['_'.join(edge_type)]\n",
        "    out_product = n_emb[:, None, :] * n_emb[None,: , :] * r_emb[0][ None, None, :]\n",
        "    N,_,K = out_product.shape\n",
        "    out_adj = torch.sum(out_product, dim=2)\n",
        "    print(out_adj.shape)\n",
        "    # Use torch.topk to get the indices of K largest entrie\n",
        "    values, indices = torch.topk(out_adj.view(-1), K)\n",
        "    row_indices = indices // N\n",
        "    col_indices = indices % N\n",
        "\n",
        "    indices = torch.stack([row_indices, col_indices], dim=0)\n",
        "    # Convert tensors to sets\n",
        "    true_idx = batch[edge_type].edge_index\n",
        "    true_idx[1] += s_nemb.shape[0]\n",
        "    set1 = set(map(tuple, true_idx.t().tolist()))\n",
        "    set2 = set(map(tuple, indices.t().tolist()))\n",
        "    # Find the difference between sets\n",
        "    print(set1,set2)\n",
        "    acc = len(set1 - set2)/len(set1)\n",
        "    print(acc)\n",
        "'''\n",
        "    \n",
        "  \n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "jy9Uo58f7zSQ",
        "outputId": "a25795a5-64c0-4c83-d955-dabd5c3a92df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3582 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-9158dd1eef86>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0madj_tsr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mestimate_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_tsr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpds_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0mGMSE_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest_tensor\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0madj_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pds_opt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(GMSE_error/total_samples)\n",
        "print(torch.mean(torch.tensor(link_errors)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD0Trli-ZW0p",
        "outputId": "e310ce79-6c17-4558-fc90-d19063f121bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0025, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.5301)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimate adjacency tensor"
      ],
      "metadata": {
        "id": "mIodzxzGQRV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import scipy.sparse as sparse\n",
        "\n",
        "def coo_to_sparseTensor(coo):\n",
        "    values = coo.data\n",
        "    indices = np.vstack((coo.row, coo.col))\n",
        "\n",
        "    i = torch.LongTensor(indices)\n",
        "    v = torch.FloatTensor(values)\n",
        "    shape = coo.shape\n",
        "\n",
        "    return torch.sparse.FloatTensor(i, v, torch.Size(shape)).to_dense()\n",
        "\n",
        "def get_degree_operator(m):\n",
        "  \n",
        "    ncols =int(m*(m - 1)/2)\n",
        "\n",
        "    I = np.zeros(ncols)\n",
        "    J = np.zeros(ncols)\n",
        "\n",
        "    k = 0\n",
        "    for i in np.arange(1, m):\n",
        "        I[k:(k + m - i)] = np.arange(i, m)\n",
        "        k = k + (m - i)\n",
        "\n",
        "    k = 0\n",
        "    for i in np.arange(1, m):\n",
        "        J[k: (k + m - i)] = i - 1\n",
        "        k = k + m - i\n",
        "\n",
        "    Row = np.tile(np.arange(0, ncols), 2)\n",
        "    Col = np.append(I, J)\n",
        "    Data = np.ones(Col.size)\n",
        "    St = sparse.coo_matrix((Data, (Row, Col)), shape=(ncols, m))\n",
        "    return St.T"
      ],
      "metadata": {
        "id": "_n7ldolBQfrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ADMM():\n",
        "    def __init__(self, l2_penalty, log_penalty, step_size=1e-02, relaxation_factor = 1.8):\n",
        "        self.alpha = log_penalty  # the penalty before log barrier\n",
        "        self.beta = l2_penalty  # the penalty before l2 term\n",
        "        self.gn = step_size\n",
        "        self.relax = relaxation_factor\n",
        "\n",
        "    def initialisation(self, l, m, batch_size=1):\n",
        "        w = torch.zeros((batch_size, l)).float().to(device)\n",
        "        v = torch.zeros((batch_size, m)).float().to(device)\n",
        "        return w, v\n",
        "\n",
        "    def prox_log_barrier(self, y, gn, alpha):\n",
        "        up = y ** 2 + 4 * gn * alpha\n",
        "        up = torch.clamp(up, 1e-08)\n",
        "        return (y - torch.sqrt(up)) / 2\n",
        "\n",
        "    def objective(self, w, D, z):\n",
        "        f1 = self.beta * torch.norm(w, 2) ** 2\n",
        "        f2 = w.T @ z\n",
        "        f3 = - self.alpha * torch.sum(torch.log(D @ w))\n",
        "\n",
        "        if all(np.round(w, 4) >= 0):\n",
        "            return f1 + f2 + f3\n",
        "        else:\n",
        "            return 10**3\n",
        "\n",
        "    def solve(self, smooth, max_iter=1000, verbose=True):\n",
        "        batch_size, m, _, r = smooth.shape\n",
        "        mask = torch.triu(torch.ones(m, m), diagonal=1).bool()\n",
        "        z = smooth[:, mask].view(batch_size, -1)\n",
        "        l = int(m*(m-1)*r/2)\n",
        "\n",
        "        D_ori = coo_to_sparseTensor(get_degree_operator(m)).to(device)\n",
        "        eye = torch.eye(int(m*(m-1)/2))\n",
        "        shift_sum = eye.repeat_interleave(r, dim=1)\n",
        "        D = D_ori @ shift_sum.to(device)\n",
        "\n",
        "        # D * shift_sum if the new operator\n",
        "\n",
        "        # initialise:\n",
        "        w, v = self.initialisation(l, m, batch_size)\n",
        "        zero_vec = torch.zeros((batch_size, l)).to(device)\n",
        "        # w_list = torch.empty(size=(batch_size, max_ite, l))\n",
        "        # print(w_list.shape)\n",
        "\n",
        "        lambda_ = self.relax\n",
        "\n",
        "        for i in range(max_iter):\n",
        "\n",
        "            y1 = w - self.gn * (2 * self.beta * w + torch.matmul(v, D))\n",
        "            p1 = torch.max(zero_vec, y1 - 2 * self.gn * z)\n",
        "\n",
        "            y2 = v + self.gn * torch.matmul(2 * p1 - w, D.T)\n",
        "            p2 = self.prox_log_barrier(y2, self.gn, self.alpha)\n",
        "\n",
        "            w = w + lambda_ * (p1 - w)\n",
        "            v = v + lambda_ * (p2 - v)\n",
        "\n",
        "            # w_list[:, i, :] = w\n",
        "\n",
        "        return w\n",
        "\n",
        "#%%\n",
        "\n",
        "class PDS():\n",
        "    def __init__(self, l2_psi, log_psi, step_size):\n",
        "        self.alpha = log_psi  # the penalty before log barrier\n",
        "        self.beta = l2_psi  # the penalty before l2 term\n",
        "        self.gn = step_size\n",
        "\n",
        "    def prox_log_barrier(self, y, gn, alpha):\n",
        "        return (y - torch.sqrt(y ** 2 + 4 * gn * alpha)) / 2\n",
        "\n",
        "    def initialisation(self, l, m, batch_size):\n",
        "        w = torch.zeros((batch_size, l)).float().to(device)\n",
        "        v = torch.zeros((batch_size, m)).float().to(device)\n",
        "        return w, v\n",
        "\n",
        "    def solve(self, smooth, max_iter = 500):\n",
        "        # z \\in 1* m* m * r\n",
        "        batch_size, m, _, r = smooth.shape\n",
        "        mask = torch.triu(torch.ones(m, m), diagonal=1).bool()\n",
        "        z = smooth[:, mask].view(batch_size, -1)\n",
        "        l = int(m*(m-1)*r/2)\n",
        "\n",
        "        D_ori = coo_to_sparseTensor(get_degree_operator(m)).to(device)\n",
        "        eye = torch.eye(int(m*(m-1)/2))\n",
        "        shift_sum = eye.repeat_interleave(r, dim=1)\n",
        "        D = D_ori @ shift_sum.to(device)\n",
        "\n",
        "        # initialise:\n",
        "        w, v = self.initialisation(l, m, batch_size)\n",
        "        zero_vec = torch.zeros((batch_size, l)).to(device)\n",
        "        # w_list = torch.empty(size=(batch_size, max_iter, l)).to(device)\n",
        "        for i in range(max_iter):\n",
        "            # print(z.shape)\n",
        "            y1 = w - self.gn * (2 * self.beta * w + 2 * z + torch.matmul(v, D))\n",
        "            y2 = v + self.gn * torch.matmul(w, D.T)\n",
        "\n",
        "            p1 = torch.max(zero_vec, y1)\n",
        "            p2 = self.prox_log_barrier(y2, self.gn, self.alpha)\n",
        "\n",
        "            q1 = p1 - self.gn * (2 * self.beta * p1 + 2 * z + torch.matmul(p2, D))\n",
        "            q2 = p2 + self.gn * torch.matmul(p1, D.T)\n",
        "\n",
        "            w = w - y1 + q1\n",
        "            v = v - y2 + q2\n",
        "\n",
        "            # w_list[:, i, :] = w\n",
        "\n",
        "        return w\n"
      ],
      "metadata": {
        "id": "wjO1shkKN5iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "def estimate_w(signal_vtx, signals_edge, adj_tensor, optimizer):\n",
        "  diff_tensor = (signal_vtx[:, None, None, :] - signal_vtx[None,: , None, :]) * signals_edge[ None, None, :, :]\n",
        "  diff_tensor = torch.Tensor(diff_tensor)\n",
        "  num_nodes,_,num_relations ,K = diff_tensor.shape\n",
        "  smooth_tensor = torch.norm(diff_tensor, dim=-1)**2\n",
        "  # est_tensor = torch.exp(- 1/sigma * smooth_tensor)\n",
        "  adj_tensor = torch.Tensor(adj_tensor).unsqueeze(0).to(device)\n",
        "  mask = torch.triu(torch.ones(num_nodes, num_nodes), diagonal=1).bool()\n",
        "  adj_vec = adj_tensor[:, mask].view(1, -1)\n",
        "  print(smooth_tensor.shape)\n",
        "  w = optimizer.solve(smooth_tensor.unsqueeze(0).to(device), max_iter = 500)\n",
        "  # Normalization\n",
        "  w = w/torch.max(w)\n",
        "  GMSE_error = torch.sum(torch.square(w-adj_vec)/(w+1e-12))\n",
        "\n",
        "  num_samples = w.shape[0] * w.shape[1]*w.shape[0]\n",
        "  edge_indices = torch.where(adj_vec == 1)\n",
        "  # Use these indices to access the corresponding elements in B\n",
        "  out_edges = w[edge_indices]\n",
        "  print(adj_vec[edge_indices])\n",
        "  print(w)\n",
        "  print(out_edges)\n",
        "  link_error = torch.mean(torch.square(adj_vec[edge_indices] - out_edges))\n",
        "  print(GMSE_error/num_samples)\n",
        "  print(torch.sqrt(link_error))\n",
        "  auc = roc_auc_score(adj_vec.to('cpu').detach().numpy()[0], w.to('cpu').detach().numpy()[0])\n",
        "  f1 = f1_score(adj_vec.to('cpu')[0], (w>0.8).to('cpu')[0])\n",
        "  print(auc, f1)\n",
        "  # auc = roc_auc_score(adj_vec[edge_indices].to('cpu').detach().numpy(), out_edge\n",
        "\n",
        "\n",
        "estimate_w(signal_vtx, signals_edge, adj_tsr, pds_opt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "iZJ7R5XIH6NN",
        "outputId": "991e63d5-2f16-4ce6-acf7-bb1401bc36ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-20ca17942a00>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mestimate_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal_vtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignals_edge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_tsr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpds_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'signal_vtx' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "\n",
        "step = 0\n",
        "GMSE_error = 0\n",
        "total_samples = 0\n",
        "link_errors = []\n",
        "sigma = 1e-3\n",
        "\n",
        "for batch in tqdm.tqdm(test_loader):\n",
        "  step+=1\n",
        "  batch.to(device)\n",
        "  model.to(device)\n",
        "  pds_opt = PDS(1, 1, 1e-2)\n",
        "  \n",
        "  node_out, edge_out = model.hetero_linear(batch)\n",
        "  n_emb, adj_tsr = batch_to_adjtensor(batch, node_out, edge_out)\n",
        "  # r_emb = [edge_out['_'.join(e_type)] for e_type in batch.edge_types if batch[e_type].edge_index.shape[0] != 0]\n",
        "  r_emb = [edge_out['_'.join(e_type)] for e_type in batch.edge_types if batch[e_type].edge_index.shape[-1] != 0]\n",
        "  \n",
        "  r_emb = torch.cat(r_emb, dim=0)\n",
        "  diff_tensor = (n_emb[:, None, None, :] - n_emb[None,: , None, :]) * r_emb[ None, None, :, :]\n",
        "  num_nodes,_,num_relations ,K = diff_tensor.shape\n",
        "  smooth_tensor = torch.norm(diff_tensor, dim=-1)**2\n",
        "  # est_tensor = torch.exp(- 1/sigma * smooth_tensor)\n",
        "  adj_tsr = adj_tsr.unsqueeze(0).to(device)\n",
        "  \n",
        "  mask = torch.triu(torch.ones(num_nodes, num_nodes), diagonal=1).bool()\n",
        "  adj_vec = adj_tsr[:, mask].view(1, -1)\n",
        "  print(smooth_tensor.shape)\n",
        "  w = pds_opt.solve(smooth_tensor.unsqueeze(0), max_iter = 500)\n",
        "  estimate_w(n_emb, r_emb, adj_tsr.squeeze(0), pds_opt)\n",
        "  GMSE_error += torch.sum(torch.square(w-adj_vec))\n",
        "\n",
        "  num_samples = w.shape[0] * w.shape[1]*w.shape[0]\n",
        "  total_samples += num_samples\n",
        "\n",
        "  edge_indices = torch.where(adj_vec == 1)\n",
        "  \n",
        "  # Use these indices to access the corresponding elements in B\n",
        "  out_edges = w[edge_indices]\n",
        "  print(adj_vec[edge_indices])\n",
        "  print(w)\n",
        "  link_error = torch.mean(torch.square(adj_vec[edge_indices] - out_edges))\n",
        "  link_errors.append(torch.sqrt(link_error))\n",
        "  print(GMSE_error/total_samples)\n",
        "  print(torch.mean(torch.tensor(link_errors)))\n",
        "  # auc = roc_auc_score(adj_vec[edge_indices].to('cpu').detach().numpy(), out_edges.to('cpu').detach().numpy())\n",
        "  # print(auc)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "73Eowy8-Oz7r",
        "outputId": "93191e7a-e5f8-46df-da28-55798d836df9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3582 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([71, 71, 5])\n",
            "torch.Size([71, 71, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 1/3582 [00:01<1:44:25,  1.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.8955, 0.8936, 0.8577,  ..., 0.9024, 0.9070, 0.8666]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8718, 0.8692, 0.8106, 0.9026, 0.8503, 0.8872, 0.8704, 0.8463, 0.9093,\n",
            "        0.8792], device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.7400, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1332, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8211115585984696 0.0035868005738880914\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.0587, 0.0586, 0.0562,  ..., 0.0591, 0.0594, 0.0568]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0032, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9430)\n",
            "torch.Size([68, 68, 6])\n",
            "torch.Size([68, 68, 6])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 2/3582 [00:02<1:08:01,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.9288, 0.9237, 0.6749,  ..., 0.9217, 0.9272, 0.9238]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.9171, 0.9174, 0.8865, 0.8997, 0.8952, 0.9023, 0.9095, 0.9094, 0.9027,\n",
            "        0.9120, 0.9074], device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.7720, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.0950, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8469050170741612 0.0029617662897145933\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.0548, 0.0545, 0.0398,  ..., 0.0544, 0.0547, 0.0545]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0030, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9448)\n",
            "torch.Size([65, 65, 5])\n",
            "torch.Size([65, 65, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 3/3582 [00:03<54:06,  1.10it/s]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.9229, 0.9226, 0.6851,  ..., 0.9294, 0.9307, 0.8882]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8925, 0.8982, 0.8612, 0.8273, 0.8993, 0.9073, 0.8944, 0.8951, 0.9057,\n",
            "        0.9231, 0.8961, 0.8999], device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.7704, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1109, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8353669297907842 0.004595060310166571\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.0614, 0.0614, 0.0456,  ..., 0.0618, 0.0619, 0.0591]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0032, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9434)\n",
            "torch.Size([63, 63, 6])\n",
            "torch.Size([63, 63, 6])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 4/3582 [00:03<48:07,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.9685, 0.9689, 0.4605,  ..., 0.9250, 0.9247, 0.9166]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.9505, 0.8891, 0.9100, 0.8536, 0.9293, 0.8948, 0.8987, 0.9036, 0.8954,\n",
            "        0.8951, 0.8940, 0.9042, 0.8985, 0.8954, 0.8959, 0.8927, 0.8909],\n",
            "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.7729, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1023, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8311255448252287 0.005299251870324188\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.0596, 0.0596, 0.0283,  ..., 0.0569, 0.0569, 0.0564]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0033, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9437)\n",
            "torch.Size([75, 75, 6])\n",
            "torch.Size([75, 75, 6])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 5/3582 [00:04<47:52,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.8392, 0.8879, 0.4653,  ..., 0.9015, 0.9012, 0.8955]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8777, 0.8781, 0.8880, 0.8790, 0.8745, 0.8903, 0.8781, 0.8735, 0.8507,\n",
            "        0.8070, 0.8671, 0.8733, 0.8835, 0.8751], device='cuda:0',\n",
            "       grad_fn=<IndexBackward0>)\n",
            "tensor(0.7333, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1304, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8594077388108405 0.0036425133342005986\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.0490, 0.0519, 0.0272,  ..., 0.0527, 0.0527, 0.0523]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0032, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9448)\n",
            "torch.Size([70, 70, 5])\n",
            "torch.Size([70, 70, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 6/3582 [00:05<49:02,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.8757, 0.8737, 0.8358,  ..., 0.8886, 0.8888, 0.8625]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8559, 0.8411, 0.8547, 0.8413, 0.8579, 0.8205, 0.8325, 0.8754, 0.8524,\n",
            "        0.8329, 0.8660, 0.8250, 0.8464, 0.8483, 0.8418, 0.8505, 0.8531, 0.8491,\n",
            "        0.8598, 0.8517, 0.8529], device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.7242, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1525, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8390654752028569 0.008320126782884312\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.0593, 0.0592, 0.0566,  ..., 0.0602, 0.0602, 0.0584]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0033, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9444)\n",
            "torch.Size([69, 69, 6])\n",
            "torch.Size([69, 69, 6])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 7/3582 [00:06<52:10,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.9424, 0.9470, 0.8713,  ..., 0.9467, 0.9539, 0.8943]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.9307, 0.9206, 0.9085, 0.9126, 0.9101, 0.9164, 0.9147, 0.9144, 0.9204,\n",
            "        0.9144, 0.9103, 0.9461, 0.9455, 0.9198, 0.9220, 0.9214, 0.9166, 0.9202,\n",
            "        0.9152, 0.9120, 0.8874, 0.9503, 0.9148], device='cuda:0',\n",
            "       grad_fn=<IndexBackward0>)\n",
            "tensor(0.7944, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.0818, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8502470461204323 0.0056379458266944465\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.0541, 0.0544, 0.0500,  ..., 0.0544, 0.0548, 0.0514]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0033, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9448)\n",
            "torch.Size([61, 61, 5])\n",
            "torch.Size([61, 61, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 8/3582 [00:07<50:43,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.9782, 0.9220, 0.8974,  ..., 0.9295, 0.9335, 0.8917]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.9234, 0.9015, 0.8977, 0.8982, 0.9041, 0.9020, 0.8993, 0.8892, 0.8999,\n",
            "        0.8977, 0.9052, 0.9010, 0.9032, 0.8996, 0.9031, 0.8910, 0.8968, 0.9021,\n",
            "        0.8959, 0.9001, 0.9019], device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.8145, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.0996, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.800145011449645 0.006676204101096804\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.0654, 0.0616, 0.0600,  ..., 0.0621, 0.0624, 0.0596]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9442)\n",
            "torch.Size([76, 76, 5])\n",
            "torch.Size([76, 76, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 9/3582 [00:08<56:02,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.8497, 0.8321, 0.7636,  ..., 0.8793, 0.8822, 0.8600]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8464, 0.8468, 0.8060, 0.8199, 0.8328, 0.8280, 0.8572, 0.8643, 0.8447,\n",
            "        0.8633, 0.8731, 0.8491, 0.8446], device='cuda:0',\n",
            "       grad_fn=<IndexBackward0>)\n",
            "tensor(0.7102, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1567, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8262382416347437 0.004779411764705882\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.0557, 0.0545, 0.0500,  ..., 0.0576, 0.0578, 0.0564]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9443)\n",
            "torch.Size([63, 63, 5])\n",
            "torch.Size([63, 63, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 10/3582 [00:09<51:27,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.8992, 0.9175, 0.3415,  ..., 0.9088, 0.9102, 0.8915]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8875, 0.8885, 0.8986, 0.8849, 0.8867, 0.9024, 0.9200, 0.8876, 0.9175,\n",
            "        0.8674, 0.8872, 0.9047, 0.9029, 0.9106], device='cuda:0',\n",
            "       grad_fn=<IndexBackward0>)\n",
            "tensor(0.7548, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1048, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8877331262727632 0.006012454369765944\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.0617, 0.0630, 0.0234,  ..., 0.0624, 0.0625, 0.0612]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9437)\n",
            "torch.Size([71, 71, 5])\n",
            "torch.Size([71, 71, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 11/3582 [00:09<48:04,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.8480, 0.8892, 0.1342,  ..., 0.8894, 0.8952, 0.8675]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8595, 0.8616, 0.8160, 0.8540, 0.8491, 0.8562, 0.8057, 0.8570, 0.8885,\n",
            "        0.8540, 0.8514, 0.8575, 0.8594, 0.8613, 0.8571, 0.8710],\n",
            "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.7235, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1475, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8363889112740752 0.005923731951129212\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.0569, 0.0596, 0.0090,  ..., 0.0596, 0.0600, 0.0582]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9436)\n",
            "torch.Size([68, 68, 6])\n",
            "torch.Size([68, 68, 6])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 12/3582 [00:10<45:30,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.9378, 0.9407, 0.7727,  ..., 0.9137, 0.9217, 0.8749]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8862, 0.9055, 0.8907, 0.9021, 0.8806, 0.8593, 0.8876, 0.8915, 0.8826,\n",
            "        0.8790, 0.8931, 0.8947, 0.8828, 0.9100, 0.9141], device='cuda:0',\n",
            "       grad_fn=<IndexBackward0>)\n",
            "tensor(0.7542, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1102, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.838621548377646 0.004257734885041158\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.0564, 0.0565, 0.0464,  ..., 0.0549, 0.0554, 0.0526]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9438)\n",
            "torch.Size([58, 58, 5])\n",
            "torch.Size([58, 58, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 13/3582 [00:10<41:28,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.9018, 0.9256, 0.3236,  ..., 0.9181, 0.9189, 0.8943]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8923, 0.8859, 0.8891, 0.8868, 0.8854, 0.8789, 0.8857, 0.8781, 0.8908,\n",
            "        0.8865, 0.8874, 0.8921, 0.8867, 0.8760], device='cuda:0',\n",
            "       grad_fn=<IndexBackward0>)\n",
            "tensor(0.7701, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1143, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8477240853922468 0.0064709960711809575\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.0640, 0.0657, 0.0230,  ..., 0.0652, 0.0652, 0.0635]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9433)\n",
            "torch.Size([65, 65, 5])\n",
            "torch.Size([65, 65, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 14/3582 [00:11<39:41,  1.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.8697, 0.8717, 0.8150,  ..., 0.8902, 0.8904, 0.8650]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8575, 0.8200, 0.8378, 0.8503, 0.8758, 0.8545, 0.8542, 0.8470, 0.9092,\n",
            "        0.8545, 0.8497, 0.8543, 0.8556, 0.8514, 0.8870, 0.8542, 0.8476],\n",
            "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.7310, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1447, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8310133646061717 0.007192722657076369\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.0608, 0.0610, 0.0570,  ..., 0.0623, 0.0623, 0.0605]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9431)\n",
            "torch.Size([62, 62, 6])\n",
            "torch.Size([62, 62, 6])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 15/3582 [00:12<38:26,  1.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.9246, 0.9518, 0.5972,  ..., 0.9319, 0.9321, 0.9068]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.9306, 0.9352, 0.8474, 0.9017, 0.9096, 0.9076, 0.8861, 0.9030, 0.8980,\n",
            "        0.9001, 0.8994, 0.8918, 0.8957, 0.8944], device='cuda:0',\n",
            "       grad_fn=<IndexBackward0>)\n",
            "tensor(0.7787, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1019, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8352642327668802 0.004477134633834346\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.0571, 0.0587, 0.0369,  ..., 0.0575, 0.0575, 0.0560]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9432)\n",
            "torch.Size([70, 70, 6])\n",
            "torch.Size([70, 70, 6])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 16/3582 [00:12<39:07,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.8934, 0.9243, 0.1007,  ..., 0.8914, 0.8973, 0.8917]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8996, 0.8559, 0.8812, 0.8839, 0.8701, 0.8769, 0.8810, 0.8865, 0.8674,\n",
            "        0.8667, 0.8797], device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.7304, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1233, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8994342904143304 0.0033082706766917294\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.0544, 0.0563, 0.0061,  ..., 0.0543, 0.0546, 0.0543]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9434)\n",
            "torch.Size([67, 67, 5])\n",
            "torch.Size([67, 67, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 17/3582 [00:13<38:31,  1.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1.], device='cuda:0')\n",
            "tensor([[0.8969, 0.9001, 0.7181,  ..., 0.8915, 0.8960, 0.7873]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8660, 0.8931, 0.8820, 0.8315, 0.8653, 0.8534, 0.8719, 0.8672, 0.8763,\n",
            "        0.8831, 0.9012, 0.8711, 0.8783, 0.8790, 0.9016, 0.8976, 0.8710, 0.8736,\n",
            "        0.8807, 0.8779], device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.7505, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1249, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8554191209787041 0.007366482504604051\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1.], device='cuda:0')\n",
            "tensor([[0.0603, 0.0606, 0.0483,  ..., 0.0600, 0.0603, 0.0530]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9433)\n",
            "torch.Size([70, 70, 6])\n",
            "torch.Size([70, 70, 6])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 18/3582 [00:14<39:21,  1.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.8942, 0.8901, 0.6602,  ..., 0.9068, 0.9076, 0.8767]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8826, 0.8684, 0.8791, 0.8852, 0.8769, 0.8745, 0.8669, 0.8865, 0.8826,\n",
            "        0.8861, 0.8841, 0.8797, 0.8996], device='cuda:0',\n",
            "       grad_fn=<IndexBackward0>)\n",
            "tensor(0.7379, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1193, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8532951472096323 0.0038382049010924125\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.0537, 0.0535, 0.0397,  ..., 0.0545, 0.0545, 0.0527]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9435)\n",
            "torch.Size([72, 72, 6])\n",
            "torch.Size([72, 72, 6])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 19/3582 [00:14<40:27,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.9445, 0.9474, 0.2945,  ..., 0.8997, 0.9016, 0.8751]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8620, 0.8944, 0.9121, 0.8872, 0.8740, 0.8830, 0.8319, 0.8733, 0.8797,\n",
            "        0.8889, 0.8704, 0.8835, 0.8856, 0.8684, 0.8859, 0.8808],\n",
            "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.7298, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1223, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8466710182767624 0.004596380350474001\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.0562, 0.0564, 0.0175,  ..., 0.0535, 0.0537, 0.0521]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9437)\n",
            "torch.Size([69, 69, 6])\n",
            "torch.Size([69, 69, 6])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 20/3582 [00:15<40:28,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1.], device='cuda:0')\n",
            "tensor([[0.9040, 0.9053, 0.8766,  ..., 0.9036, 0.9080, 0.8618]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8354, 0.8792, 0.8802, 0.8697, 0.8724, 0.8817, 0.8822, 0.8714, 0.8632,\n",
            "        0.8841, 0.8803, 0.8795, 0.8767, 0.8702, 0.8699, 0.8722, 0.8850, 0.8697,\n",
            "        0.8689, 0.8741], device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.7484, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1271, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8348000853727945 0.005685856432125089\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1.], device='cuda:0')\n",
            "tensor([[0.0547, 0.0547, 0.0530,  ..., 0.0546, 0.0549, 0.0521]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9439)\n",
            "torch.Size([72, 72, 6])\n",
            "torch.Size([72, 72, 6])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 21/3582 [00:16<41:36,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.9849, 0.9878, 0.3613,  ..., 0.9438, 0.9437, 0.8908]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8383, 0.9243, 0.9270, 0.9145, 0.8838, 0.9030, 0.9186, 0.8818, 0.8827,\n",
            "        0.9039, 0.8970, 0.9106, 0.8909, 0.9108], device='cuda:0',\n",
            "       grad_fn=<IndexBackward0>)\n",
            "tensor(0.7715, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1033, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8204822197773509 0.0035769034236075624\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.0564, 0.0566, 0.0207,  ..., 0.0540, 0.0540, 0.0510]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9441)\n",
            "torch.Size([69, 69, 5])\n",
            "torch.Size([69, 69, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 22/3582 [00:16<40:32,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.9477, 0.9605, 0.9201,  ..., 0.9304, 0.9404, 0.9022]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8845, 0.9062, 0.9121, 0.9079, 0.9376, 0.9102, 0.9119, 0.9191, 0.9127,\n",
            "        0.9087, 0.9080, 0.9125, 0.9046, 0.9124], device='cuda:0',\n",
            "       grad_fn=<IndexBackward0>)\n",
            "tensor(0.7860, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.0900, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8112410378968931 0.0043811610076670325\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.0600, 0.0609, 0.0583,  ..., 0.0589, 0.0596, 0.0572]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9440)\n",
            "torch.Size([75, 75, 6])\n",
            "torch.Size([75, 75, 6])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 23/3582 [00:17<42:34,  1.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.9141, 0.9185, 0.8528,  ..., 0.9199, 0.9222, 0.8981]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8742, 0.8904, 0.9047, 0.9378, 0.9400, 0.9148, 0.9025, 0.9005, 0.8758,\n",
            "        0.8813, 0.8988, 0.8737, 0.8808, 0.8824, 0.8956, 0.8785, 0.8739, 0.8745,\n",
            "        0.8860, 0.8843, 0.8883, 0.8954, 0.8445], device='cuda:0',\n",
            "       grad_fn=<IndexBackward0>)\n",
            "tensor(0.7465, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1115, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.867358225620455 0.005577785861525402\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.0527, 0.0529, 0.0491,  ..., 0.0530, 0.0531, 0.0517]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9442)\n",
            "torch.Size([68, 68, 5])\n",
            "torch.Size([68, 68, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 24/3582 [00:18<41:12,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.9000, 0.9003, 0.8609,  ..., 0.9184, 0.9197, 0.8979]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8820, 0.8747, 0.8838, 0.8350, 0.8371, 0.8431, 0.8883, 0.8813, 0.8749,\n",
            "        0.9015, 0.8477, 0.8844], device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.7564, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1323, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.7811822522997597 0.004289544235924933\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.0597, 0.0597, 0.0571,  ..., 0.0609, 0.0610, 0.0595]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9441)\n",
            "torch.Size([59, 59, 6])\n",
            "torch.Size([59, 59, 6])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 25/3582 [00:19<41:17,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.8951, 0.9550, 0.6215,  ..., 0.9495, 0.9492, 0.9376]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.9231, 0.9338, 0.9406, 0.9186, 0.9288, 0.9329, 0.9193, 0.8857],\n",
            "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.7973, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.0787, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8814461883408071 0.0027127839945744322\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.0555, 0.0592, 0.0385,  ..., 0.0589, 0.0589, 0.0582]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9441)\n",
            "torch.Size([69, 69, 5])\n",
            "torch.Size([69, 69, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 26/3582 [00:19<44:48,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.9063, 0.9106, 0.6471,  ..., 0.9149, 0.9205, 0.9200]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8679, 0.8310, 0.8705, 0.8695, 0.8756, 0.8747, 0.8658, 0.8743, 0.8942,\n",
            "        0.8948, 0.8923, 0.9167, 0.8986, 0.8917, 0.9287, 0.8782],\n",
            "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.7567, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1193, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8488880826361619 0.005804462180301106\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.0595, 0.0597, 0.0425,  ..., 0.0600, 0.0604, 0.0604]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9440)\n",
            "torch.Size([74, 74, 6])\n",
            "torch.Size([74, 74, 6])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 27/3582 [00:20<50:00,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.9199, 0.9226, 0.8355,  ..., 0.9198, 0.9200, 0.8748]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8745, 0.8983, 0.8822, 0.9099, 0.9035, 0.8939, 0.8832, 0.8832, 0.9055,\n",
            "        0.8989, 0.8874, 0.8967, 0.9281, 0.9132, 0.8810, 0.8954, 0.8631],\n",
            "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.7551, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1071, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8310781104090287 0.004276729559748427\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.0527, 0.0529, 0.0479,  ..., 0.0527, 0.0527, 0.0502]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9442)\n",
            "torch.Size([65, 65, 5])\n",
            "torch.Size([65, 65, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 28/3582 [00:21<49:47,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.9075, 0.9092, 0.8785,  ..., 0.9317, 0.9325, 0.9054]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8933, 0.8867, 0.8825, 0.9081, 0.9008, 0.8927, 0.8979, 0.8821, 0.9040,\n",
            "        0.8990, 0.9066], device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.7801, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1046, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8349128011270662 0.004117536964252292\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.0599, 0.0600, 0.0580,  ..., 0.0615, 0.0616, 0.0598]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9441)\n",
            "torch.Size([69, 69, 5])\n",
            "torch.Size([69, 69, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 29/3582 [00:22<49:59,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.9757, 0.9660, 0.4660,  ..., 0.9521, 0.9622, 0.9069]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.9410, 0.9322, 0.9356, 0.9312, 0.9325, 0.9229, 0.9341, 0.9167, 0.9234,\n",
            "        0.9047, 0.9360, 0.9339, 0.9329, 0.9551, 0.9299, 0.9294, 0.9306, 0.9506],\n",
            "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.8020, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.0691, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8565336596842744 0.005568445475638051\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.0606, 0.0600, 0.0289,  ..., 0.0591, 0.0597, 0.0563]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9440)\n",
            "torch.Size([67, 67, 6])\n",
            "torch.Size([67, 67, 6])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 30/3582 [00:23<46:26,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.9034, 0.8955, 0.7684,  ..., 0.9027, 0.9040, 0.8724]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8633, 0.8890, 0.9029, 0.8777, 0.9361, 0.8948, 0.8816, 0.9257, 0.8911],\n",
            "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.7625, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1064, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8645663087844577 0.0024526502248262707\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.0546, 0.0541, 0.0464,  ..., 0.0545, 0.0546, 0.0527]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9441)\n",
            "torch.Size([67, 67, 5])\n",
            "torch.Size([67, 67, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 31/3582 [00:23<43:26,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.9238, 0.9192, 0.8294,  ..., 0.9488, 0.9532, 0.9101]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.9036, 0.8268, 0.9113, 0.8982, 0.9037, 0.9204, 0.8999, 0.9075, 0.9237,\n",
            "        0.9006, 0.9284, 0.8983, 0.9484, 0.9124, 0.9111, 0.9076, 0.8622, 0.9055],\n",
            "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.7765, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.0993, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8374885486192906 0.006396588486140726\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.0599, 0.0596, 0.0538,  ..., 0.0616, 0.0618, 0.0590]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9440)\n",
            "torch.Size([72, 72, 6])\n",
            "torch.Size([72, 72, 6])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 32/3582 [00:24<43:17,  1.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.9660, 0.9560, 0.8037,  ..., 0.9250, 0.9253, 0.9053]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.8608, 0.9312, 0.8895, 0.8952, 0.8686, 0.8944, 0.8800, 0.8962, 0.8830,\n",
            "        0.8945, 0.8848, 0.8889, 0.8836, 0.8832, 0.8847, 0.8918, 0.8778, 0.9011,\n",
            "        0.8974, 0.8760, 0.9476, 0.8884], device='cuda:0',\n",
            "       grad_fn=<IndexBackward0>)\n",
            "tensor(0.7587, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.1106, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8131374143683141 0.005517933283170304\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1.], device='cuda:0')\n",
            "tensor([[0.0562, 0.0556, 0.0468,  ..., 0.0538, 0.0538, 0.0527]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9441)\n",
            "torch.Size([67, 67, 5])\n",
            "torch.Size([67, 67, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 33/3582 [00:25<41:09,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.9064, 0.9519, 0.4195,  ..., 0.9577, 0.9625, 0.9210]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.9147, 0.9168, 0.9230, 0.9173, 0.9304, 0.9167, 0.9230, 0.9278, 0.9224,\n",
            "        0.9323, 0.9212, 0.9221, 0.9117, 0.9389, 0.9194, 0.9250, 0.9273],\n",
            "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
            "tensor(0.7872, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.0773, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.875014655255108 0.0058219178082191785\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.0579, 0.0608, 0.0268,  ..., 0.0612, 0.0615, 0.0589]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9440)\n",
            "torch.Size([72, 72, 5])\n",
            "torch.Size([72, 72, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 34/3582 [00:25<40:57,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.9218, 0.9390, 0.6540,  ..., 0.9314, 0.9389, 0.9234]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor([0.9400, 0.8947, 0.9178, 0.9030, 0.9100, 0.8940, 0.9118, 0.8996, 0.8961,\n",
            "        0.9135, 0.8967, 0.9067, 0.9010, 0.9439, 0.9050], device='cuda:0',\n",
            "       grad_fn=<IndexBackward0>)\n",
            "tensor(0.7671, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.0923, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "0.8796500848674762 0.004731114966093676\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "tensor([[0.0581, 0.0592, 0.0412,  ..., 0.0587, 0.0592, 0.0582]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9440)\n",
            "torch.Size([78, 78, 6])\n",
            "torch.Size([78, 78, 6])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 34/3582 [00:26<46:32,  1.27it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-0667ad4e3e86>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmooth_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpds_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmooth_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0mestimate_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_tsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpds_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m   \u001b[0mGMSE_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0madj_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-20ca17942a00>\u001b[0m in \u001b[0;36mestimate_w\u001b[0;34m(signal_vtx, signals_edge, adj_tensor, optimizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0madj_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmooth_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmooth_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0;31m# Normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-7204c37dd995>\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, smooth, max_iter)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mq1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mq2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m# w_list[:, i, :] = w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8C3OSs4jQQFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "a = torch.arange(36).view(3,3,4)"
      ],
      "metadata": {
        "id": "eAyG4X36D1Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_matrix(a, k):\n",
        "    # Create an identity matrix with shape a * b\n",
        "    eye = np.eye(a)\n",
        "\n",
        "    # Repeat each element k times along the column axis\n",
        "    repeated_eye = np.repeat(eye, k, axis=1)\n",
        "\n",
        "    return repeated_eye\n",
        "\n",
        "generate_matrix(2, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AOCOAEb_8bV",
        "outputId": "f28a9917-c1d1-44fc-852f-05f31a4c26ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnjCbMepfG3I"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import HGTLoader\n",
        "\n",
        "from torch_geometric.utils import to_scipy_sparse_matrix\n",
        "import torch_geometric.transforms as T\n",
        "import numpy as np\n",
        "\n",
        "loader = HGTLoader(\n",
        "  data,\n",
        "  # Sample 512 nodes per type and per iteration for 4 iterations\n",
        "  num_samples={key: [16] * 4 for key in data.node_types},\n",
        "  # Use a batch size of 128 for sampling training nodes of type paper\n",
        "  batch_size=8,\n",
        "  input_nodes=('paper'),\n",
        "  )\n",
        "\n",
        "sampled_hetero_data = next(iter(loader))\n",
        "def connected_graph(sampled_graph):\n",
        "  subsampled_G = sampled_graph.to_homogeneous()\n",
        "  largest_component = T.LargestConnectedComponents(num_components=1, connection = 'strong')(subsampled_G)\n",
        "  adj = to_scipy_sparse_matrix(subsampled_G.edge_index, num_nodes=subsampled_G.num_nodes)\n",
        "  return largest_component, adj\n",
        "\n",
        "connected_subsample, adj = connected_graph(sampled_hetero_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ldJSUBTJCQBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWHBVXiljqdb"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "# from torch_geometric.data import dataset\n",
        "def sampling_hetero_graphs(hetero_graph, nnodes_per_type,graph_depth):\n",
        "  input_node = 'paper'\n",
        "  num_samples = 1000\n",
        "  rd_idx = np.random.choice(len(data[input_node].x),num_samples,replace=False)\n",
        "  \n",
        "  loader = HGTLoader(\n",
        "  hetero_graph,\n",
        "  # Sample 512 nodes per type and per iteration for 4 iterations\n",
        "  num_samples={key: [nnodes_per_type] * graph_depth for key in hetero_graph.node_types},\n",
        "  # Use a batch size of 128 for sampling training nodes of type paper\n",
        "  batch_size=1,\n",
        "  input_nodes=(input_node),\n",
        "  )\n",
        "  sub_graphs = []\n",
        "  for idx in rd_idx:\n",
        "    sampled_graph = loader.collate_fn(index=[idx])\n",
        "    connected_subsample, adj = connected_graph(loader.filter_fn(sampled_graph))\n",
        "    sub_graphs.append(connected_subsample)\n",
        "    # sub_graphs.append(sampled_graph)\n",
        "  graph_loader = DataLoader(sub_graphs, batch_size=4, shuffle=False)\n",
        "  return graph_loader\n",
        "\n",
        "data_loader = sampling_hetero_graphs(data, 16, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdkRrtaVA48P"
      },
      "outputs": [],
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import HeteroConv, Linear, SAGEConv, HeteroDictLinear\n",
        "\n",
        "class LinearProj(torch.nn.Module):\n",
        "    def __init__(self, node_shape, edge_shape, out_channels):\n",
        "        super().__init__()\n",
        "        self.NodeLinear = HeteroDictLinear(in_channels=node_shape,out_channels=out_channels)\n",
        "        self.EdgeLinear = HeteroDictLinear(in_channels=edge_shape,out_channels=out_channels)\n",
        "\n",
        "    def forward(self, batch):\n",
        "      node_attrs = {node_type: batch[node_type].x for node_type in batch.node_types}\n",
        "      num_edges = {edge_type: len(batch[edge_type].e_id) for edge_type in batch.edge_types}\n",
        "      edge_attrs = {'_'.join(edge_type): batch[edge_type].x.repeat(num_edges[edge_type], 1) for edge_type in batch.edge_types}\n",
        "      node_out = self.NodeLinear(node_attrs)\n",
        "      edge_out = self.EdgeLinear(edge_attrs)\n",
        "      \n",
        "      for node_type in batch.node_types:\n",
        "        node_out[node_type] = node_out[node_type]/(node_out[node_type].norm(dim=1)[:, None])\n",
        "      for edge_type in edge_attrs:\n",
        "        edge_out[edge_type] = edge_out[edge_type]/(edge_out[edge_type].norm(dim=1)[:, None])\n",
        "      \n",
        "\n",
        "      return (node_out, edge_out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X86GvFMfdBZf"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from torch_geometric.utils import negative_sampling\n",
        "import numpy as np\n",
        "\n",
        "def neg_sample(batch, s_ntype, s_idx, num_samples):\n",
        "  possible_edge = [(s_type, t_type) for s_type, _ , t_type in batch.edge_types if s_ntype in [s_type, t_type]]\n",
        "  # print(possible_edge)\n",
        "  # possible_edge += [(s_ntype, s_ntype)]\n",
        "  samples = []\n",
        "  for t_type in batch.node_types:\n",
        "    # print(t_type)\n",
        "    if s_ntype == t_type: continue;\n",
        "    if (s_ntype, t_type) not in possible_edge and (t_type, s_ntype) not in possible_edge:\n",
        "      sample = np.random.choice(range(batch[t_type].x.shape[0]), num_samples, replace=True)\n",
        "      samples += [{t_type: val} for val in sample]\n",
        "    # print(samples)\n",
        "  \n",
        "  while len(samples) <= 2*num_samples:\n",
        "    # within type sampling\n",
        "    for t_type in batch.node_types:\n",
        "      if (s_ntype, t_type) in possible_edge:\n",
        "        # print((s_ntype, 'to', t_type))\n",
        "        edge_idx = batch[(s_ntype, 'to', t_type)].edge_index\n",
        "        # print(edge_idx)\n",
        "        # within_samples = negative_sampling(edge_idx, num_neg_samples=num_samples)\n",
        "        # print(edge_idx, s_idx)\n",
        "        indices = torch.where(edge_idx == s_idx)[0]\n",
        "        # Create a list of integers from 0 to 33, excluding 2, 5, and 10\n",
        "        numbers = [i for i in range(batch[t_type].x.shape[0]) if i not in indices.tolist()]\n",
        "        try:\n",
        "          sampled_node = np.random.choice(numbers, num_samples, replace=False)\n",
        "        except:\n",
        "          sampled_node = np.random.choice(numbers, num_samples, replace=True)\n",
        "        samples += [{t_type: idx} for idx in sampled_node]\n",
        "      elif (t_type, s_ntype) in possible_edge:\n",
        "        edge_idx = batch[(t_type, 'to', s_ntype)].edge_index\n",
        "        # within_samples = negative_sampling(edge_idx, num_neg_samples=num_samples)\n",
        "        indices = torch.where(edge_idx == s_idx)[0]\n",
        "        # Create a list of integers from 0 to 33, excluding 2, 5, and 10\n",
        "        numbers = [i for i in range(batch[t_type].x.shape[0]) if i not in indices.tolist()]\n",
        "        try:\n",
        "          sampled_node = np.random.choice(numbers, num_samples, replace=False)\n",
        "        except:\n",
        "          sampled_node = np.random.choice(numbers, num_samples, replace=True)\n",
        "        samples += [{t_type: idx} for idx in sampled_node]\n",
        "  random.shuffle(samples)\n",
        "  samples = samples[0:2*num_samples]\n",
        "  sampels_dict = {}\n",
        "  for t_type in batch.node_types:\n",
        "    element = []\n",
        "    for item in samples:\n",
        "      key = next(iter(item))\n",
        "      if key == t_type:\n",
        "        element += [item[key]]\n",
        "    sampels_dict[t_type] =  torch.LongTensor(element)\n",
        "\n",
        "  return sampels_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvHKpXBDN5D2",
        "outputId": "339709b3-00a5-444c-8ad4-e00a9e7ea808"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HeteroData(\n",
            "  \u001b[1mauthor\u001b[0m={\n",
            "    x=[35, 334],\n",
            "    y=[35],\n",
            "    train_mask=[35],\n",
            "    val_mask=[35],\n",
            "    test_mask=[35],\n",
            "    n_id=[35]\n",
            "  },\n",
            "  \u001b[1mpaper\u001b[0m={\n",
            "    x=[56, 4231],\n",
            "    n_id=[56],\n",
            "    input_id=[8],\n",
            "    batch_size=8\n",
            "  },\n",
            "  \u001b[1mterm\u001b[0m={\n",
            "    x=[64, 50],\n",
            "    n_id=[64]\n",
            "  },\n",
            "  \u001b[1mconference\u001b[0m={\n",
            "    num_nodes=11,\n",
            "    x=[11, 20],\n",
            "    n_id=[11]\n",
            "  },\n",
            "  \u001b[1m(author, to, paper)\u001b[0m={\n",
            "    edge_index=[2, 63],\n",
            "    x=[6],\n",
            "    e_id=[63]\n",
            "  },\n",
            "  \u001b[1m(paper, to, author)\u001b[0m={\n",
            "    edge_index=[2, 63],\n",
            "    x=[6],\n",
            "    e_id=[63]\n",
            "  },\n",
            "  \u001b[1m(paper, to, term)\u001b[0m={\n",
            "    edge_index=[2, 90],\n",
            "    x=[6],\n",
            "    e_id=[90]\n",
            "  },\n",
            "  \u001b[1m(paper, to, conference)\u001b[0m={\n",
            "    edge_index=[2, 3],\n",
            "    x=[6],\n",
            "    e_id=[3]\n",
            "  },\n",
            "  \u001b[1m(term, to, paper)\u001b[0m={\n",
            "    edge_index=[2, 143],\n",
            "    x=[6],\n",
            "    e_id=[143]\n",
            "  },\n",
            "  \u001b[1m(conference, to, paper)\u001b[0m={\n",
            "    edge_index=[2, 52],\n",
            "    x=[6],\n",
            "    e_id=[52]\n",
            "  }\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "for batch in loader:\n",
        "  print(batch)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGIS3gw65ky1"
      },
      "outputs": [],
      "source": [
        "def loss_ns(embs, neg_embs, sigma = 0.01):\n",
        "  # loss with negative sampling\n",
        "  s_emb, r_emb, t_emb = embs\n",
        "  diff_vector = r_emb * (s_emb - t_emb)\n",
        "  smooth = - 1/sigma * torch.diagonal(diff_vector @ diff_vector.t())\n",
        "  diff_neg = r_emb[None,:,:] * (s_emb[None,:,:] - neg_embs)\n",
        "  # N \\times neg_sample \\times K -> N * neg_sample \\times K\n",
        "  inner_product = torch.bmm(diff_neg, diff_neg.permute(0,2,1))\n",
        "  diagonal = torch.diagonal(inner_product, dim1 = 1, dim2= 2)\n",
        "  smooth_neg = torch.sum(torch.exp( - 1/sigma * diagonal), dim=0) + torch.exp(smooth)\n",
        "  # print(torch.exp(smooth)/smooth_neg)\n",
        "  loss = - torch.sum(torch.log(torch.exp(smooth)/smooth_neg))\n",
        "  # loss = - torch.sum(torch.exp(smooth)/smooth_neg)\n",
        "  # loss = torch.sum(smooth - torch.log(smooth_neg))\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cynixjstWlZR"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, loader, device):\n",
        "    total_examples = total_loss = 0\n",
        "    \"\"\"\n",
        "    for batch in loader:\n",
        "      for edge_type in batch.edge_types:\n",
        "          if batch[edge_type].edge_index.shape[-1] ==0:\n",
        "            continue\n",
        "          s_ntype, _ , t_ntype = edge_type\n",
        "          # source node index and target node idx\n",
        "          s_idx, t_idx  = batch[edge_type].edge_index\n",
        "          for s in s_idx:\n",
        "            samples = neg_sample(batch, s_ntype, s, 5)\n",
        "    \"\"\"\n",
        "    step=0\n",
        "    for batch in loader:\n",
        "        step += 1\n",
        "        if step == 100:break;\n",
        "        optimizer.zero_grad()\n",
        "        batch_size = batch['paper'].batch_size\n",
        "        batch = batch.to(device)\n",
        "        node_out, edge_out = model(batch)\n",
        "        loss = 0\n",
        "        num_edges = 0\n",
        "        \n",
        "        for edge_type in batch.edge_types:\n",
        "          if batch[edge_type].edge_index.shape[-1] ==0:\n",
        "            continue\n",
        "          num_edges += batch[edge_type].edge_index.shape[1]\n",
        "          # source node type and target node type\n",
        "          s_ntype, _ , t_ntype = edge_type\n",
        "          # source node index and target node idx\n",
        "          s_idx, t_idx  = batch[edge_type].edge_index\n",
        "          # source node embedding and target node embedding\n",
        "          s_nemb, t_nemb = node_out[s_ntype][s_idx], node_out[t_ntype][t_idx]\n",
        "          # relation embedding\n",
        "          r_emb = edge_out['_'.join(edge_type)]\n",
        "          tensor_list = []\n",
        "          for s in s_idx:\n",
        "            samples = neg_sample(batch, s_ntype, s, 5)\n",
        "\n",
        "            neg_emb_list = []\n",
        "            for node_type, idx_tensor in samples.items():\n",
        "              if idx_tensor is not None:\n",
        "                neg_emb_list += [node_out[node_type][idx_tensor]]\n",
        "            tensor_list += [torch.cat(neg_emb_list, dim=0)]\n",
        "          \n",
        "          neg_emb = torch.stack(tensor_list, dim=0).permute(1,0,2)\n",
        "          \n",
        "          # print(neg_emb.shape)\n",
        "          # print(s_nemb.shape, t_nemb.shape, r_emb.shape)\n",
        "          loss += loss_ns((s_nemb, r_emb, t_nemb), neg_emb, sigma = 1e-3)\n",
        "          \n",
        "          # diff_vector = r_emb * (s_nemb - t_nemb)\n",
        "          # loss += torch.trace(diff_vector.t() @ diff_vector)\n",
        "        # loss = loss / num_edges\n",
        "        # print('Your bullshit training process: step {}, loss {}'.format(step, loss))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_examples += batch_size\n",
        "        total_loss += float(loss) * batch_size\n",
        "\n",
        "    return total_loss / total_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "ikzVN0S0extJ",
        "outputId": "f80da534-17d3-41c8-bf5e-f4ea4bac2dc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 596.065084129873, epoch: 1\n",
            "loss: 372.0150146484375, epoch: 2\n",
            "loss: 307.96123543170967, epoch: 3\n",
            "loss: 275.6006227743746, epoch: 4\n",
            "loss: 253.29685789166075, epoch: 5\n",
            "loss: 244.41133672540838, epoch: 6\n",
            "loss: 239.55379478377526, epoch: 7\n",
            "loss: 233.59777955334596, epoch: 8\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-64f20fafa9a9>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss: {}, epoch: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-99e5c0013e1a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loader, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m           \u001b[0mtensor_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_ntype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mneg_emb_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-301ba4b60674>\u001b[0m in \u001b[0;36mneg_sample\u001b[0;34m(batch, s_ntype, s_idx, num_samples)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ms_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Create a list of integers from 0 to 33, excluding 2, 5, and 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mnumbers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m           \u001b[0msampled_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-301ba4b60674>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ms_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Create a list of integers from 0 to 33, excluding 2, 5, and 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mnumbers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m           \u001b[0msampled_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "loader = HGTLoader(\n",
        "  data,\n",
        "  # Sample 512 nodes per type and per iteration for 4 iterations\n",
        "  num_samples={key: [16] * 4 for key in data.node_types},\n",
        "  # Use a batch size of 128 for sampling training nodes of type paper\n",
        "  batch_size=8,\n",
        "  input_nodes=('paper'),\n",
        "  )\n",
        "device = 'cuda:0'\n",
        "node_shape = {node_type: data[node_type].x.shape[-1] for node_type in data.node_types}\n",
        "edge_shape = {'_'.join(edge_type): data[edge_type].x.shape[-1] for edge_type in data.edge_types}\n",
        "model = LinearProj(node_shape, edge_shape, 100)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=0.001)\n",
        "\n",
        "for epoch in range(1, 21):\n",
        "  model.train()\n",
        "  loss = train(model, optimizer, loader, device)\n",
        "  print('loss: {}, epoch: {}'.format(loss, epoch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22yPa42lNwFx"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "model2 = copy.deepcopy(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-bTUsiFQPgD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "7zJB7rfelb-h",
        "outputId": "53aa1bc0-96d1-4649-bb8a-a3464e9e65b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(9.7781e-05, grad_fn=<DivBackward0>)\n",
            "tensor(0.9894)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n  # combinations = itertools.combinations_with_replacement(batch.node_types, 2)\\n  # possible_edge =  [(s_type,'to',t_type) for s_type,t_type in combinations]\\n  true_edge_types = batch.edge_types\\n  for edge_type in true_edge_types:\\n    if batch[edge_type].edge_index.shape[-1] ==0:\\n      continue\\n    if edge_type in true_edge_types:\\n      print(batch[edge_type].edge_index)\\n    s_ntype, _ , t_ntype = edge_type\\n    # source node embedding and target node embedding\\n    s_nemb, t_nemb = node_out[s_ntype], node_out[t_ntype]\\n    \\n    n_emb = torch.cat([s_nemb, t_nemb], dim=0)\\n    r_emb = edge_out['_'.join(edge_type)]\\n    out_product = n_emb[:, None, :] * n_emb[None,: , :] * r_emb[0][ None, None, :]\\n    N,_,K = out_product.shape\\n    out_adj = torch.sum(out_product, dim=2)\\n    print(out_adj.shape)\\n    # Use torch.topk to get the indices of K largest entrie\\n    values, indices = torch.topk(out_adj.view(-1), K)\\n    row_indices = indices // N\\n    col_indices = indices % N\\n\\n    indices = torch.stack([row_indices, col_indices], dim=0)\\n    # Convert tensors to sets\\n    true_idx = batch[edge_type].edge_index\\n    true_idx[1] += s_nemb.shape[0]\\n    set1 = set(map(tuple, true_idx.t().tolist()))\\n    set2 = set(map(tuple, indices.t().tolist()))\\n    # Find the difference between sets\\n    print(set1,set2)\\n    acc = len(set1 - set2)/len(set1)\\n    print(acc)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "import itertools\n",
        "\n",
        "step = 0\n",
        "model = model.to('cpu')\n",
        "GMSE_error = 0\n",
        "total_samples = 0\n",
        "link_errors = []\n",
        "\n",
        "for batch in loader:\n",
        "  step+=1\n",
        "  if step <= 100:\n",
        "    continue\n",
        "  \n",
        "  if step ==200:\n",
        "    break\n",
        "\n",
        "  node_out, edge_out = model(batch)\n",
        "  n_emb, adj_matrices = batch_to_adjtensor(batch, node_out, edge_out)\n",
        "  r_emb = [e_tensor[0] for e_type, e_tensor in edge_out.items() if e_tensor.shape[0] != 0]\n",
        "  r_emb = torch.stack(r_emb, dim=0)\n",
        "  out_product = n_emb[:, None, None, :] * n_emb[None,: , None, :] * r_emb[ None, None, :, :]\n",
        "  num_nodes,_,num_relations ,K = out_product.shape\n",
        "  out_adj = torch.sum(out_product, dim=3)\n",
        "  # print(out_adj, adj_matrices)\n",
        "  GMSE_error += torch.sum(torch.square(out_adj-adj_matrices))\n",
        "  num_samples = out_adj.shape[0] * out_adj.shape[1]*out_adj.shape[0]\n",
        "  total_samples += num_samples\n",
        "\n",
        "  edge_indices = torch.where(adj_matrices == 1)\n",
        "  # Use these indices to access the corresponding elements in B\n",
        "  out_edges = out_adj[edge_indices]\n",
        "  link_error = torch.mean(torch.square(adj_matrices[edge_indices] - out_edges))\n",
        "  link_errors.append(link_error)\n",
        "\n",
        "print(GMSE_error/total_samples)\n",
        "print(torch.mean(torch.tensor(link_errors)))\n",
        "\n",
        "# Find the indices of the 1 elements in A\n",
        "\n",
        "'''\n",
        "  # combinations = itertools.combinations_with_replacement(batch.node_types, 2)\n",
        "  # possible_edge =  [(s_type,'to',t_type) for s_type,t_type in combinations]\n",
        "  true_edge_types = batch.edge_types\n",
        "  for edge_type in true_edge_types:\n",
        "    if batch[edge_type].edge_index.shape[-1] ==0:\n",
        "      continue\n",
        "    if edge_type in true_edge_types:\n",
        "      print(batch[edge_type].edge_index)\n",
        "    s_ntype, _ , t_ntype = edge_type\n",
        "    # source node embedding and target node embedding\n",
        "    s_nemb, t_nemb = node_out[s_ntype], node_out[t_ntype]\n",
        "    \n",
        "    n_emb = torch.cat([s_nemb, t_nemb], dim=0)\n",
        "    r_emb = edge_out['_'.join(edge_type)]\n",
        "    out_product = n_emb[:, None, :] * n_emb[None,: , :] * r_emb[0][ None, None, :]\n",
        "    N,_,K = out_product.shape\n",
        "    out_adj = torch.sum(out_product, dim=2)\n",
        "    print(out_adj.shape)\n",
        "    # Use torch.topk to get the indices of K largest entrie\n",
        "    values, indices = torch.topk(out_adj.view(-1), K)\n",
        "    row_indices = indices // N\n",
        "    col_indices = indices % N\n",
        "\n",
        "    indices = torch.stack([row_indices, col_indices], dim=0)\n",
        "    # Convert tensors to sets\n",
        "    true_idx = batch[edge_type].edge_index\n",
        "    true_idx[1] += s_nemb.shape[0]\n",
        "    set1 = set(map(tuple, true_idx.t().tolist()))\n",
        "    set2 = set(map(tuple, indices.t().tolist()))\n",
        "    # Find the difference between sets\n",
        "    print(set1,set2)\n",
        "    acc = len(set1 - set2)/len(set1)\n",
        "    print(acc)\n",
        "'''\n",
        "    \n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gzov4LywHNS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Assuming you have two tensors of shape 2xK\n",
        "tensor1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "tensor2 = torch.tensor([[1, 2, 7], [4, 5, 8]])\n",
        "\n",
        "# Convert tensors to sets\n",
        "set1 = set(map(tuple, tensor1.t().tolist()))\n",
        "set2 = set(map(tuple, tensor2.t().tolist()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3apSmsYKYyTV"
      },
      "source": [
        "## Blessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFxAgVNc0zOO"
      },
      "outputs": [],
      "source": [
        "# Divine beast bless no bug here! \n",
        "#         ┌─┐    ┌─┐\n",
        "#      ┌─┘ ┴───┘ ┴──┐\n",
        "#      │                   │\n",
        "#      │       ───       │\n",
        "#      │  ─┬┘     └┬─  │\n",
        "#      │                   │\n",
        "#      │       ─┴─       │\n",
        "#      │                   │\n",
        "#      └─┐         ┌───┘\n",
        "#          │         │\n",
        "#          │         │\n",
        "#          │         │\n",
        "#          │         └──────────────┐\n",
        "#          │                                  │\n",
        "#          │                                  ├─┐\n",
        "#          │                                  ┌─┘\n",
        "#          │                                  │\n",
        "#          └─┐  ┐  ┌──────┬──┐  ┌──┘\n",
        "#            │  ─┤ ─┤         │  ─┤ ─┤\n",
        "#            └──┴──┘         └──┴──┘"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}